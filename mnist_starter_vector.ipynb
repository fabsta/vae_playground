{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE example from http://nbviewer.jupyter.org/github/tfolkman/deep-learning-experiments/blob/master/VAE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:15.570908Z",
     "start_time": "2018-11-21T15:56:15.104530Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:16.475128Z",
     "start_time": "2018-11-21T15:56:16.447287Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load and transform data\n",
    "trainset = torchvision.datasets.MNIST('/tmp', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('/tmp', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:16.627389Z",
     "start_time": "2018-11-21T15:56:16.617238Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "\n",
    "my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
    "my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
    "\n",
    "tensor_x = torch.stack([torch.Tensor(i) for i in my_x]) # transform to torch tensors\n",
    "tensor_y = torch.stack([torch.Tensor(i) for i in my_y])\n",
    "\n",
    "my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "trainloader = utils.DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:17.015527Z",
     "start_time": "2018-11-21T15:56:16.986686Z"
    }
   },
   "outputs": [],
   "source": [
    "my_dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:17.328017Z",
     "start_time": "2018-11-21T15:56:17.318289Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=500, coding_size=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2_mean = nn.Linear(hidden_size, coding_size)\n",
    "        self.hidden2_gamma = nn.Linear(hidden_size, coding_size)\n",
    "        self.hidden3 = nn.Linear(coding_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        hidden1_output = self.relu(self.hidden1(x))\n",
    "        return self.hidden2_mean(hidden1_output), self.hidden2_gamma(hidden1_output)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            noise = Variable(logvar.data.new(logvar.size()).normal_())\n",
    "            return mu + torch.exp(0.5 * logvar) * noise\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        output_value = self.relu(self.hidden3(x))\n",
    "        output_value = self.output(output_value)\n",
    "        return output_value, self.sigmoid(output_value)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mean, gamma = self.encoder(x)\n",
    "        z = self.reparameterize(mean, gamma)\n",
    "        output, output_sigmoid = self.decoder(z)\n",
    "        return output, output_sigmoid, mean, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:17.977025Z",
     "start_time": "2018-11-21T15:56:17.973430Z"
    }
   },
   "outputs": [],
   "source": [
    "# vae = VAE(input_size=28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:18.122249Z",
     "start_time": "2018-11-21T15:56:18.116736Z"
    }
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_size=2*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:18.974738Z",
     "start_time": "2018-11-21T15:56:18.970117Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def custom_loss(recon_x, x, mu, logvar):\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(recon_x, x,\n",
    "                                                size_average=False)\n",
    "    latent_loss = 0.5 * torch.sum(torch.exp(logvar) + mu.pow(2) - 1 - logvar)\n",
    "    return ce_loss + latent_loss\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:19.119867Z",
     "start_time": "2018-11-21T15:56:19.115980Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:20.482766Z",
     "start_time": "2018-11-21T15:56:20.477459Z"
    }
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "total_loss = 0\n",
    "print_every = 70000 // BATCH_SIZE\n",
    "plot_every = 7000 // BATCH_SIZE\n",
    "all_losses = []\n",
    "iter = 1\n",
    "\n",
    "start = time.time()\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:21.826311Z",
     "start_time": "2018-11-21T15:56:20.631641Z"
    }
   },
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T16:46:19.142187Z",
     "start_time": "2018-11-21T16:46:18.560726Z"
    },
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "%%pixie_debugger\n",
    "for e in range(N_EPOCHS):\n",
    "    for i_batch, batch in enumerate(trainloader):\n",
    "        image = batch[0]\n",
    "        label = batch[1]\n",
    "        image = Variable(image.view(-1, 1, 28*28))\n",
    "        \n",
    "        vae.zero_grad()\n",
    "        output, output_sigmoid, mean, gamma = vae(image)\n",
    "        loss = custom_loss(output, image, mean, gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / (len(trainloader) * N_EPOCHS) * 100, loss))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / (plot_every*BATCH_SIZE))\n",
    "            total_loss = 0\n",
    "        \n",
    "        iter = iter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:11.534633Z",
     "start_time": "2018-11-21T15:56:09.853Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:56:11.535435Z",
     "start_time": "2018-11-21T15:56:10.142Z"
    }
   },
   "outputs": [],
   "source": [
    "# get weights of first hiddenlayer\n",
    "hidden1_weights = vae.hidden1.weight.data.cpu().numpy(); len(hidden1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot example of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 0\n",
    "t = plt.imshow(hidden1_weights[neuron].reshape([28,28]),\n",
    "           cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set module to evaluation mode\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a random latent picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = Variable(torch.randn(64, 20)); random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode latent picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sample = vae.decoder(random); sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = plt.imshow(sample[58].data.numpy().reshape(28,28),cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T16:56:53.035609Z",
     "start_time": "2018-11-21T16:56:52.735408Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a7141980ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMolEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMolDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMolEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMolDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T17:05:30.903523Z",
     "start_time": "2018-11-21T17:05:30.742883Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'molencoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6264a3493e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmolencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMolEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMolDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMolEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMolDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'molencoder'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from rdkit import Chem\n",
    "from molencoder.models import MolEncoder, MolDecoder\n",
    "from molencoder.models import MolEncoder, MolDecoder\n",
    "from molencoder.utils import( load_dataset, initialize_weights,ReduceLROnPlateau, save_checkpoint, validate_model)\n",
    "from molencoder.featurizers import Featurizer, OneHotFeaturizer\n",
    " \n",
    " \n",
    "SOURCE = 'c1ccccn1'\n",
    "DEST =  'c1ccccc1'\n",
    "STEPS = 200\n",
    "#charset from chembl\n",
    "WIDTH=120\n",
    " \n",
    "charset = [' ', '#', '%', '(', ')', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '@', 'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'S', 'T', 'V', 'X', 'Z', '[', '\\\\', ']', 'a', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's', 't']\n",
    " \n",
    "def decode_smiles_from_index(vec, charset):\n",
    "    return \"\".join(map(lambda x:charset[x],vec)).strip()\n",
    " \n",
    " \n",
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Interpolate from source to dest in steps\")\n",
    "    parser.add_argument(\"--source\", type=str, default=DEST)\n",
    "    parser.add_argument(\"--dest\", type=str, default=SOURCE)\n",
    "    parser.add_argument(\"--steps\", type=int, default=STEPS)\n",
    "    return parser.parse_args()\n",
    " \n",
    "def interpolate(source, dest, steps, charset, encoder, decoder):\n",
    "    width=WIDTH\n",
    "    source_just = source.ljust(width)\n",
    "    dest_just = dest.ljust(width)\n",
    "    onehot = OneHotFeaturizer(charset=charset)\n",
    "    sourcevec = onehot.featurize(smiles=[source_just])\n",
    "    destvec = onehot.featurize(smiles=[dest_just])\n",
    "    source_encoded = Variable(torch.from_numpy(sourcevec).float()).cuda()\n",
    "    dest_encoded = Variable(torch.from_numpy(destvec).float()).cuda()\n",
    "    source_x_latent = encoder(source_encoded)\n",
    "    dest_x_latent = encoder(dest_encoded)\n",
    "    step = (dest_x_latent-source_x_latent)/float(steps)\n",
    "    results = []\n",
    "    for i in range(steps):\n",
    "        item = source_x_latent + (step*i)\n",
    "        sampled = np.argmax(decoder(item).cpu().data.numpy(), axis=2)\n",
    "        #print(sampled)\n",
    "        decode_smiles = decode_smiles_from_index(sampled[0], charset)\n",
    "        results.append((i, item, decode_smiles))\n",
    "    return results\n",
    " \n",
    "def main():\n",
    "    args= get_arguments()\n",
    "    encoder = MolEncoder( c = len(charset))\n",
    "    decoder = MolDecoder( c = len(charset))\n",
    "    encoder.apply(initialize_weights)\n",
    "    decoder.apply(initialize_weights)\n",
    "     \n",
    "    print( torch.cuda.is_available() )\n",
    "    encoder = MolEncoder( c = len(charset)).cuda()\n",
    "    encoder.apply(initialize_weights)\n",
    "     \n",
    "    decoder = MolDecoder( c = len(charset)).cuda()\n",
    "    decoder.apply(initialize_weights)\n",
    "     \n",
    "    bestmodel = torch.load(\"model_best.pth.tar\")\n",
    "    #bestmodel = torch.load(\"tempcheckpoint.pth.tar\")\n",
    "    encoder.load_state_dict(bestmodel[\"encoder\"])\n",
    "    decoder.load_state_dict(bestmodel[\"decoder\"])\n",
    " \n",
    "    results = interpolate( args.source, args.dest, args.steps, charset, encoder, decoder )\n",
    "    for result in results:\n",
    "        print(result[0], result[2])\n",
    " \n",
    "#if __name__==\"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdn_molecule",
   "language": "python",
   "name": "cdn_molecule"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
