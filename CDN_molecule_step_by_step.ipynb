{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand paper from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from os.path import dirname\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add python modules\n",
    "#import sys\n",
    "sys.path.append('libs/')\n",
    "from parameters import *\n",
    "\n",
    "sys.path.append(DATA_PATH)\n",
    "sys.path.append('CDN_Molecule/')\n",
    "\n",
    "#from preprocess import *\n",
    "import preProcess\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-69cec3e7e6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/CDN_Molecule/preProcess.py\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Load drug-like data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_json\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_file' is not defined"
     ]
    }
   ],
   "source": [
    "preProcess.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The dataset are 250k randomly chosen drug-like molecules from the ZINC database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file '../CDN_Molecule/data/TrainVectors_python3.pickle' ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load data, by default from data/TrainVectors.pickle\n",
    "print(f\"Loading data from file '{FLAGS.data_file}' ...\")\n",
    "x = np.array(pickle.load(open(FLAGS.data_file, \"rb\")))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have 173196 molecules, with 50 features each\n"
     ]
    }
   ],
   "source": [
    "x.shape\n",
    "print(f\" We have {x.shape[0]} molecules, with {x.shape[1]} features each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34,  0,  1,  0,  2,  2,  3,  4,  5,  0,  0,  6,  7,  8,  9,  8,\n",
       "        10,  8,  8,  6, 11,  8, 12, 10, 10,  8, 10, 12,  0, 13,  8,  9,\n",
       "        13,  0,  1,  0,  2,  2,  3,  4,  6,  0, 13,  0,  5, 35, 36, 36,\n",
       "        36, 36],\n",
       "       [34,  0,  0,  1,  7,  3, 19,  4,  6,  0,  0, 13,  1,  0,  2,  4,\n",
       "         6,  0, 13,  6,  0,  0, 13,  1,  0,  2,  3,  4,  6, 15, 13,  8,\n",
       "         5,  8, 20,  8,  8,  5, 21, 22, 35, 36, 36, 36, 36, 36, 36, 36,\n",
       "        36, 36]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.arange(len(x)) = [     0      1      2 ... 173193 173194 173195]\n",
      "len(x) = 173196\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data -- ok, shuffle indices of rows and create a copy, it's memory effective\n",
    "np.random.seed(10)\n",
    "print(f\"np.arange(len(x)) = {np.arange(len(x))}\\nlen(x) = {len(x)}\")\n",
    "shuffle_indices = np.random.permutation(np.arange(len(x)))\n",
    "x_shuffled = x[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 * int(0.03) * float(173196)\n",
      "x_shuffled[:-5195], x_shuffled[-5195:]\n",
      "Train/Dev split: 168001/5195\n"
     ]
    }
   ],
   "source": [
    "print(f\"-1 * int({FLAGS.dev_sample_percentage}) * float({x_shuffled.shape[0]})\")\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(x_shuffled.shape[0]))\n",
    "print(f\"x_shuffled[:{dev_sample_index}], x_shuffled[{dev_sample_index}:]\")\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "print((\"Train/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input(batch):\n",
    "    \"\"\"\n",
    "    This looks like \"shifting\" each row (instance) by one position to the left and adding 36 at the end,\n",
    "    for example\n",
    "    [34  0 16  0  6  0 13  0 10  5  8  6 16 15 13  8  9  8  6 10  8 12 10  9\n",
    "      0  0  0  7 12  8  9  8  8  8  8  8  9 15  0 13 10  6  0 13  8  5 16 15\n",
    "     35 36]\n",
    "\n",
    "    becomes\n",
    "\n",
    "    [ 0 16  0  6  0 13  0 10  5  8  6 16 15 13  8  9  8  6 10  8 12 10  9  0\n",
    "      0  0  7 12  8  9  8  8  8  8  8  9 15  0 13 10  6  0 13  8  5 16 15 35\n",
    "     36 36]\n",
    "\n",
    "    :param current_batch_as_ndarray: 64 rows -- 64 instances per batch, each instance has 50 dimensions\n",
    "    :return: tuple: the parameter itself (current_batch_as_ndarray) and the shifted one, see above\n",
    "    \"\"\"\n",
    "\n",
    "    # There are 64 rows -- 64 instances per batch, each instance has 50 dimensions\n",
    "    # print(\"split_input(), current_batch_as_ndarray.shape:\", current_batch_as_ndarray.shape)\n",
    "\n",
    "    # simply copy current_batch_as_ndarray to x_batch output\n",
    "    x_batch = batch\n",
    "\n",
    "    # remove the first position and add 36 to the end\n",
    "    y_batch = np.concatenate([x_batch[:, 1:], np.zeros(shape=[x_batch.shape[0], 1], dtype=np.int32) + 36],\n",
    "                             axis=1)\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = split_input(x_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 15, 16,  0,  6,  1, 15, 11,  4, 13,  8,  5,  8,  8,  8,  6,\n",
       "        11,  8,  9,  8,  8,  8,  6,  0, 18, 13,  8,  8,  9, 13,  8,  8,\n",
       "         5, 24, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "        36, 36]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 16,  0,  6,  1, 15, 11,  4, 13,  8,  5,  8,  8,  8,  6, 11,\n",
       "         8,  9,  8,  8,  8,  6,  0, 18, 13,  8,  8,  9, 13,  8,  8,  5,\n",
       "        24, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "        36, 36]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CDN:\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters, max_molecule_length,\n",
    "                 gaussian_samples, variational=True, l2_reg_lambda=0.5, generation_mode=False, test_mode=False):\n",
    "# define some variables\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.max_molecule_length = max_molecule_length\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.gaussian_samples_dim = gaussian_samples\n",
    "        self.variational = variational\n",
    "        self.encoder_input_GO = tf.placeholder(tf.int32, [None, sequence_length], name=\"encoder_input\")\n",
    "        self.encoder_input = tf.placeholder(tf.int32, [None, sequence_length], name=\"encoder_input\")  ## no go\n",
    "        self.gaussian_samples = tf.placeholder(tf.float32, [None, self.gaussian_samples_dim], name=\"unit_gaussians\")\n",
    "        self.generation_mode = generation_mode\n",
    "        self.test_mode = test_mode\n",
    "# how to encode \n",
    "    def encode(self):\n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.E = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.E, self.encoder_input)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            self.embedded_chars_go = tf.nn.embedding_lookup(self.E, self.encoder_input_GO)\n",
    "\n",
    "        # Create a convolution layers for each filter size\n",
    "        conv_flatten = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                conv_flatten.append(tf.contrib.layers.flatten(h))\n",
    "        conv_output = tf.concat(conv_flatten, axis=1)\n",
    "\n",
    "        # Flatten feature vector\n",
    "        h_pool_flat3 = tf.nn.relu(tf.contrib.layers.linear(conv_output, 450))\n",
    "\n",
    "        if self.variational:\n",
    "            with tf.name_scope(\"Variational\"):\n",
    "                self.z_mean = tf.contrib.layers.linear(h_pool_flat3, 300)\n",
    "                self.z_stddev = tf.contrib.layers.linear(h_pool_flat3, 300)\n",
    "                latent_loss = 0.5 * tf.reduce_sum(\n",
    "                    tf.square(self.z_mean) + tf.square(self.z_stddev) -\n",
    "                    tf.log(tf.square(self.z_stddev) + 1e-10 ) - 1,\n",
    "                    1\n",
    "                )\n",
    "                self.mean_latent_loss = tf.reduce_mean(latent_loss)\n",
    "                if self.generation_mode:\n",
    "                    h_pool_flat = self.gaussian_samples\n",
    "                else:\n",
    "                    h_pool_flat = self.z_mean + (self.z_stddev * self.gaussian_samples)\n",
    "\n",
    "                h_pool_flat = tf.identity(h_pool_flat, \"encoded_final\")\n",
    "\n",
    "        return h_pool_flat, self.mean_latent_loss\n",
    "\n",
    "    def decode_rnn(self, z):\n",
    "        def pick_next_argmax(former_output, step):\n",
    "            next_symbol = tf.expand_dims(tf.stop_gradient(tf.argmax(former_output, 1)), axis=-1)\n",
    "            return tf.nn.embedding_lookup(self.E, next_symbol), next_symbol\n",
    "\n",
    "        def pick_next_top_k(former_output, step):\n",
    "            next_symbol = tf.multinomial(former_output, 1)\n",
    "            return tf.nn.embedding_lookup(self.E, next_symbol), next_symbol\n",
    "\n",
    "        with tf.name_scope(\"Decoder\"):\n",
    "            self.decode_start = tf.nn.relu(tf.contrib.layers.linear(z, 150))\n",
    "            decoder_inputs_list = tf.split(self.embedded_chars_go, self.max_molecule_length, axis=1)\n",
    "            decoder_inputs_list = [tf.squeeze(i, axis=1) for i in decoder_inputs_list]\n",
    "            rnn_cell = tf.nn.rnn_cell.LSTMCell(150, state_is_tuple=False)\n",
    "\n",
    "            self.lstm_outputs = []\n",
    "            temp_logits = []\n",
    "            self.all_symbols = []\n",
    "            symbol = tf.ones(1)  # output for test mode\n",
    "            for i in range(self.max_molecule_length):\n",
    "                if not self.test_mode or i == 0:\n",
    "                    if i == 0:\n",
    "                        output, state = rnn_cell(decoder_inputs_list[i], state=z)\n",
    "                    else:\n",
    "                        output, state = rnn_cell(decoder_inputs_list[i], state=state)\n",
    "                else:\n",
    "                    next_decoder_input, symbol = pick_next_argmax(temp_logits[-1], i)\n",
    "                    next_decoder_input = tf.squeeze(next_decoder_input, axis=1)\n",
    "                    output, state = rnn_cell(next_decoder_input, state=state)\n",
    "                with tf.variable_scope(\"decoder_output_to_logits\") as scope_logits:\n",
    "                    if i > 0:\n",
    "                        scope_logits.reuse_variables()\n",
    "                    temp_logits.append(tf.contrib.layers.linear(output, self.vocab_size))\n",
    "\n",
    "                self.lstm_outputs.append(output)\n",
    "                if i > 0:\n",
    "                    self.all_symbols.append(symbol)\n",
    "                if i == self.max_molecule_length - 1 and self.test_mode:\n",
    "                    self.all_symbols.append(pick_next_argmax(temp_logits[-1], i)[1])\n",
    "            if self.test_mode:\n",
    "                self.all_symbols = tf.squeeze(tf.transpose(tf.stack(self.all_symbols), [1,0,2]), axis=-1)\n",
    "\n",
    "            self.decoder_logits = tf.transpose(tf.stack(temp_logits), perm=[1, 0, 2])\n",
    "            self.decoder_prediction = tf.argmax(self.decoder_logits, 2, name=\"decoder_predictions\")\n",
    "\n",
    "            return self.decoder_logits\n",
    "\n",
    "    def loss(self, logits, latent_loss):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.output_onehot = tf.one_hot(self.encoder_input, self.vocab_size)\n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.output_onehot)\n",
    "            self.CE_loss = tf.reduce_mean(self.losses)\n",
    "            self.total_loss = self.CE_loss + .00001 * latent_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            decoder_prediction = tf.argmax(logits, 2, name=\"decoder_predictions\")\n",
    "            x_target = tf.to_int64(self.encoder_input)\n",
    "            correct_predictions = tf.equal(decoder_prediction, x_target)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "        return self.total_loss, self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = model.CDN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            vocab_size=FLAGS.vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            max_molecule_length=FLAGS.max_molecule_length,\n",
    "            gaussian_samples=FLAGS.unit_gaussian_dim,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            variational=True,\n",
    "            test_mode=False,\n",
    "            generation_mode=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CDN' object has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-aa3a1fb6a56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-aa3a1fb6a56b>\u001b[0m in \u001b[0;36mmodel_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CDN' object has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.layers import base\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.summary' from '/home/schreibf/anaconda3/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/summary/__init__.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "What the code does\n",
    "\n",
    "1. instantiate model\n",
    "2. set learning rate\n",
    "3. set adam optimizer\n",
    "4. Instantiate Encoder\n",
    "5. Instantiate Decoder\n",
    "6. Instantiate Loss function (optimize gradients)\n",
    "7. Define summaries\n",
    "8. Generate batches\n",
    "9. Run training over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa3f7cabf60>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From ../CDN_Molecule/model.py:122: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected/weights:0/grad/hist is illegal; using fully_connected/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected/weights:0/grad/sparsity is illegal; using fully_connected/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected/biases:0/grad/hist is illegal; using fully_connected/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected/biases:0/grad/sparsity is illegal; using fully_connected/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/weights:0/grad/hist is illegal; using fully_connected_1/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/weights:0/grad/sparsity is illegal; using fully_connected_1/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/biases:0/grad/hist is illegal; using fully_connected_1/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/biases:0/grad/sparsity is illegal; using fully_connected_1/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/weights:0/grad/hist is illegal; using fully_connected_2/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/weights:0/grad/sparsity is illegal; using fully_connected_2/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/biases:0/grad/hist is illegal; using fully_connected_2/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/biases:0/grad/sparsity is illegal; using fully_connected_2/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/kernel:0/grad/hist is illegal; using lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/kernel:0/grad/sparsity is illegal; using lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/bias:0/grad/hist is illegal; using lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/bias:0/grad/sparsity is illegal; using lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/weights:0/grad/hist is illegal; using decoder_output_to_logits/fully_connected/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/weights:0/grad/sparsity is illegal; using decoder_output_to_logits/fully_connected/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/biases:0/grad/hist is illegal; using decoder_output_to_logits/fully_connected/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/biases:0/grad/sparsity is illegal; using decoder_output_to_logits/fully_connected/biases_0/grad/sparsity instead.\n",
      "Writing to /home/schreibf/projects/vae_playground/runs/1542801995\n",
      "\n",
      "2018-11-21T07:07:20.354837: step 101, loss 1.0198, latentLoss: 893.19, reconstructionLoss: 1.01086, acc 0.697187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:07:21.004808: step 101, loss 1.01704, klDiv: 948.106, CE-loss: 1.00755, acc 0.698438\n",
      "\n",
      "2018-11-21T07:07:55.936118: step 201, loss 0.781611, latentLoss: 899.533, reconstructionLoss: 0.772616, acc 0.754375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:07:55.981918: step 201, loss 0.780985, klDiv: 865.064, CE-loss: 0.772334, acc 0.754687\n",
      "\n",
      "2018-11-21T07:08:29.875956: step 301, loss 0.720014, latentLoss: 842.392, reconstructionLoss: 0.71159, acc 0.775625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:08:29.921280: step 301, loss 0.705258, klDiv: 816.507, CE-loss: 0.697093, acc 0.780625\n",
      "\n",
      "2018-11-21T07:09:04.136851: step 401, loss 0.622221, latentLoss: 778.696, reconstructionLoss: 0.614434, acc 0.814687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:09:04.175780: step 401, loss 0.61255, klDiv: 765.567, CE-loss: 0.604894, acc 0.82\n",
      "\n",
      "2018-11-21T07:09:36.841183: step 501, loss 0.582954, latentLoss: 761.147, reconstructionLoss: 0.575342, acc 0.819687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:09:36.885619: step 501, loss 0.568745, klDiv: 770.335, CE-loss: 0.561042, acc 0.819687\n",
      "\n",
      "2018-11-21T07:10:08.746794: step 601, loss 0.528018, latentLoss: 715.258, reconstructionLoss: 0.520865, acc 0.831875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:10:08.801205: step 601, loss 0.512437, klDiv: 725.768, CE-loss: 0.50518, acc 0.83875\n",
      "\n",
      "2018-11-21T07:10:40.747513: step 701, loss 0.495132, latentLoss: 767.819, reconstructionLoss: 0.487453, acc 0.837188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:10:40.795820: step 701, loss 0.477886, klDiv: 776.99, CE-loss: 0.470116, acc 0.85\n",
      "\n",
      "2018-11-21T07:11:12.483761: step 801, loss 0.447715, latentLoss: 743.03, reconstructionLoss: 0.440285, acc 0.849688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:11:12.527889: step 801, loss 0.432958, klDiv: 755.178, CE-loss: 0.425406, acc 0.859062\n",
      "\n",
      "2018-11-21T07:11:44.756982: step 901, loss 0.447941, latentLoss: 799.782, reconstructionLoss: 0.439943, acc 0.85625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:11:44.802286: step 901, loss 0.434839, klDiv: 792.233, CE-loss: 0.426917, acc 0.86375\n",
      "\n",
      "2018-11-21T07:12:16.928823: step 1001, loss 0.411869, latentLoss: 814.807, reconstructionLoss: 0.403721, acc 0.868438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:12:16.976849: step 1001, loss 0.396122, klDiv: 798.7, CE-loss: 0.388135, acc 0.877813\n",
      "\n",
      "2018-11-21T07:12:48.321501: step 1101, loss 0.39497, latentLoss: 749.855, reconstructionLoss: 0.387471, acc 0.874687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:12:48.365928: step 1101, loss 0.379569, klDiv: 743.545, CE-loss: 0.372134, acc 0.880625\n",
      "\n",
      "2018-11-21T07:13:19.261838: step 1201, loss 0.372378, latentLoss: 747.615, reconstructionLoss: 0.364902, acc 0.880625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:13:19.334706: step 1201, loss 0.357206, klDiv: 731.814, CE-loss: 0.349888, acc 0.878438\n",
      "\n",
      "2018-11-21T07:13:49.990784: step 1301, loss 0.34519, latentLoss: 780.653, reconstructionLoss: 0.337383, acc 0.891875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:13:50.041755: step 1301, loss 0.333053, klDiv: 765.235, CE-loss: 0.325401, acc 0.894063\n",
      "\n",
      "2018-11-21T07:14:20.679571: step 1401, loss 0.361711, latentLoss: 742.151, reconstructionLoss: 0.354289, acc 0.882187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:14:20.726220: step 1401, loss 0.34806, klDiv: 743.79, CE-loss: 0.340622, acc 0.885312\n",
      "\n",
      "2018-11-21T07:14:50.972115: step 1501, loss 0.330436, latentLoss: 745.132, reconstructionLoss: 0.322985, acc 0.893438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:14:51.015595: step 1501, loss 0.317265, klDiv: 756.165, CE-loss: 0.309703, acc 0.902188\n",
      "\n",
      "2018-11-21T07:15:21.186978: step 1601, loss 0.319794, latentLoss: 738.409, reconstructionLoss: 0.31241, acc 0.900625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:15:21.233963: step 1601, loss 0.303247, klDiv: 749.067, CE-loss: 0.295756, acc 0.906563\n",
      "\n",
      "2018-11-21T07:15:50.816488: step 1701, loss 0.287626, latentLoss: 746.195, reconstructionLoss: 0.280164, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:15:50.862590: step 1701, loss 0.275721, klDiv: 747.92, CE-loss: 0.268242, acc 0.915\n",
      "\n",
      "2018-11-21T07:16:20.718093: step 1801, loss 0.297884, latentLoss: 735.309, reconstructionLoss: 0.290531, acc 0.902812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:16:20.759503: step 1801, loss 0.277206, klDiv: 747.704, CE-loss: 0.269729, acc 0.912188\n",
      "\n",
      "2018-11-21T07:16:50.552746: step 1901, loss 0.260852, latentLoss: 710.829, reconstructionLoss: 0.253744, acc 0.914688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:16:50.600585: step 1901, loss 0.262126, klDiv: 703.586, CE-loss: 0.25509, acc 0.909688\n",
      "\n",
      "2018-11-21T07:17:21.795120: step 2001, loss 0.272195, latentLoss: 727.888, reconstructionLoss: 0.264916, acc 0.913125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:17:21.841728: step 2001, loss 0.257079, klDiv: 724.914, CE-loss: 0.24983, acc 0.919688\n",
      "\n",
      "2018-11-21T07:17:51.714009: step 2101, loss 0.241436, latentLoss: 692.269, reconstructionLoss: 0.234513, acc 0.918437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:17:51.759537: step 2101, loss 0.234111, klDiv: 691.802, CE-loss: 0.227193, acc 0.922188\n",
      "\n",
      "2018-11-21T07:18:22.097582: step 2201, loss 0.280688, latentLoss: 719.262, reconstructionLoss: 0.273495, acc 0.904063\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:18:22.139189: step 2201, loss 0.267067, klDiv: 717.009, CE-loss: 0.259897, acc 0.913437\n",
      "\n",
      "2018-11-21T07:18:51.786272: step 2301, loss 0.244133, latentLoss: 737.943, reconstructionLoss: 0.236754, acc 0.9175\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:18:51.828216: step 2301, loss 0.225455, klDiv: 753.846, CE-loss: 0.217917, acc 0.923125\n",
      "\n",
      "2018-11-21T07:19:22.232055: step 2401, loss 0.23743, latentLoss: 707.437, reconstructionLoss: 0.230356, acc 0.925313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:19:22.273305: step 2401, loss 0.220211, klDiv: 702.972, CE-loss: 0.213182, acc 0.929375\n",
      "\n",
      "2018-11-21T07:19:51.738815: step 2501, loss 0.234982, latentLoss: 695.77, reconstructionLoss: 0.228025, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:19:51.780525: step 2501, loss 0.20684, klDiv: 704.365, CE-loss: 0.199797, acc 0.934062\n",
      "\n",
      "2018-11-21T07:20:21.181232: step 2601, loss 0.202175, latentLoss: 688.084, reconstructionLoss: 0.195294, acc 0.935938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T07:20:21.225130: step 2601, loss 0.197768, klDiv: 681.081, CE-loss: 0.190957, acc 0.940625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in the scope of a default TF graph... why? TL;DR: It's unnecessary, but it's a good practice to follow.\n",
    "# See https://stackoverflow.com/a/39616491\n",
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True)\n",
    "    )\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    # here we begin the tensorflow session\n",
    "    with sess.as_default():\n",
    "        cnn = model.CDN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            vocab_size=FLAGS.vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            max_molecule_length=FLAGS.max_molecule_length,\n",
    "            gaussian_samples=FLAGS.unit_gaussian_dim,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            variational=True,\n",
    "            test_mode=False,\n",
    "            generation_mode=False\n",
    "        )\n",
    "        # cnn is a simple CDN object, nothing inherited from TF classes\n",
    "        assert isinstance(cnn, model.CDN)\n",
    "\n",
    "        # TODO why do we need a global_step non-trainable variable?\n",
    "        # According to this StackOverflow, it's a way to train AdamOptimizer with custom learning rate, see\n",
    "        # https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay\n",
    "        global_step = tf.Variable(1, name=\"global_step\", trainable=False)\n",
    "\n",
    "        # A scalar `Tensor` of the same type as `learning_rate`.  The decayed learning rate.\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=FLAGS.initial_learning_rate, global_step=global_step,\n",
    "                                                   decay_steps=5000, decay_rate=0.96, staircase=True)\n",
    "\n",
    "        # somehow confusing how the learning rate is passed to the AdamOptimizer...\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        # but we know the type: it's AdamOptimizer instance, inherited from Optimizer\n",
    "        assert isinstance(optimizer, tf.train.AdamOptimizer)\n",
    "\n",
    "        # CDN.encode() defines the computational graph - returns two tensors\n",
    "        # TODO explore encode() in detail\n",
    "        \n",
    "        #####\n",
    "        # Encoding part\n",
    "        #####\n",
    "        # returns pooling and loss function (mean_latent_loss)\n",
    "        encoded, latent_loss = cnn.encode()\n",
    "        assert isinstance(encoded, tf.Tensor)\n",
    "        assert isinstance(latent_loss, tf.Tensor)\n",
    "        \n",
    "        #####\n",
    "        # Decoder part\n",
    "        #####\n",
    "        # another Tensor output from the decoder\n",
    "        logits = cnn.decode_rnn(encoded)\n",
    "        assert isinstance(logits, tf.Tensor)\n",
    "\n",
    "        #####\n",
    "        # loss function\n",
    "        #####\n",
    "        # and CDN.loss() returns two Tensors: loss and accuracy\n",
    "        loss, accuracy = cnn.loss(logits, latent_loss)\n",
    "        assert isinstance(loss, tf.Tensor)\n",
    "        assert isinstance(accuracy, tf.Tensor)\n",
    "\n",
    "        # grads_and_vars is a list\n",
    "        # It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\".\n",
    "        # It is the first part of minimize()\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "        # The second part of minimize() which applies gradients to variables\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        assert isinstance(train_op, tf.Operation)\n",
    "\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory\n",
    "        timestamp = str(int(time.time()))\n",
    "        if FLAGS.parameters_file is not None:\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, FLAGS.parameters_file))\n",
    "        else:\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print((\"Writing to {}\\n\".format(out_dir)))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        kl_div_summary = tf.summary.scalar(\"KL-div\", latent_loss)\n",
    "        CE_loss_summary = tf.summary.scalar(\"CE-loss\", cnn.CE_loss)\n",
    "        learning_rate_summary = tf.summary.scalar(\"learningRate\", learning_rate)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge(\n",
    "            [loss_summary, acc_summary, grad_summaries_merged, kl_div_summary, CE_loss_summary,\n",
    "             learning_rate_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\") # file directory\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph) # save in file\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary, kl_div_summary, CE_loss_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory.\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        \n",
    "        # Resuming training\n",
    "        if (FLAGS.load_param) and (FLAGS.parameters_file is not None):\n",
    "            # model file to restart training\n",
    "            checkpoint_file = FLAGS.parameters_file\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            print(\"Loaded: \" + str(checkpoint_file))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Generate batches - a batch iterator for a data set\n",
    "        num_epochs  = 1\n",
    "        #batches = preProcess.batch_iter(x_train, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        batches = preProcess.batch_iter(x_train, FLAGS.batch_size, num_epochs)\n",
    "        # what is actually 'batches'? - it is a generator\n",
    "        # a batch is defined as an x and a y.\n",
    "        \n",
    "        for idx, batch in enumerate(batches):\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            # split each batch into x and y\n",
    "            x_bat, y_bat = split_input(batch)\n",
    "            # feed data to placeholders, so data that changes with each batch\n",
    "            \n",
    "            feed_dict = {\n",
    "                # these are CDN placeholders\n",
    "                cnn.encoder_input: y_bat,\n",
    "                cnn.encoder_input_GO: x_bat,\n",
    "                cnn.gaussian_samples: np.random.normal(size=[x_bat.shape[0], FLAGS.unit_gaussian_dim]),\n",
    "            }\n",
    "\n",
    "            # Runs operations and evaluates tensors in `fetches`.\n",
    "            # TODO Why the heck do we need to call train_op AND total_loss, summary, etc.? Just for debugging?\n",
    "            _, step, summaries, loss, accuracy, kldiv, CEloss = sess.run(\n",
    "                fetches=[train_op, global_step, train_summary_op, cnn.total_loss, cnn.accuracy, cnn.mean_latent_loss,\n",
    "                         cnn.CE_loss], feed_dict=feed_dict)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if current_step % 100 == 0:\n",
    "                print((\"{}: step {}, loss {:g}, latentLoss: {:g}, reconstructionLoss: {:g}, acc {:g}\".format(\n",
    "                    time_str, step, loss, kldiv, CEloss, accuracy)))\n",
    "\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            # every N steps we run also evaluation\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "\n",
    "                x_bat, y_bat = split_input(batch)\n",
    "                feed_dict = {\n",
    "                    cnn.encoder_input: y_bat,\n",
    "                    cnn.encoder_input_GO: x_bat,\n",
    "                    cnn.gaussian_samples: np.random.normal(size=[x_bat.shape[0], FLAGS.unit_gaussian_dim]),\n",
    "                }\n",
    "\n",
    "                step, summaries, loss, accuracy, kldiv, CEloss = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.total_loss, cnn.accuracy, cnn.mean_latent_loss, cnn.CE_loss],\n",
    "                    feed_dict)\n",
    "\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\n",
    "                    (\"{}: step {}, loss {:g}, klDiv: {:g}, CE-loss: {:g}, acc {:g}\".format(time_str, step, loss, kldiv,\n",
    "                                                                                           CEloss, accuracy)))\n",
    "                if dev_summary_writer:\n",
    "                    dev_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print((\"Saved model checkpoint to {}\\n\".format(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdn_molecule",
   "language": "python",
   "name": "cdn_molecule"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
