{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand paper from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:13:20.182820Z",
     "start_time": "2018-11-21T15:13:20.153139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 17681910358801304916),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10858605268217959304),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 701322241902570240),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 7927899751, 406705154930552638)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:51.599364Z",
     "start_time": "2018-11-21T14:08:51.421588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## notebook setup\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:52.410985Z",
     "start_time": "2018-11-21T14:08:51.644268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:52.436130Z",
     "start_time": "2018-11-21T14:08:52.412732Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./libs/')\n",
    "from parameters import *\n",
    "\n",
    "sys.path.append(DATA_PATH)\n",
    "sys.path.append('./CDN_Molecule/')\n",
    "\n",
    "import preProcess\n",
    "import model\n",
    "\n",
    "#from challenge_setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:53.556511Z",
     "start_time": "2018-11-21T14:08:53.536302Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:05:26.015515Z",
     "start_time": "2018-11-21T14:05:25.985022Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    " \n",
    "The dataset are 250k randomly chosen drug-like molecules from the ZINC database.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:54.978050Z",
     "start_time": "2018-11-21T14:08:54.457779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file '/home/fabsta/projects/deeplearning/vae_playground/CDN_Molecule/data/TrainVectors.pickle' ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load data, by default from data/TrainVectors.pickle\n",
    "print(f\"Loading data from file '{FLAGS.data_file}' ...\")\n",
    "x = np.array(pickle.load(open(FLAGS.data_file, \"rb\")))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:56.420786Z",
     "start_time": "2018-11-21T14:08:56.400464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have 173196 molecules, with 50 features each\n"
     ]
    }
   ],
   "source": [
    "x.shape\n",
    "print(f\" We have {x.shape[0]} molecules, with {x.shape[1]} features each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:08:59.031636Z",
     "start_time": "2018-11-21T14:08:59.005799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34,  0,  1,  0,  2,  2,  3,  4,  5,  0,  0,  6,  7,  8,  9,  8,\n",
       "        10,  8,  8,  6, 11,  8, 12, 10, 10,  8, 10, 12,  0, 13,  8,  9,\n",
       "        13,  0,  1,  0,  2,  2,  3,  4,  6,  0, 13,  0,  5, 35, 36, 36,\n",
       "        36, 36],\n",
       "       [34,  0,  0,  1,  7,  3, 19,  4,  6,  0,  0, 13,  1,  0,  2,  4,\n",
       "         6,  0, 13,  6,  0,  0, 13,  1,  0,  2,  3,  4,  6, 15, 13,  8,\n",
       "         5,  8, 20,  8,  8,  5, 21, 22, 35, 36, 36, 36, 36, 36, 36, 36,\n",
       "        36, 36]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:00.539279Z",
     "start_time": "2018-11-21T14:09:00.499592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.arange(len(x)) = [     0      1      2 ... 173193 173194 173195]\n",
      "len(x) = 173196\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data -- ok, shuffle indices of rows and create a copy, it's memory effective\n",
    "np.random.seed(10)\n",
    "print(f\"np.arange(len(x)) = {np.arange(len(x))}\\nlen(x) = {len(x)}\")\n",
    "shuffle_indices = np.random.permutation(np.arange(len(x)))\n",
    "x_shuffled = x[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:02.741142Z",
     "start_time": "2018-11-21T14:09:02.716093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 * int(0.03) * float(173196)\n",
      "x_shuffled[:-5195], x_shuffled[-5195:]\n",
      "Train/Dev split: 168001/5195\n"
     ]
    }
   ],
   "source": [
    "print(f\"-1 * int({FLAGS.dev_sample_percentage}) * float({x_shuffled.shape[0]})\")\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(x_shuffled.shape[0]))\n",
    "print(f\"x_shuffled[:{dev_sample_index}], x_shuffled[{dev_sample_index}:]\")\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "print((\"Train/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:03.902809Z",
     "start_time": "2018-11-21T14:09:03.880460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_input(batch):\n",
    "    \"\"\"\n",
    "    This looks like \"shifting\" each row (instance) by one position to the left and adding 36 at the end,\n",
    "    for example\n",
    "    [34  0 16  0  6  0 13  0 10  5  8  6 16 15 13  8  9  8  6 10  8 12 10  9\n",
    "      0  0  0  7 12  8  9  8  8  8  8  8  9 15  0 13 10  6  0 13  8  5 16 15\n",
    "     35 36]\n",
    " \n",
    "    becomes\n",
    " \n",
    "    [ 0 16  0  6  0 13  0 10  5  8  6 16 15 13  8  9  8  6 10  8 12 10  9  0\n",
    "      0  0  7 12  8  9  8  8  8  8  8  9 15  0 13 10  6  0 13  8  5 16 15 35\n",
    "     36 36]\n",
    " \n",
    "    :param current_batch_as_ndarray: 64 rows -- 64 instances per batch, each instance has 50 dimensions\n",
    "    :return: tuple: the parameter itself (current_batch_as_ndarray) and the shifted one, see above\n",
    "    \"\"\"\n",
    " \n",
    "    # There are 64 rows -- 64 instances per batch, each instance has 50 dimensions\n",
    "    # print(\"split_input(), current_batch_as_ndarray.shape:\", current_batch_as_ndarray.shape)\n",
    " \n",
    "    # simply copy current_batch_as_ndarray to x_batch output\n",
    "    x_batch = batch\n",
    " \n",
    "    # remove the first position and add 36 to the end\n",
    "    y_batch = np.concatenate([x_batch[:, 1:], np.zeros(shape=[x_batch.shape[0], 1], dtype=np.int32) + 36],\n",
    "                             axis=1)\n",
    " \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:04.062619Z",
     "start_time": "2018-11-21T14:09:04.043168Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = split_input(x_train[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:09.475833Z",
     "start_time": "2018-11-21T14:09:09.429203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    " \n",
    "class CDN:\n",
    " \n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters, max_molecule_length,\n",
    "                 gaussian_samples, variational=True, l2_reg_lambda=0.5, generation_mode=False, test_mode=False):\n",
    "# define some variables\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.max_molecule_length = max_molecule_length\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.gaussian_samples_dim = gaussian_samples\n",
    "        self.variational = variational\n",
    "        self.encoder_input_GO = tf.placeholder(tf.int32, [None, sequence_length], name=\"encoder_input\")\n",
    "        self.encoder_input = tf.placeholder(tf.int32, [None, sequence_length], name=\"encoder_input\")  ## no go\n",
    "        self.gaussian_samples = tf.placeholder(tf.float32, [None, self.gaussian_samples_dim], name=\"unit_gaussians\")\n",
    "        self.generation_mode = generation_mode\n",
    "        self.test_mode = test_mode\n",
    "# how to encode\n",
    "    def encode(self):\n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.E = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.E, self.encoder_input)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            self.embedded_chars_go = tf.nn.embedding_lookup(self.E, self.encoder_input_GO)\n",
    " \n",
    "        # Create a convolution layers for each filter size\n",
    "        conv_flatten = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                conv_flatten.append(tf.contrib.layers.flatten(h))\n",
    "        conv_output = tf.concat(conv_flatten, axis=1)\n",
    " \n",
    "        # Flatten feature vector\n",
    "        h_pool_flat3 = tf.nn.relu(tf.contrib.layers.linear(conv_output, 450))\n",
    " \n",
    "        if self.variational:\n",
    "            with tf.name_scope(\"Variational\"):\n",
    "                self.z_mean = tf.contrib.layers.linear(h_pool_flat3, 300)\n",
    "                self.z_stddev = tf.contrib.layers.linear(h_pool_flat3, 300)\n",
    "                latent_loss = 0.5 * tf.reduce_sum(\n",
    "                    tf.square(self.z_mean) + tf.square(self.z_stddev) -\n",
    "                    tf.log(tf.square(self.z_stddev) + 1e-10 ) - 1,\n",
    "                    1\n",
    "                )\n",
    "                self.mean_latent_loss = tf.reduce_mean(latent_loss)\n",
    "                if self.generation_mode:\n",
    "                    h_pool_flat = self.gaussian_samples\n",
    "                else:\n",
    "                    h_pool_flat = self.z_mean + (self.z_stddev * self.gaussian_samples)\n",
    " \n",
    "                h_pool_flat = tf.identity(h_pool_flat, \"encoded_final\")\n",
    " \n",
    "        return h_pool_flat, self.mean_latent_loss\n",
    " \n",
    "    def decode_rnn(self, z):\n",
    "        def pick_next_argmax(former_output, step):\n",
    "            next_symbol = tf.expand_dims(tf.stop_gradient(tf.argmax(former_output, 1)), axis=-1)\n",
    "            return tf.nn.embedding_lookup(self.E, next_symbol), next_symbol\n",
    " \n",
    "        def pick_next_top_k(former_output, step):\n",
    "            next_symbol = tf.multinomial(former_output, 1)\n",
    "            return tf.nn.embedding_lookup(self.E, next_symbol), next_symbol\n",
    " \n",
    "        with tf.name_scope(\"Decoder\"):\n",
    "            self.decode_start = tf.nn.relu(tf.contrib.layers.linear(z, 150))\n",
    "            decoder_inputs_list = tf.split(self.embedded_chars_go, self.max_molecule_length, axis=1)\n",
    "            decoder_inputs_list = [tf.squeeze(i, axis=1) for i in decoder_inputs_list]\n",
    "            rnn_cell = tf.nn.rnn_cell.LSTMCell(150, state_is_tuple=False)\n",
    " \n",
    "            self.lstm_outputs = []\n",
    "            temp_logits = []\n",
    "            self.all_symbols = []\n",
    "            symbol = tf.ones(1)  # output for test mode\n",
    "            for i in range(self.max_molecule_length):\n",
    "                if not self.test_mode or i == 0:\n",
    "                    if i == 0:\n",
    "                        output, state = rnn_cell(decoder_inputs_list[i], state=z)\n",
    "                    else:\n",
    "                        output, state = rnn_cell(decoder_inputs_list[i], state=state)\n",
    "                else:\n",
    "                    next_decoder_input, symbol = pick_next_argmax(temp_logits[-1], i)\n",
    "                    next_decoder_input = tf.squeeze(next_decoder_input, axis=1)\n",
    "                    output, state = rnn_cell(next_decoder_input, state=state)\n",
    "                with tf.variable_scope(\"decoder_output_to_logits\") as scope_logits:\n",
    "                    if i > 0:\n",
    "                        scope_logits.reuse_variables()\n",
    "                    temp_logits.append(tf.contrib.layers.linear(output, self.vocab_size))\n",
    " \n",
    "                self.lstm_outputs.append(output)\n",
    "                if i > 0:\n",
    "                    self.all_symbols.append(symbol)\n",
    "                if i == self.max_molecule_length - 1 and self.test_mode:\n",
    "                    self.all_symbols.append(pick_next_argmax(temp_logits[-1], i)[1])\n",
    "            if self.test_mode:\n",
    "                self.all_symbols = tf.squeeze(tf.transpose(tf.stack(self.all_symbols), [1,0,2]), axis=-1)\n",
    " \n",
    "            self.decoder_logits = tf.transpose(tf.stack(temp_logits), perm=[1, 0, 2])\n",
    "            self.decoder_prediction = tf.argmax(self.decoder_logits, 2, name=\"decoder_predictions\")\n",
    " \n",
    "            return self.decoder_logits\n",
    " \n",
    "    def loss(self, logits, latent_loss):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.output_onehot = tf.one_hot(self.encoder_input, self.vocab_size)\n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.output_onehot)\n",
    "            self.CE_loss = tf.reduce_mean(self.losses)\n",
    "            self.total_loss = self.CE_loss + .00001 * latent_loss\n",
    " \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            decoder_prediction = tf.argmax(logits, 2, name=\"decoder_predictions\")\n",
    "            x_target = tf.to_int64(self.encoder_input)\n",
    "            correct_predictions = tf.equal(decoder_prediction, x_target)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    " \n",
    "        return self.total_loss, self.accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:09:10.503432Z",
     "start_time": "2018-11-21T14:09:10.470335Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = model.CDN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            vocab_size=FLAGS.vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            max_molecule_length=FLAGS.max_molecule_length,\n",
    "            gaussian_samples=FLAGS.unit_gaussian_dim,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            variational=True,\n",
    "            test_mode=False,\n",
    "            generation_mode=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    " \n",
    "What the code does\n",
    " \n",
    "1. instantiate model\n",
    "2. set learning rate\n",
    "3. set adam optimizer\n",
    "4. Instantiate Encoder\n",
    "5. Instantiate Decoder\n",
    "6. Instantiate Loss function (optimize gradients)\n",
    "7. Define summaries\n",
    "8. Generate batches\n",
    "9. Run training over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T17:28:11.803717Z",
     "start_time": "2018-11-21T15:15:26.338302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac2467e240>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected/weights:0/grad/hist is illegal; using fully_connected/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected/weights:0/grad/sparsity is illegal; using fully_connected/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected/biases:0/grad/hist is illegal; using fully_connected/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected/biases:0/grad/sparsity is illegal; using fully_connected/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/weights:0/grad/hist is illegal; using fully_connected_1/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/weights:0/grad/sparsity is illegal; using fully_connected_1/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/biases:0/grad/hist is illegal; using fully_connected_1/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_1/biases:0/grad/sparsity is illegal; using fully_connected_1/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/weights:0/grad/hist is illegal; using fully_connected_2/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/weights:0/grad/sparsity is illegal; using fully_connected_2/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/biases:0/grad/hist is illegal; using fully_connected_2/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fully_connected_2/biases:0/grad/sparsity is illegal; using fully_connected_2/biases_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/kernel:0/grad/hist is illegal; using lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/kernel:0/grad/sparsity is illegal; using lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/bias:0/grad/hist is illegal; using lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_cell/bias:0/grad/sparsity is illegal; using lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/weights:0/grad/hist is illegal; using decoder_output_to_logits/fully_connected/weights_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/weights:0/grad/sparsity is illegal; using decoder_output_to_logits/fully_connected/weights_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/biases:0/grad/hist is illegal; using decoder_output_to_logits/fully_connected/biases_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name decoder_output_to_logits/fully_connected/biases:0/grad/sparsity is illegal; using decoder_output_to_logits/fully_connected/biases_0/grad/sparsity instead.\n",
      "Writing to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331\n",
      "\n",
      "2018-11-21T16:15:59.993157: step 101, loss 1.02969, latentLoss: 940.388, reconstructionLoss: 1.02028, acc 0.694687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:16:00.357662: step 101, loss 1.02319, klDiv: 886.665, CE-loss: 1.01433, acc 0.700938\n",
      "\n",
      "2018-11-21T16:16:21.642229: step 201, loss 0.811808, latentLoss: 867.661, reconstructionLoss: 0.803131, acc 0.748438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:16:21.658794: step 201, loss 0.811611, klDiv: 905.44, CE-loss: 0.802556, acc 0.749375\n",
      "\n",
      "2018-11-21T16:16:42.421823: step 301, loss 0.706512, latentLoss: 823.709, reconstructionLoss: 0.698275, acc 0.77625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:16:42.430969: step 301, loss 0.700479, klDiv: 830.517, CE-loss: 0.692173, acc 0.782188\n",
      "\n",
      "2018-11-21T16:17:02.424110: step 401, loss 0.57436, latentLoss: 732.374, reconstructionLoss: 0.567037, acc 0.815625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:17:02.438396: step 401, loss 0.572458, klDiv: 741.59, CE-loss: 0.565042, acc 0.817187\n",
      "\n",
      "2018-11-21T16:17:21.704109: step 501, loss 0.531989, latentLoss: 795.657, reconstructionLoss: 0.524033, acc 0.833438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:17:21.716243: step 501, loss 0.525918, klDiv: 770.214, CE-loss: 0.518216, acc 0.830625\n",
      "\n",
      "2018-11-21T16:17:40.318290: step 601, loss 0.501137, latentLoss: 793.697, reconstructionLoss: 0.4932, acc 0.845625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:17:40.329836: step 601, loss 0.492907, klDiv: 774.368, CE-loss: 0.485163, acc 0.845937\n",
      "\n",
      "2018-11-21T16:17:58.650739: step 701, loss 0.524532, latentLoss: 758.064, reconstructionLoss: 0.516952, acc 0.835625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:17:58.668431: step 701, loss 0.519822, klDiv: 741.063, CE-loss: 0.512412, acc 0.84125\n",
      "\n",
      "2018-11-21T16:18:16.562005: step 801, loss 0.478379, latentLoss: 800.943, reconstructionLoss: 0.47037, acc 0.842812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:18:16.573582: step 801, loss 0.466779, klDiv: 791.046, CE-loss: 0.458868, acc 0.85\n",
      "\n",
      "2018-11-21T16:18:34.165770: step 901, loss 0.431765, latentLoss: 743.629, reconstructionLoss: 0.424329, acc 0.85875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:18:34.176089: step 901, loss 0.419379, klDiv: 746.552, CE-loss: 0.411914, acc 0.866875\n",
      "\n",
      "2018-11-21T16:18:51.487079: step 1001, loss 0.401635, latentLoss: 735.31, reconstructionLoss: 0.394282, acc 0.872813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:18:51.499083: step 1001, loss 0.385641, klDiv: 750.237, CE-loss: 0.378139, acc 0.88\n",
      "\n",
      "2018-11-21T16:19:08.602638: step 1101, loss 0.396962, latentLoss: 720.076, reconstructionLoss: 0.389761, acc 0.870625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:19:08.619150: step 1101, loss 0.37618, klDiv: 718.434, CE-loss: 0.368996, acc 0.874687\n",
      "\n",
      "2018-11-21T16:19:25.719676: step 1201, loss 0.377306, latentLoss: 785.587, reconstructionLoss: 0.36945, acc 0.875938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:19:25.732689: step 1201, loss 0.363144, klDiv: 792.579, CE-loss: 0.355218, acc 0.883125\n",
      "\n",
      "2018-11-21T16:19:42.735807: step 1301, loss 0.345733, latentLoss: 803.111, reconstructionLoss: 0.337702, acc 0.885938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:19:42.747532: step 1301, loss 0.338392, klDiv: 765.479, CE-loss: 0.330738, acc 0.886875\n",
      "\n",
      "2018-11-21T16:19:59.567777: step 1401, loss 0.349595, latentLoss: 763.66, reconstructionLoss: 0.341958, acc 0.881875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:19:59.579453: step 1401, loss 0.337573, klDiv: 773.479, CE-loss: 0.329838, acc 0.890625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T16:20:16.364127: step 1501, loss 0.32642, latentLoss: 771.36, reconstructionLoss: 0.318706, acc 0.892187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:20:16.378448: step 1501, loss 0.322542, klDiv: 766.123, CE-loss: 0.314881, acc 0.894375\n",
      "\n",
      "2018-11-21T16:20:33.086793: step 1601, loss 0.306876, latentLoss: 759.483, reconstructionLoss: 0.299282, acc 0.900937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:20:33.103466: step 1601, loss 0.29961, klDiv: 764.292, CE-loss: 0.291967, acc 0.904688\n",
      "\n",
      "2018-11-21T16:20:49.696861: step 1701, loss 0.309802, latentLoss: 769.577, reconstructionLoss: 0.302107, acc 0.895\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:20:49.710649: step 1701, loss 0.289741, klDiv: 777.01, CE-loss: 0.281971, acc 0.90625\n",
      "\n",
      "2018-11-21T16:21:06.280753: step 1801, loss 0.29437, latentLoss: 741.794, reconstructionLoss: 0.286952, acc 0.898125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:21:06.297168: step 1801, loss 0.274604, klDiv: 736.772, CE-loss: 0.267237, acc 0.905937\n",
      "\n",
      "2018-11-21T16:21:22.823052: step 1901, loss 0.267026, latentLoss: 735.651, reconstructionLoss: 0.25967, acc 0.912188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:21:22.837310: step 1901, loss 0.256379, klDiv: 731.565, CE-loss: 0.249064, acc 0.915\n",
      "\n",
      "2018-11-21T16:21:39.290362: step 2001, loss 0.262453, latentLoss: 722.557, reconstructionLoss: 0.255228, acc 0.90875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:21:39.306028: step 2001, loss 0.251709, klDiv: 723.501, CE-loss: 0.244474, acc 0.919688\n",
      "\n",
      "2018-11-21T16:21:55.789517: step 2101, loss 0.236766, latentLoss: 683.707, reconstructionLoss: 0.229929, acc 0.920313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:21:55.809634: step 2101, loss 0.229828, klDiv: 677.652, CE-loss: 0.223051, acc 0.921562\n",
      "\n",
      "2018-11-21T16:22:12.224832: step 2201, loss 0.235912, latentLoss: 709.619, reconstructionLoss: 0.228816, acc 0.92125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:22:12.238818: step 2201, loss 0.221672, klDiv: 706.266, CE-loss: 0.214609, acc 0.930625\n",
      "\n",
      "2018-11-21T16:22:28.631629: step 2301, loss 0.257591, latentLoss: 745.23, reconstructionLoss: 0.250138, acc 0.915313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:22:28.640483: step 2301, loss 0.244014, klDiv: 748.46, CE-loss: 0.23653, acc 0.921875\n",
      "\n",
      "2018-11-21T16:22:44.982156: step 2401, loss 0.266089, latentLoss: 795.369, reconstructionLoss: 0.258136, acc 0.9075\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:22:44.994019: step 2401, loss 0.248956, klDiv: 794.325, CE-loss: 0.241013, acc 0.9175\n",
      "\n",
      "2018-11-21T16:23:01.251430: step 2501, loss 0.199761, latentLoss: 697.732, reconstructionLoss: 0.192784, acc 0.936562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:23:01.262907: step 2501, loss 0.19473, klDiv: 704.783, CE-loss: 0.187682, acc 0.937813\n",
      "\n",
      "2018-11-21T16:23:17.476038: step 2601, loss 0.254209, latentLoss: 736.502, reconstructionLoss: 0.246844, acc 0.919375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:23:17.492380: step 2601, loss 0.228943, klDiv: 737.741, CE-loss: 0.221565, acc 0.9275\n",
      "\n",
      "2018-11-21T16:23:33.782310: step 2701, loss 0.216966, latentLoss: 699.255, reconstructionLoss: 0.209974, acc 0.925937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:23:33.796294: step 2701, loss 0.205812, klDiv: 702.519, CE-loss: 0.198787, acc 0.935938\n",
      "\n",
      "2018-11-21T16:23:50.065434: step 2801, loss 0.188514, latentLoss: 709.773, reconstructionLoss: 0.181416, acc 0.938125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:23:50.074595: step 2801, loss 0.176338, klDiv: 710.6, CE-loss: 0.169232, acc 0.94375\n",
      "\n",
      "2018-11-21T16:24:06.311851: step 2901, loss 0.230703, latentLoss: 727.367, reconstructionLoss: 0.223429, acc 0.927813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:24:06.326335: step 2901, loss 0.217562, klDiv: 724.506, CE-loss: 0.210317, acc 0.929062\n",
      "\n",
      "2018-11-21T16:24:22.627557: step 3001, loss 0.209141, latentLoss: 755.577, reconstructionLoss: 0.201585, acc 0.931875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:24:22.637764: step 3001, loss 0.199609, klDiv: 751.251, CE-loss: 0.192097, acc 0.938125\n",
      "\n",
      "2018-11-21T16:24:38.794034: step 3101, loss 0.181752, latentLoss: 710.676, reconstructionLoss: 0.174645, acc 0.947812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:24:38.807480: step 3101, loss 0.176883, klDiv: 719.381, CE-loss: 0.169689, acc 0.949063\n",
      "\n",
      "2018-11-21T16:24:55.000839: step 3201, loss 0.184456, latentLoss: 732.938, reconstructionLoss: 0.177126, acc 0.94125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:24:55.019009: step 3201, loss 0.165954, klDiv: 722.767, CE-loss: 0.158726, acc 0.951563\n",
      "\n",
      "2018-11-21T16:25:11.121185: step 3301, loss 0.227893, latentLoss: 711.601, reconstructionLoss: 0.220777, acc 0.925625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:25:11.132547: step 3301, loss 0.205513, klDiv: 702.897, CE-loss: 0.198484, acc 0.935313\n",
      "\n",
      "2018-11-21T16:25:27.256550: step 3401, loss 0.188249, latentLoss: 703.115, reconstructionLoss: 0.181217, acc 0.93375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:25:27.266215: step 3401, loss 0.154536, klDiv: 706.312, CE-loss: 0.147473, acc 0.948438\n",
      "\n",
      "2018-11-21T16:25:43.363912: step 3501, loss 0.169312, latentLoss: 674.552, reconstructionLoss: 0.162566, acc 0.9525\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:25:43.376329: step 3501, loss 0.157056, klDiv: 670.251, CE-loss: 0.150354, acc 0.950625\n",
      "\n",
      "2018-11-21T16:25:59.514516: step 3601, loss 0.181342, latentLoss: 664.696, reconstructionLoss: 0.174695, acc 0.938125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:25:59.523494: step 3601, loss 0.160788, klDiv: 665.484, CE-loss: 0.154133, acc 0.947187\n",
      "\n",
      "2018-11-21T16:26:15.559542: step 3701, loss 0.193153, latentLoss: 705.639, reconstructionLoss: 0.186096, acc 0.94125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:26:15.570635: step 3701, loss 0.179059, klDiv: 706.448, CE-loss: 0.171995, acc 0.945625\n",
      "\n",
      "2018-11-21T16:26:31.605701: step 3801, loss 0.157833, latentLoss: 668.01, reconstructionLoss: 0.151153, acc 0.946875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:26:31.616418: step 3801, loss 0.153281, klDiv: 667.876, CE-loss: 0.146602, acc 0.951563\n",
      "\n",
      "2018-11-21T16:26:47.771734: step 3901, loss 0.147019, latentLoss: 709.075, reconstructionLoss: 0.139928, acc 0.949375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:26:47.784329: step 3901, loss 0.129819, klDiv: 708.396, CE-loss: 0.122735, acc 0.959375\n",
      "\n",
      "2018-11-21T16:27:03.817254: step 4001, loss 0.17623, latentLoss: 673.242, reconstructionLoss: 0.169498, acc 0.945938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:27:03.835506: step 4001, loss 0.171935, klDiv: 678.85, CE-loss: 0.165146, acc 0.943438\n",
      "\n",
      "2018-11-21T16:27:19.926348: step 4101, loss 0.157496, latentLoss: 668.563, reconstructionLoss: 0.15081, acc 0.947187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:27:19.941346: step 4101, loss 0.144634, klDiv: 672.071, CE-loss: 0.137913, acc 0.954375\n",
      "\n",
      "2018-11-21T16:27:35.973012: step 4201, loss 0.149391, latentLoss: 670.67, reconstructionLoss: 0.142684, acc 0.949687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:27:35.988025: step 4201, loss 0.133392, klDiv: 672.298, CE-loss: 0.126669, acc 0.958125\n",
      "\n",
      "2018-11-21T16:27:52.008176: step 4301, loss 0.148628, latentLoss: 672.404, reconstructionLoss: 0.141904, acc 0.950312\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:27:52.020141: step 4301, loss 0.132501, klDiv: 666.164, CE-loss: 0.125839, acc 0.956875\n",
      "\n",
      "2018-11-21T16:28:08.032509: step 4401, loss 0.135224, latentLoss: 678.553, reconstructionLoss: 0.128438, acc 0.95875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:28:08.048629: step 4401, loss 0.126612, klDiv: 672.887, CE-loss: 0.119883, acc 0.960938\n",
      "\n",
      "2018-11-21T16:28:24.127149: step 4501, loss 0.127329, latentLoss: 629.542, reconstructionLoss: 0.121033, acc 0.960312\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:28:24.136424: step 4501, loss 0.115429, klDiv: 623.238, CE-loss: 0.109197, acc 0.964688\n",
      "\n",
      "2018-11-21T16:28:40.038920: step 4601, loss 0.158352, latentLoss: 699.392, reconstructionLoss: 0.151358, acc 0.9475\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:28:40.054680: step 4601, loss 0.14881, klDiv: 696.992, CE-loss: 0.14184, acc 0.950625\n",
      "\n",
      "2018-11-21T16:28:56.094560: step 4701, loss 0.137743, latentLoss: 662.771, reconstructionLoss: 0.131116, acc 0.95375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:28:56.106110: step 4701, loss 0.129661, klDiv: 659.498, CE-loss: 0.123066, acc 0.959687\n",
      "\n",
      "2018-11-21T16:29:12.106716: step 4801, loss 0.146361, latentLoss: 692.482, reconstructionLoss: 0.139436, acc 0.952187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:29:12.123732: step 4801, loss 0.134619, klDiv: 686.844, CE-loss: 0.12775, acc 0.9575\n",
      "\n",
      "2018-11-21T16:29:28.207523: step 4901, loss 0.145649, latentLoss: 663.331, reconstructionLoss: 0.139015, acc 0.954063\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:29:28.216860: step 4901, loss 0.135674, klDiv: 668.121, CE-loss: 0.128993, acc 0.959375\n",
      "\n",
      "2018-11-21T16:29:44.166963: step 5001, loss 0.108428, latentLoss: 657.124, reconstructionLoss: 0.101857, acc 0.966563\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:29:44.176074: step 5001, loss 0.0984122, klDiv: 651.116, CE-loss: 0.0919011, acc 0.970625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-5000\n",
      "\n",
      "2018-11-21T16:30:00.683778: step 5101, loss 0.125073, latentLoss: 681.057, reconstructionLoss: 0.118262, acc 0.959375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:30:00.695056: step 5101, loss 0.11944, klDiv: 673.486, CE-loss: 0.112705, acc 0.961563\n",
      "\n",
      "2018-11-21T16:30:16.611417: step 5201, loss 0.11876, latentLoss: 674.292, reconstructionLoss: 0.112017, acc 0.962812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:30:16.624710: step 5201, loss 0.109415, klDiv: 678.153, CE-loss: 0.102633, acc 0.9675\n",
      "\n",
      "2018-11-21T16:30:32.566839: step 5301, loss 0.124348, latentLoss: 676.501, reconstructionLoss: 0.117583, acc 0.961875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:30:32.580599: step 5301, loss 0.117604, klDiv: 673.193, CE-loss: 0.110872, acc 0.964375\n",
      "\n",
      "2018-11-21T16:30:48.481325: step 5401, loss 0.123392, latentLoss: 658.309, reconstructionLoss: 0.116808, acc 0.960625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:30:48.490213: step 5401, loss 0.116591, klDiv: 653.409, CE-loss: 0.110057, acc 0.964688\n",
      "\n",
      "2018-11-21T16:31:04.489024: step 5501, loss 0.128284, latentLoss: 655.286, reconstructionLoss: 0.121731, acc 0.957812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:31:04.499662: step 5501, loss 0.118346, klDiv: 657.59, CE-loss: 0.11177, acc 0.961563\n",
      "\n",
      "2018-11-21T16:31:20.430954: step 5601, loss 0.142401, latentLoss: 633.957, reconstructionLoss: 0.136062, acc 0.956563\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:31:20.439894: step 5601, loss 0.12071, klDiv: 646.525, CE-loss: 0.114245, acc 0.962188\n",
      "\n",
      "2018-11-21T16:31:36.385445: step 5701, loss 0.109226, latentLoss: 622.943, reconstructionLoss: 0.102996, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:31:36.394238: step 5701, loss 0.10446, klDiv: 621.931, CE-loss: 0.098241, acc 0.969375\n",
      "\n",
      "2018-11-21T16:31:52.302420: step 5801, loss 0.12225, latentLoss: 637.032, reconstructionLoss: 0.11588, acc 0.96375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:31:52.318594: step 5801, loss 0.111486, klDiv: 632.373, CE-loss: 0.105163, acc 0.96375\n",
      "\n",
      "2018-11-21T16:32:08.177428: step 5901, loss 0.105486, latentLoss: 629.056, reconstructionLoss: 0.0991951, acc 0.964688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:32:08.188656: step 5901, loss 0.098087, klDiv: 631.463, CE-loss: 0.0917724, acc 0.96875\n",
      "\n",
      "2018-11-21T16:32:24.145686: step 6001, loss 0.084907, latentLoss: 645.38, reconstructionLoss: 0.0784532, acc 0.974063\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:32:24.157141: step 6001, loss 0.0756233, klDiv: 642.077, CE-loss: 0.0692026, acc 0.977188\n",
      "\n",
      "2018-11-21T16:32:40.010806: step 6101, loss 0.104949, latentLoss: 638.132, reconstructionLoss: 0.0985677, acc 0.966875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:32:40.028244: step 6101, loss 0.092701, klDiv: 632.963, CE-loss: 0.0863714, acc 0.970312\n",
      "\n",
      "2018-11-21T16:32:55.919457: step 6201, loss 0.104278, latentLoss: 626.139, reconstructionLoss: 0.0980164, acc 0.9675\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:32:55.934726: step 6201, loss 0.101802, klDiv: 614.432, CE-loss: 0.0956579, acc 0.969688\n",
      "\n",
      "2018-11-21T16:33:11.897496: step 6301, loss 0.112213, latentLoss: 659.61, reconstructionLoss: 0.105617, acc 0.958438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:33:11.910533: step 6301, loss 0.0967105, klDiv: 653.755, CE-loss: 0.0901729, acc 0.968125\n",
      "\n",
      "2018-11-21T16:33:27.835928: step 6401, loss 0.102992, latentLoss: 651.241, reconstructionLoss: 0.0964796, acc 0.9625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:33:27.844885: step 6401, loss 0.0861526, klDiv: 650.053, CE-loss: 0.0796521, acc 0.973125\n",
      "\n",
      "2018-11-21T16:33:43.724454: step 6501, loss 0.105792, latentLoss: 630.146, reconstructionLoss: 0.0994904, acc 0.966875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:33:43.735888: step 6501, loss 0.0960206, klDiv: 627.997, CE-loss: 0.0897407, acc 0.970937\n",
      "\n",
      "2018-11-21T16:33:59.602150: step 6601, loss 0.125359, latentLoss: 647.521, reconstructionLoss: 0.118883, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:33:59.617112: step 6601, loss 0.11308, klDiv: 644.49, CE-loss: 0.106635, acc 0.962812\n",
      "\n",
      "2018-11-21T16:34:15.440030: step 6701, loss 0.117645, latentLoss: 625.075, reconstructionLoss: 0.111395, acc 0.960625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:34:15.454104: step 6701, loss 0.0995687, klDiv: 622.758, CE-loss: 0.0933412, acc 0.973125\n",
      "\n",
      "2018-11-21T16:34:31.359327: step 6801, loss 0.107422, latentLoss: 617.156, reconstructionLoss: 0.10125, acc 0.965625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:34:31.368697: step 6801, loss 0.0967497, klDiv: 620.268, CE-loss: 0.0905471, acc 0.971563\n",
      "\n",
      "2018-11-21T16:34:47.272854: step 6901, loss 0.119343, latentLoss: 620.562, reconstructionLoss: 0.113137, acc 0.962188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:34:47.286700: step 6901, loss 0.111403, klDiv: 620.109, CE-loss: 0.105202, acc 0.964063\n",
      "\n",
      "2018-11-21T16:35:03.164444: step 7001, loss 0.103998, latentLoss: 628.42, reconstructionLoss: 0.0977138, acc 0.97125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:35:03.179175: step 7001, loss 0.0856981, klDiv: 626.079, CE-loss: 0.0794373, acc 0.973437\n",
      "\n",
      "2018-11-21T16:35:18.955227: step 7101, loss 0.0972246, latentLoss: 619.578, reconstructionLoss: 0.0910288, acc 0.965937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:35:18.964355: step 7101, loss 0.0821553, klDiv: 617.07, CE-loss: 0.0759846, acc 0.973125\n",
      "\n",
      "2018-11-21T16:35:34.802496: step 7201, loss 0.0988613, latentLoss: 628.27, reconstructionLoss: 0.0925786, acc 0.970625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:35:34.814027: step 7201, loss 0.0914537, klDiv: 630.482, CE-loss: 0.0851488, acc 0.9725\n",
      "\n",
      "2018-11-21T16:35:50.708326: step 7301, loss 0.0936359, latentLoss: 587.264, reconstructionLoss: 0.0877633, acc 0.970312\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:35:50.718041: step 7301, loss 0.0763011, klDiv: 592.172, CE-loss: 0.0703794, acc 0.975937\n",
      "\n",
      "2018-11-21T16:36:06.551456: step 7401, loss 0.0810925, latentLoss: 590.114, reconstructionLoss: 0.0751913, acc 0.973437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:36:06.564905: step 7401, loss 0.0724326, klDiv: 590.101, CE-loss: 0.0665316, acc 0.979688\n",
      "\n",
      "2018-11-21T16:36:22.476528: step 7501, loss 0.113377, latentLoss: 595.816, reconstructionLoss: 0.107418, acc 0.966563\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:36:22.487177: step 7501, loss 0.0885614, klDiv: 600.381, CE-loss: 0.0825576, acc 0.975\n",
      "\n",
      "2018-11-21T16:36:38.310238: step 7601, loss 0.110318, latentLoss: 638.69, reconstructionLoss: 0.103931, acc 0.963437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:36:38.324715: step 7601, loss 0.0942735, klDiv: 643.832, CE-loss: 0.0878352, acc 0.970625\n",
      "\n",
      "2018-11-21T16:36:54.153879: step 7701, loss 0.0972393, latentLoss: 582.091, reconstructionLoss: 0.0914184, acc 0.9675\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:36:54.164527: step 7701, loss 0.0882388, klDiv: 580.954, CE-loss: 0.0824292, acc 0.971875\n",
      "\n",
      "2018-11-21T16:37:10.159675: step 7801, loss 0.0842204, latentLoss: 590.733, reconstructionLoss: 0.0783131, acc 0.97125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:37:10.169749: step 7801, loss 0.0769218, klDiv: 585.777, CE-loss: 0.071064, acc 0.976562\n",
      "\n",
      "2018-11-21T16:37:26.036500: step 7901, loss 0.13326, latentLoss: 652.63, reconstructionLoss: 0.126734, acc 0.958438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:37:26.050443: step 7901, loss 0.115048, klDiv: 649.4, CE-loss: 0.108554, acc 0.963125\n",
      "\n",
      "2018-11-21T16:37:41.906009: step 8001, loss 0.102474, latentLoss: 595.793, reconstructionLoss: 0.0965161, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:37:41.921413: step 8001, loss 0.0887607, klDiv: 596.525, CE-loss: 0.0827955, acc 0.972812\n",
      "\n",
      "2018-11-21T16:37:57.766744: step 8101, loss 0.0783147, latentLoss: 604.362, reconstructionLoss: 0.0722711, acc 0.977188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:37:57.778659: step 8101, loss 0.068558, klDiv: 605.045, CE-loss: 0.0625076, acc 0.98125\n",
      "\n",
      "2018-11-21T16:38:13.639472: step 8201, loss 0.0811666, latentLoss: 594.777, reconstructionLoss: 0.0752189, acc 0.976875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:38:13.654052: step 8201, loss 0.0674382, klDiv: 593.407, CE-loss: 0.0615041, acc 0.980937\n",
      "\n",
      "2018-11-21T16:38:29.462441: step 8301, loss 0.0787669, latentLoss: 585.194, reconstructionLoss: 0.072915, acc 0.972812\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:38:29.475386: step 8301, loss 0.0639445, klDiv: 590.538, CE-loss: 0.0580391, acc 0.98125\n",
      "\n",
      "2018-11-21T16:38:45.287030: step 8401, loss 0.081896, latentLoss: 577.418, reconstructionLoss: 0.0761218, acc 0.974688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:38:45.298421: step 8401, loss 0.0809811, klDiv: 577.909, CE-loss: 0.075202, acc 0.975\n",
      "\n",
      "2018-11-21T16:39:01.121789: step 8501, loss 0.0901856, latentLoss: 609.765, reconstructionLoss: 0.0840879, acc 0.970312\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:39:01.139580: step 8501, loss 0.0738517, klDiv: 613.414, CE-loss: 0.0677176, acc 0.978437\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T16:39:16.919304: step 8601, loss 0.0777792, latentLoss: 595.371, reconstructionLoss: 0.0718254, acc 0.975312\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:39:16.928865: step 8601, loss 0.0660094, klDiv: 594.918, CE-loss: 0.0600602, acc 0.979688\n",
      "\n",
      "2018-11-21T16:39:32.739754: step 8701, loss 0.0836766, latentLoss: 582.154, reconstructionLoss: 0.0778551, acc 0.973437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:39:32.749445: step 8701, loss 0.0766376, klDiv: 581.863, CE-loss: 0.0708189, acc 0.977813\n",
      "\n",
      "2018-11-21T16:39:48.561852: step 8801, loss 0.0623446, latentLoss: 566.272, reconstructionLoss: 0.0566819, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:39:48.575333: step 8801, loss 0.0555499, klDiv: 564, CE-loss: 0.0499099, acc 0.984062\n",
      "\n",
      "2018-11-21T16:40:04.343917: step 8901, loss 0.0908148, latentLoss: 588.216, reconstructionLoss: 0.0849326, acc 0.966563\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:40:04.360443: step 8901, loss 0.0798287, klDiv: 586.823, CE-loss: 0.0739605, acc 0.972812\n",
      "\n",
      "2018-11-21T16:40:20.154925: step 9001, loss 0.0759545, latentLoss: 572.109, reconstructionLoss: 0.0702334, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:40:20.170168: step 9001, loss 0.0660264, klDiv: 569.474, CE-loss: 0.0603317, acc 0.980313\n",
      "\n",
      "2018-11-21T16:40:35.965766: step 9101, loss 0.0940309, latentLoss: 597.703, reconstructionLoss: 0.0880538, acc 0.968125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:40:35.983992: step 9101, loss 0.085055, klDiv: 598.62, CE-loss: 0.0790688, acc 0.971875\n",
      "\n",
      "2018-11-21T16:40:51.781336: step 9201, loss 0.076877, latentLoss: 571.823, reconstructionLoss: 0.0711587, acc 0.9775\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:40:51.792634: step 9201, loss 0.0663128, klDiv: 569.583, CE-loss: 0.0606169, acc 0.981875\n",
      "\n",
      "2018-11-21T16:41:07.607428: step 9301, loss 0.0828003, latentLoss: 572.508, reconstructionLoss: 0.0770752, acc 0.973437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:41:07.618774: step 9301, loss 0.0736575, klDiv: 572.34, CE-loss: 0.0679341, acc 0.975\n",
      "\n",
      "2018-11-21T16:41:23.300704: step 9401, loss 0.0765634, latentLoss: 570.28, reconstructionLoss: 0.0708606, acc 0.976875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:41:23.312244: step 9401, loss 0.0682793, klDiv: 567.285, CE-loss: 0.0626064, acc 0.97875\n",
      "\n",
      "2018-11-21T16:41:39.173010: step 9501, loss 0.0748704, latentLoss: 568.912, reconstructionLoss: 0.0691813, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:41:39.189495: step 9501, loss 0.0710417, klDiv: 565.867, CE-loss: 0.0653831, acc 0.978125\n",
      "\n",
      "2018-11-21T16:41:55.021013: step 9601, loss 0.0675762, latentLoss: 554.638, reconstructionLoss: 0.0620298, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:41:55.041338: step 9601, loss 0.0669525, klDiv: 554.374, CE-loss: 0.0614087, acc 0.97875\n",
      "\n",
      "2018-11-21T16:42:10.821125: step 9701, loss 0.0944204, latentLoss: 581.597, reconstructionLoss: 0.0886044, acc 0.969375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:42:10.839666: step 9701, loss 0.0837347, klDiv: 580.667, CE-loss: 0.077928, acc 0.974063\n",
      "\n",
      "2018-11-21T16:42:26.661685: step 9801, loss 0.0910524, latentLoss: 567.901, reconstructionLoss: 0.0853734, acc 0.970937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:42:26.677087: step 9801, loss 0.0836077, klDiv: 570.292, CE-loss: 0.0779047, acc 0.9725\n",
      "\n",
      "2018-11-21T16:42:42.470525: step 9901, loss 0.0766239, latentLoss: 568.389, reconstructionLoss: 0.07094, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:42:42.481997: step 9901, loss 0.0635262, klDiv: 566.712, CE-loss: 0.0578591, acc 0.9825\n",
      "\n",
      "2018-11-21T16:42:58.288961: step 10001, loss 0.0850773, latentLoss: 585.443, reconstructionLoss: 0.0792229, acc 0.972188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:42:58.304116: step 10001, loss 0.0607935, klDiv: 589.009, CE-loss: 0.0549034, acc 0.981875\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-10000\n",
      "\n",
      "2018-11-21T16:43:14.617249: step 10101, loss 0.0642091, latentLoss: 571.272, reconstructionLoss: 0.0584964, acc 0.978125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:43:14.627187: step 10101, loss 0.050914, klDiv: 564.415, CE-loss: 0.0452698, acc 0.985313\n",
      "\n",
      "2018-11-21T16:43:30.331347: step 10201, loss 0.0698994, latentLoss: 561.93, reconstructionLoss: 0.0642801, acc 0.977813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:43:30.342878: step 10201, loss 0.0572577, klDiv: 559.878, CE-loss: 0.0516589, acc 0.982813\n",
      "\n",
      "2018-11-21T16:43:46.079948: step 10301, loss 0.0801877, latentLoss: 541.504, reconstructionLoss: 0.0747727, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:43:46.097536: step 10301, loss 0.0690017, klDiv: 544.778, CE-loss: 0.0635539, acc 0.979688\n",
      "\n",
      "2018-11-21T16:44:01.898302: step 10401, loss 0.0737836, latentLoss: 549.298, reconstructionLoss: 0.0682906, acc 0.979688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:44:01.916010: step 10401, loss 0.0676405, klDiv: 549.283, CE-loss: 0.0621477, acc 0.98\n",
      "\n",
      "2018-11-21T16:44:17.724803: step 10501, loss 0.0654989, latentLoss: 525.589, reconstructionLoss: 0.060243, acc 0.978437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:44:17.740570: step 10501, loss 0.0612289, klDiv: 525.431, CE-loss: 0.0559746, acc 0.9825\n",
      "\n",
      "2018-11-21T16:44:33.584794: step 10601, loss 0.0517897, latentLoss: 538.873, reconstructionLoss: 0.046401, acc 0.985625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:44:33.601298: step 10601, loss 0.0419209, klDiv: 540.279, CE-loss: 0.0365181, acc 0.989375\n",
      "\n",
      "2018-11-21T16:44:49.407490: step 10701, loss 0.0685882, latentLoss: 546.121, reconstructionLoss: 0.063127, acc 0.979062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:44:49.423726: step 10701, loss 0.0538472, klDiv: 548.178, CE-loss: 0.0483654, acc 0.984062\n",
      "\n",
      "2018-11-21T16:45:05.231892: step 10801, loss 0.0651357, latentLoss: 517.006, reconstructionLoss: 0.0599656, acc 0.978437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:45:05.245929: step 10801, loss 0.0544101, klDiv: 521.005, CE-loss: 0.0492, acc 0.983437\n",
      "\n",
      "2018-11-21T16:45:21.075952: step 10901, loss 0.0634503, latentLoss: 552.474, reconstructionLoss: 0.0579256, acc 0.981562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:45:21.092891: step 10901, loss 0.0543246, klDiv: 554.78, CE-loss: 0.0487768, acc 0.985313\n",
      "\n",
      "2018-11-21T16:45:36.750345: step 11001, loss 0.0612388, latentLoss: 538.483, reconstructionLoss: 0.055854, acc 0.981562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:45:36.765137: step 11001, loss 0.0499841, klDiv: 539.678, CE-loss: 0.0445873, acc 0.9875\n",
      "\n",
      "2018-11-21T16:45:52.464796: step 11101, loss 0.0669789, latentLoss: 547.661, reconstructionLoss: 0.0615023, acc 0.979688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:45:52.474860: step 11101, loss 0.0569905, klDiv: 548.884, CE-loss: 0.0515017, acc 0.984062\n",
      "\n",
      "2018-11-21T16:46:08.278622: step 11201, loss 0.0586623, latentLoss: 541.052, reconstructionLoss: 0.0532518, acc 0.983125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:46:08.296102: step 11201, loss 0.0486369, klDiv: 533.902, CE-loss: 0.0432979, acc 0.987813\n",
      "\n",
      "2018-11-21T16:46:24.119995: step 11301, loss 0.0696029, latentLoss: 556.082, reconstructionLoss: 0.064042, acc 0.978437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:46:24.136256: step 11301, loss 0.0564935, klDiv: 554.882, CE-loss: 0.0509447, acc 0.981562\n",
      "\n",
      "2018-11-21T16:46:39.920788: step 11401, loss 0.057801, latentLoss: 555.442, reconstructionLoss: 0.0522466, acc 0.982188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:46:39.935384: step 11401, loss 0.0528772, klDiv: 556.165, CE-loss: 0.0473155, acc 0.985313\n",
      "\n",
      "2018-11-21T16:46:55.684786: step 11501, loss 0.0571263, latentLoss: 525.645, reconstructionLoss: 0.0518699, acc 0.982813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:46:55.697286: step 11501, loss 0.0521598, klDiv: 527.23, CE-loss: 0.0468875, acc 0.982813\n",
      "\n",
      "2018-11-21T16:47:11.791703: step 11601, loss 0.0647053, latentLoss: 539.222, reconstructionLoss: 0.0593131, acc 0.982188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:47:11.808645: step 11601, loss 0.0579366, klDiv: 540.827, CE-loss: 0.0525284, acc 0.982188\n",
      "\n",
      "2018-11-21T16:47:27.604396: step 11701, loss 0.0741402, latentLoss: 553.012, reconstructionLoss: 0.06861, acc 0.977188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:47:27.620991: step 11701, loss 0.0610812, klDiv: 551.97, CE-loss: 0.0555615, acc 0.980937\n",
      "\n",
      "2018-11-21T16:47:43.489458: step 11801, loss 0.0753438, latentLoss: 536.024, reconstructionLoss: 0.0699836, acc 0.975937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:47:43.507464: step 11801, loss 0.0627884, klDiv: 541.994, CE-loss: 0.0573684, acc 0.98125\n",
      "\n",
      "2018-11-21T16:47:59.346991: step 11901, loss 0.0751458, latentLoss: 524.04, reconstructionLoss: 0.0699054, acc 0.975937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:47:59.364322: step 11901, loss 0.0655689, klDiv: 522.759, CE-loss: 0.0603413, acc 0.98\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T16:48:15.186110: step 12001, loss 0.0606165, latentLoss: 526.173, reconstructionLoss: 0.0553548, acc 0.983125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:48:15.204445: step 12001, loss 0.0553451, klDiv: 522.86, CE-loss: 0.0501165, acc 0.983125\n",
      "\n",
      "2018-11-21T16:48:30.941155: step 12101, loss 0.0606653, latentLoss: 566.529, reconstructionLoss: 0.055, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:48:30.952190: step 12101, loss 0.0550465, klDiv: 565.661, CE-loss: 0.0493899, acc 0.983125\n",
      "\n",
      "2018-11-21T16:48:46.766987: step 12201, loss 0.0587699, latentLoss: 547.127, reconstructionLoss: 0.0532986, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:48:46.778066: step 12201, loss 0.0486316, klDiv: 548.87, CE-loss: 0.0431429, acc 0.98625\n",
      "\n",
      "2018-11-21T16:49:02.525807: step 12301, loss 0.0708015, latentLoss: 542.182, reconstructionLoss: 0.0653797, acc 0.975625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:49:02.544475: step 12301, loss 0.056677, klDiv: 543.615, CE-loss: 0.0512408, acc 0.983125\n",
      "\n",
      "2018-11-21T16:49:18.360224: step 12401, loss 0.0583636, latentLoss: 529.993, reconstructionLoss: 0.0530637, acc 0.983125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:49:18.369499: step 12401, loss 0.0458216, klDiv: 529.053, CE-loss: 0.0405311, acc 0.988125\n",
      "\n",
      "2018-11-21T16:49:34.354305: step 12501, loss 0.0695511, latentLoss: 525.537, reconstructionLoss: 0.0642957, acc 0.978437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:49:34.365858: step 12501, loss 0.0535248, klDiv: 527.528, CE-loss: 0.0482496, acc 0.982813\n",
      "\n",
      "2018-11-21T16:49:50.248865: step 12601, loss 0.0702749, latentLoss: 531.855, reconstructionLoss: 0.0649564, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:49:50.260398: step 12601, loss 0.0576116, klDiv: 534.651, CE-loss: 0.0522651, acc 0.981562\n",
      "\n",
      "2018-11-21T16:50:06.093632: step 12701, loss 0.0538384, latentLoss: 537.463, reconstructionLoss: 0.0484637, acc 0.983437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:50:06.108229: step 12701, loss 0.0419067, klDiv: 538.407, CE-loss: 0.0365227, acc 0.987813\n",
      "\n",
      "2018-11-21T16:50:21.885788: step 12801, loss 0.0640327, latentLoss: 533.446, reconstructionLoss: 0.0586983, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:50:21.899619: step 12801, loss 0.0518251, klDiv: 538.798, CE-loss: 0.0464372, acc 0.984688\n",
      "\n",
      "2018-11-21T16:50:38.051352: step 12901, loss 0.0665431, latentLoss: 521.892, reconstructionLoss: 0.0613242, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:50:38.063020: step 12901, loss 0.0553318, klDiv: 525.759, CE-loss: 0.0500742, acc 0.984375\n",
      "\n",
      "2018-11-21T16:50:54.094224: step 13001, loss 0.0525281, latentLoss: 506.096, reconstructionLoss: 0.0474671, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:50:54.105253: step 13001, loss 0.0504936, klDiv: 505.292, CE-loss: 0.0454407, acc 0.985\n",
      "\n",
      "2018-11-21T16:51:09.856402: step 13101, loss 0.0694466, latentLoss: 536.379, reconstructionLoss: 0.0640828, acc 0.978125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:51:09.867957: step 13101, loss 0.0592774, klDiv: 532.269, CE-loss: 0.0539547, acc 0.982813\n",
      "\n",
      "2018-11-21T16:51:25.562464: step 13201, loss 0.0584843, latentLoss: 521.691, reconstructionLoss: 0.0532674, acc 0.982188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:51:25.573843: step 13201, loss 0.0532216, klDiv: 517.844, CE-loss: 0.0480431, acc 0.982813\n",
      "\n",
      "2018-11-21T16:51:41.309013: step 13301, loss 0.0592686, latentLoss: 517.714, reconstructionLoss: 0.0540914, acc 0.979375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:51:41.324090: step 13301, loss 0.0527651, klDiv: 516.792, CE-loss: 0.0475972, acc 0.981875\n",
      "\n",
      "2018-11-21T16:51:57.088574: step 13401, loss 0.0548991, latentLoss: 557.473, reconstructionLoss: 0.0493244, acc 0.979375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:51:57.099989: step 13401, loss 0.0470737, klDiv: 552.011, CE-loss: 0.0415536, acc 0.984062\n",
      "\n",
      "2018-11-21T16:52:12.807907: step 13501, loss 0.059603, latentLoss: 508.448, reconstructionLoss: 0.0545186, acc 0.982188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:52:12.821752: step 13501, loss 0.0581179, klDiv: 505.591, CE-loss: 0.053062, acc 0.980313\n",
      "\n",
      "2018-11-21T16:52:28.625030: step 13601, loss 0.0514131, latentLoss: 494.53, reconstructionLoss: 0.0464678, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:52:28.637538: step 13601, loss 0.0442471, klDiv: 494.601, CE-loss: 0.0393011, acc 0.986562\n",
      "\n",
      "2018-11-21T16:52:44.450200: step 13701, loss 0.0508404, latentLoss: 535.885, reconstructionLoss: 0.0454815, acc 0.984062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:52:44.462264: step 13701, loss 0.045464, klDiv: 531.474, CE-loss: 0.0401493, acc 0.987188\n",
      "\n",
      "2018-11-21T16:53:00.271755: step 13801, loss 0.0760899, latentLoss: 518.828, reconstructionLoss: 0.0709017, acc 0.977813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:53:00.285782: step 13801, loss 0.0665265, klDiv: 517.714, CE-loss: 0.0613494, acc 0.980937\n",
      "\n",
      "2018-11-21T16:53:16.124505: step 13901, loss 0.0512284, latentLoss: 489.375, reconstructionLoss: 0.0463346, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:53:16.142685: step 13901, loss 0.0425091, klDiv: 490.22, CE-loss: 0.0376069, acc 0.9875\n",
      "\n",
      "2018-11-21T16:53:31.819882: step 14001, loss 0.0744982, latentLoss: 518.385, reconstructionLoss: 0.0693143, acc 0.975937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:53:31.836812: step 14001, loss 0.0560797, klDiv: 520.301, CE-loss: 0.0508766, acc 0.982813\n",
      "\n",
      "2018-11-21T16:53:47.639341: step 14101, loss 0.0604376, latentLoss: 533.508, reconstructionLoss: 0.0551026, acc 0.980313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:53:47.650794: step 14101, loss 0.0501008, klDiv: 532.551, CE-loss: 0.0447753, acc 0.985313\n",
      "\n",
      "2018-11-21T16:54:03.380952: step 14201, loss 0.0489384, latentLoss: 513.279, reconstructionLoss: 0.0438056, acc 0.98375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:54:03.395965: step 14201, loss 0.0384775, klDiv: 516.132, CE-loss: 0.0333162, acc 0.988437\n",
      "\n",
      "2018-11-21T16:54:19.165925: step 14301, loss 0.0485907, latentLoss: 503.491, reconstructionLoss: 0.0435558, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:54:19.175211: step 14301, loss 0.0404195, klDiv: 506.036, CE-loss: 0.0353591, acc 0.988125\n",
      "\n",
      "2018-11-21T16:54:34.909204: step 14401, loss 0.0575374, latentLoss: 505.416, reconstructionLoss: 0.0524832, acc 0.98125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:54:34.923925: step 14401, loss 0.0492985, klDiv: 505.608, CE-loss: 0.0442424, acc 0.984688\n",
      "\n",
      "2018-11-21T16:54:50.652964: step 14501, loss 0.0463698, latentLoss: 531.271, reconstructionLoss: 0.0410571, acc 0.983437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:54:50.667505: step 14501, loss 0.0357511, klDiv: 532.726, CE-loss: 0.0304239, acc 0.99\n",
      "\n",
      "2018-11-21T16:55:06.320590: step 14601, loss 0.0517163, latentLoss: 522.742, reconstructionLoss: 0.0464888, acc 0.980625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:55:06.334385: step 14601, loss 0.0444217, klDiv: 519.272, CE-loss: 0.039229, acc 0.986875\n",
      "\n",
      "2018-11-21T16:55:22.093896: step 14701, loss 0.0431272, latentLoss: 502.568, reconstructionLoss: 0.0381015, acc 0.987813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:55:22.105220: step 14701, loss 0.0419242, klDiv: 506.402, CE-loss: 0.0368602, acc 0.9875\n",
      "\n",
      "2018-11-21T16:55:37.781863: step 14801, loss 0.0604433, latentLoss: 483.945, reconstructionLoss: 0.0556038, acc 0.981875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:55:37.799519: step 14801, loss 0.0483798, klDiv: 487.868, CE-loss: 0.0435011, acc 0.984688\n",
      "\n",
      "2018-11-21T16:55:53.583004: step 14901, loss 0.0516956, latentLoss: 520.745, reconstructionLoss: 0.0464881, acc 0.982188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:55:53.596586: step 14901, loss 0.0412908, klDiv: 524.7, CE-loss: 0.0360438, acc 0.987188\n",
      "\n",
      "2018-11-21T16:56:10.303714: step 15001, loss 0.0616838, latentLoss: 519.671, reconstructionLoss: 0.0564871, acc 0.980313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:56:10.314497: step 15001, loss 0.0500141, klDiv: 519.894, CE-loss: 0.0448151, acc 0.98625\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-15000\n",
      "\n",
      "2018-11-21T16:56:26.745495: step 15101, loss 0.0431627, latentLoss: 509, reconstructionLoss: 0.0380727, acc 0.986875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:56:26.760028: step 15101, loss 0.0404834, klDiv: 508.433, CE-loss: 0.0353991, acc 0.987813\n",
      "\n",
      "2018-11-21T16:56:42.600069: step 15201, loss 0.0541246, latentLoss: 516.653, reconstructionLoss: 0.0489581, acc 0.981875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:56:42.615205: step 15201, loss 0.0468837, klDiv: 518.426, CE-loss: 0.0416994, acc 0.98875\n",
      "\n",
      "2018-11-21T16:56:58.315814: step 15301, loss 0.0451324, latentLoss: 508.308, reconstructionLoss: 0.0400493, acc 0.988125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:56:58.326758: step 15301, loss 0.0434832, klDiv: 505.646, CE-loss: 0.0384268, acc 0.989062\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T16:57:14.036904: step 15401, loss 0.0704999, latentLoss: 515.208, reconstructionLoss: 0.0653478, acc 0.9775\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:57:14.049692: step 15401, loss 0.0612947, klDiv: 515.038, CE-loss: 0.0561443, acc 0.979062\n",
      "\n",
      "2018-11-21T16:57:29.777410: step 15501, loss 0.0597107, latentLoss: 500.738, reconstructionLoss: 0.0547033, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:57:29.788697: step 15501, loss 0.052727, klDiv: 499.794, CE-loss: 0.047729, acc 0.98375\n",
      "\n",
      "2018-11-21T16:57:45.523404: step 15601, loss 0.0606158, latentLoss: 498.429, reconstructionLoss: 0.0556315, acc 0.981875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:57:45.538870: step 15601, loss 0.0501281, klDiv: 494.198, CE-loss: 0.0451862, acc 0.98375\n",
      "\n",
      "2018-11-21T16:58:01.254179: step 15701, loss 0.0368427, latentLoss: 492.135, reconstructionLoss: 0.0319213, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:58:01.269592: step 15701, loss 0.0312129, klDiv: 491.815, CE-loss: 0.0262948, acc 0.991562\n",
      "\n",
      "2018-11-21T16:58:16.986244: step 15801, loss 0.0676423, latentLoss: 569.586, reconstructionLoss: 0.0619464, acc 0.978125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:58:17.000725: step 15801, loss 0.0625513, klDiv: 568.53, CE-loss: 0.056866, acc 0.981875\n",
      "\n",
      "2018-11-21T16:58:32.749281: step 15901, loss 0.0553053, latentLoss: 517.006, reconstructionLoss: 0.0501352, acc 0.98375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:58:32.765730: step 15901, loss 0.0456652, klDiv: 514.73, CE-loss: 0.0405179, acc 0.987813\n",
      "\n",
      "2018-11-21T16:58:48.451106: step 16001, loss 0.0386805, latentLoss: 500.276, reconstructionLoss: 0.0336777, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:58:48.467995: step 16001, loss 0.0385856, klDiv: 499.809, CE-loss: 0.0335875, acc 0.989062\n",
      "\n",
      "2018-11-21T16:59:04.168214: step 16101, loss 0.053958, latentLoss: 508.63, reconstructionLoss: 0.0488717, acc 0.9825\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:59:04.177283: step 16101, loss 0.0468582, klDiv: 507.413, CE-loss: 0.0417841, acc 0.986562\n",
      "\n",
      "2018-11-21T16:59:19.972736: step 16201, loss 0.0498967, latentLoss: 470.289, reconstructionLoss: 0.0451938, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:59:19.988854: step 16201, loss 0.0471113, klDiv: 471.438, CE-loss: 0.042397, acc 0.985937\n",
      "\n",
      "2018-11-21T16:59:35.727946: step 16301, loss 0.0473176, latentLoss: 500.51, reconstructionLoss: 0.0423125, acc 0.98625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:59:35.739333: step 16301, loss 0.0346772, klDiv: 507.187, CE-loss: 0.0296053, acc 0.992188\n",
      "\n",
      "2018-11-21T16:59:51.423809: step 16401, loss 0.0476034, latentLoss: 495.266, reconstructionLoss: 0.0426508, acc 0.987188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T16:59:51.432676: step 16401, loss 0.0432993, klDiv: 494.265, CE-loss: 0.0383566, acc 0.986562\n",
      "\n",
      "2018-11-21T17:00:07.100465: step 16501, loss 0.0466818, latentLoss: 463.565, reconstructionLoss: 0.0420461, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:00:07.112148: step 16501, loss 0.0341662, klDiv: 467.758, CE-loss: 0.0294886, acc 0.9925\n",
      "\n",
      "2018-11-21T17:00:22.849862: step 16601, loss 0.0497388, latentLoss: 507.591, reconstructionLoss: 0.0446629, acc 0.984062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:00:22.863793: step 16601, loss 0.040164, klDiv: 505.663, CE-loss: 0.0351074, acc 0.989062\n",
      "\n",
      "2018-11-21T17:00:38.573032: step 16701, loss 0.0434755, latentLoss: 473.129, reconstructionLoss: 0.0387442, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:00:38.582930: step 16701, loss 0.0373175, klDiv: 475.828, CE-loss: 0.0325592, acc 0.986875\n",
      "\n",
      "2018-11-21T17:00:54.389339: step 16801, loss 0.0392937, latentLoss: 497.205, reconstructionLoss: 0.0343216, acc 0.98875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:00:54.404825: step 16801, loss 0.0322728, klDiv: 499.509, CE-loss: 0.0272777, acc 0.990313\n",
      "\n",
      "2018-11-21T17:01:10.124963: step 16901, loss 0.0376201, latentLoss: 484.584, reconstructionLoss: 0.0327742, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:01:10.139068: step 16901, loss 0.0377283, klDiv: 483.654, CE-loss: 0.0328918, acc 0.989688\n",
      "\n",
      "2018-11-21T17:01:25.845785: step 17001, loss 0.0454301, latentLoss: 502.063, reconstructionLoss: 0.0404094, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:01:25.858812: step 17001, loss 0.0370092, klDiv: 502.273, CE-loss: 0.0319865, acc 0.99125\n",
      "\n",
      "2018-11-21T17:01:41.575596: step 17101, loss 0.0374585, latentLoss: 474.161, reconstructionLoss: 0.0327169, acc 0.987813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:01:41.586604: step 17101, loss 0.0348805, klDiv: 472.673, CE-loss: 0.0301537, acc 0.990313\n",
      "\n",
      "2018-11-21T17:01:57.328030: step 17201, loss 0.0470566, latentLoss: 495.782, reconstructionLoss: 0.0420988, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:01:57.337413: step 17201, loss 0.0409781, klDiv: 495.653, CE-loss: 0.0360216, acc 0.987188\n",
      "\n",
      "2018-11-21T17:02:13.127909: step 17301, loss 0.0495295, latentLoss: 462.804, reconstructionLoss: 0.0449015, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:02:13.138625: step 17301, loss 0.043211, klDiv: 464.57, CE-loss: 0.0385653, acc 0.987813\n",
      "\n",
      "2018-11-21T17:02:28.820324: step 17401, loss 0.052767, latentLoss: 495.929, reconstructionLoss: 0.0478077, acc 0.982813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:02:28.838036: step 17401, loss 0.0429471, klDiv: 495.891, CE-loss: 0.0379882, acc 0.9875\n",
      "\n",
      "2018-11-21T17:02:44.585590: step 17501, loss 0.0496753, latentLoss: 500.386, reconstructionLoss: 0.0446715, acc 0.98625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:02:44.603960: step 17501, loss 0.0385694, klDiv: 497.006, CE-loss: 0.0335993, acc 0.99\n",
      "\n",
      "2018-11-21T17:03:00.312743: step 17601, loss 0.0502769, latentLoss: 488.473, reconstructionLoss: 0.0453922, acc 0.98625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:03:00.328709: step 17601, loss 0.0428763, klDiv: 486.016, CE-loss: 0.0380161, acc 0.989062\n",
      "\n",
      "2018-11-21T17:03:16.053721: step 17701, loss 0.0454236, latentLoss: 491.539, reconstructionLoss: 0.0405082, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:03:16.065642: step 17701, loss 0.0345948, klDiv: 488.201, CE-loss: 0.0297128, acc 0.990313\n",
      "\n",
      "2018-11-21T17:03:31.773668: step 17801, loss 0.0390246, latentLoss: 478.971, reconstructionLoss: 0.0342349, acc 0.989062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:03:31.786827: step 17801, loss 0.0344186, klDiv: 477.827, CE-loss: 0.0296403, acc 0.989375\n",
      "\n",
      "2018-11-21T17:03:47.537055: step 17901, loss 0.051809, latentLoss: 455.972, reconstructionLoss: 0.0472493, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:03:47.547387: step 17901, loss 0.0358173, klDiv: 465.538, CE-loss: 0.0311619, acc 0.99\n",
      "\n",
      "2018-11-21T17:04:03.263735: step 18001, loss 0.0544316, latentLoss: 495.852, reconstructionLoss: 0.049473, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:04:03.280887: step 18001, loss 0.0451829, klDiv: 496.648, CE-loss: 0.0402164, acc 0.987188\n",
      "\n",
      "2018-11-21T17:04:18.992812: step 18101, loss 0.0453731, latentLoss: 506.475, reconstructionLoss: 0.0403084, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:04:19.002067: step 18101, loss 0.0366159, klDiv: 505.581, CE-loss: 0.0315601, acc 0.989062\n",
      "\n",
      "2018-11-21T17:04:34.744656: step 18201, loss 0.0451539, latentLoss: 482.197, reconstructionLoss: 0.0403319, acc 0.985313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:04:34.756211: step 18201, loss 0.0343141, klDiv: 482.092, CE-loss: 0.0294931, acc 0.990313\n",
      "\n",
      "2018-11-21T17:04:50.439107: step 18301, loss 0.0383199, latentLoss: 475.016, reconstructionLoss: 0.0335697, acc 0.988125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:04:50.452950: step 18301, loss 0.0325835, klDiv: 473.415, CE-loss: 0.0278493, acc 0.99125\n",
      "\n",
      "2018-11-21T17:05:06.164818: step 18401, loss 0.0772559, latentLoss: 490.877, reconstructionLoss: 0.0723471, acc 0.977188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:05:06.180156: step 18401, loss 0.0533268, klDiv: 495.042, CE-loss: 0.0483764, acc 0.983437\n",
      "\n",
      "2018-11-21T17:05:21.927841: step 18501, loss 0.0438298, latentLoss: 472.019, reconstructionLoss: 0.0391096, acc 0.987188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:05:21.941696: step 18501, loss 0.0357826, klDiv: 473.865, CE-loss: 0.031044, acc 0.99\n",
      "\n",
      "2018-11-21T17:05:37.724848: step 18601, loss 0.0283454, latentLoss: 474.669, reconstructionLoss: 0.0235987, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:05:37.742099: step 18601, loss 0.0250525, klDiv: 469.555, CE-loss: 0.0203569, acc 0.993438\n",
      "\n",
      "2018-11-21T17:05:53.404638: step 18701, loss 0.0482582, latentLoss: 486.405, reconstructionLoss: 0.0433941, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:05:53.420902: step 18701, loss 0.04101, klDiv: 490.46, CE-loss: 0.0361054, acc 0.99\n",
      "\n",
      "2018-11-21T17:06:09.063608: step 18801, loss 0.0455799, latentLoss: 490.39, reconstructionLoss: 0.040676, acc 0.98625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:06:09.072795: step 18801, loss 0.0375336, klDiv: 489.441, CE-loss: 0.0326391, acc 0.99\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:06:24.903705: step 18901, loss 0.0373828, latentLoss: 473.564, reconstructionLoss: 0.0326472, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:06:24.914322: step 18901, loss 0.0273441, klDiv: 474.637, CE-loss: 0.0225977, acc 0.993438\n",
      "\n",
      "2018-11-21T17:06:40.600863: step 19001, loss 0.0325192, latentLoss: 492.258, reconstructionLoss: 0.0275966, acc 0.991875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:06:40.618928: step 19001, loss 0.0293144, klDiv: 491.723, CE-loss: 0.0243971, acc 0.9925\n",
      "\n",
      "2018-11-21T17:06:56.306080: step 19101, loss 0.0431745, latentLoss: 495.794, reconstructionLoss: 0.0382166, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:06:56.316998: step 19101, loss 0.0395853, klDiv: 495.654, CE-loss: 0.0346288, acc 0.989375\n",
      "\n",
      "2018-11-21T17:07:11.952531: step 19201, loss 0.0427703, latentLoss: 497.878, reconstructionLoss: 0.0377915, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:07:11.961346: step 19201, loss 0.0345614, klDiv: 500.57, CE-loss: 0.0295557, acc 0.990937\n",
      "\n",
      "2018-11-21T17:07:27.690099: step 19301, loss 0.0463125, latentLoss: 480.88, reconstructionLoss: 0.0415037, acc 0.986875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:07:27.704910: step 19301, loss 0.0351158, klDiv: 482.107, CE-loss: 0.0302948, acc 0.990313\n",
      "\n",
      "2018-11-21T17:07:43.422200: step 19401, loss 0.05907, latentLoss: 491.717, reconstructionLoss: 0.0541528, acc 0.980937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:07:43.440648: step 19401, loss 0.0487712, klDiv: 494.885, CE-loss: 0.0438224, acc 0.985313\n",
      "\n",
      "2018-11-21T17:07:59.114545: step 19501, loss 0.0369913, latentLoss: 459.517, reconstructionLoss: 0.0323961, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:07:59.123736: step 19501, loss 0.0310788, klDiv: 455.008, CE-loss: 0.0265288, acc 0.993125\n",
      "\n",
      "2018-11-21T17:08:14.895667: step 19601, loss 0.0437771, latentLoss: 471.246, reconstructionLoss: 0.0390646, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:08:14.909979: step 19601, loss 0.0356289, klDiv: 467.497, CE-loss: 0.030954, acc 0.990625\n",
      "\n",
      "2018-11-21T17:08:30.591967: step 19701, loss 0.043465, latentLoss: 464.209, reconstructionLoss: 0.0388229, acc 0.986875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:08:30.606906: step 19701, loss 0.0335231, klDiv: 465.78, CE-loss: 0.0288653, acc 0.9925\n",
      "\n",
      "2018-11-21T17:08:46.395363: step 19801, loss 0.0340272, latentLoss: 467.681, reconstructionLoss: 0.0293504, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:08:46.411075: step 19801, loss 0.0276354, klDiv: 467.99, CE-loss: 0.0229555, acc 0.994375\n",
      "\n",
      "2018-11-21T17:09:02.177153: step 19901, loss 0.0331176, latentLoss: 471.849, reconstructionLoss: 0.0283991, acc 0.99125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:09:02.195605: step 19901, loss 0.0248176, klDiv: 469.538, CE-loss: 0.0201222, acc 0.994687\n",
      "\n",
      "2018-11-21T17:09:17.824300: step 20001, loss 0.042771, latentLoss: 467.333, reconstructionLoss: 0.0380976, acc 0.986875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:09:17.840961: step 20001, loss 0.0377034, klDiv: 464.041, CE-loss: 0.033063, acc 0.990313\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-20000\n",
      "\n",
      "2018-11-21T17:09:34.036845: step 20101, loss 0.031679, latentLoss: 466.274, reconstructionLoss: 0.0270162, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:09:34.049184: step 20101, loss 0.0274883, klDiv: 465.814, CE-loss: 0.0228301, acc 0.991875\n",
      "\n",
      "2018-11-21T17:09:49.808476: step 20201, loss 0.0350112, latentLoss: 466.165, reconstructionLoss: 0.0303495, acc 0.989062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:09:49.822575: step 20201, loss 0.0313108, klDiv: 469.304, CE-loss: 0.0266177, acc 0.99125\n",
      "\n",
      "2018-11-21T17:10:05.493539: step 20301, loss 0.0434486, latentLoss: 474.132, reconstructionLoss: 0.0387073, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:10:05.507524: step 20301, loss 0.0371427, klDiv: 470.791, CE-loss: 0.0324348, acc 0.988437\n",
      "\n",
      "2018-11-21T17:10:21.286475: step 20401, loss 0.0440686, latentLoss: 464.319, reconstructionLoss: 0.0394254, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:10:21.296412: step 20401, loss 0.0354413, klDiv: 462.093, CE-loss: 0.0308204, acc 0.989688\n",
      "\n",
      "2018-11-21T17:10:36.968986: step 20501, loss 0.0439151, latentLoss: 475.147, reconstructionLoss: 0.0391636, acc 0.989062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:10:36.980126: step 20501, loss 0.0343169, klDiv: 479.497, CE-loss: 0.0295219, acc 0.989062\n",
      "\n",
      "2018-11-21T17:10:52.683027: step 20601, loss 0.0500011, latentLoss: 468.587, reconstructionLoss: 0.0453152, acc 0.982813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:10:52.698354: step 20601, loss 0.0395267, klDiv: 467.294, CE-loss: 0.0348537, acc 0.987813\n",
      "\n",
      "2018-11-21T17:11:08.379221: step 20701, loss 0.0519795, latentLoss: 464.162, reconstructionLoss: 0.0473379, acc 0.985625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:11:08.388357: step 20701, loss 0.0412764, klDiv: 467.012, CE-loss: 0.0366062, acc 0.986562\n",
      "\n",
      "2018-11-21T17:11:24.070839: step 20801, loss 0.0385537, latentLoss: 461.796, reconstructionLoss: 0.0339358, acc 0.987813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:11:24.089741: step 20801, loss 0.0296391, klDiv: 464.674, CE-loss: 0.0249924, acc 0.993125\n",
      "\n",
      "2018-11-21T17:11:39.806545: step 20901, loss 0.0382396, latentLoss: 462.838, reconstructionLoss: 0.0336113, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:11:39.815656: step 20901, loss 0.0315761, klDiv: 465.827, CE-loss: 0.0269178, acc 0.99\n",
      "\n",
      "2018-11-21T17:11:55.491954: step 21001, loss 0.0417966, latentLoss: 453.333, reconstructionLoss: 0.0372633, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:11:55.507069: step 21001, loss 0.0344026, klDiv: 452.433, CE-loss: 0.0298783, acc 0.98875\n",
      "\n",
      "2018-11-21T17:12:11.231959: step 21101, loss 0.028647, latentLoss: 459.372, reconstructionLoss: 0.0240533, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:12:11.247695: step 21101, loss 0.0258763, klDiv: 455.877, CE-loss: 0.0213176, acc 0.994375\n",
      "\n",
      "2018-11-21T17:12:26.917070: step 21201, loss 0.0366551, latentLoss: 479.458, reconstructionLoss: 0.0318605, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:12:26.933647: step 21201, loss 0.0299645, klDiv: 474.753, CE-loss: 0.025217, acc 0.9925\n",
      "\n",
      "2018-11-21T17:12:42.704975: step 21301, loss 0.0360488, latentLoss: 447.454, reconstructionLoss: 0.0315742, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:12:42.723765: step 21301, loss 0.0301683, klDiv: 446.646, CE-loss: 0.0257019, acc 0.993125\n",
      "\n",
      "2018-11-21T17:12:58.392053: step 21401, loss 0.0479481, latentLoss: 491.73, reconstructionLoss: 0.0430308, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:12:58.409463: step 21401, loss 0.0328245, klDiv: 494.723, CE-loss: 0.0278773, acc 0.99125\n",
      "\n",
      "2018-11-21T17:13:14.166229: step 21501, loss 0.0268871, latentLoss: 466.816, reconstructionLoss: 0.0222189, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:13:14.180647: step 21501, loss 0.0272558, klDiv: 464.545, CE-loss: 0.0226104, acc 0.993125\n",
      "\n",
      "2018-11-21T17:13:29.852527: step 21601, loss 0.0361693, latentLoss: 458.744, reconstructionLoss: 0.0315819, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:13:29.864901: step 21601, loss 0.0274929, klDiv: 460.616, CE-loss: 0.0228868, acc 0.993438\n",
      "\n",
      "2018-11-21T17:13:45.518263: step 21701, loss 0.0306637, latentLoss: 432.542, reconstructionLoss: 0.0263383, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:13:45.536813: step 21701, loss 0.0259131, klDiv: 428.091, CE-loss: 0.0216322, acc 0.995\n",
      "\n",
      "2018-11-21T17:14:01.186014: step 21801, loss 0.0376768, latentLoss: 454.252, reconstructionLoss: 0.0331343, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:14:01.197662: step 21801, loss 0.0283365, klDiv: 454.082, CE-loss: 0.0237956, acc 0.991875\n",
      "\n",
      "2018-11-21T17:14:16.940104: step 21901, loss 0.0487691, latentLoss: 478.832, reconstructionLoss: 0.0439808, acc 0.985937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:14:16.956651: step 21901, loss 0.0384066, klDiv: 474.864, CE-loss: 0.033658, acc 0.989375\n",
      "\n",
      "2018-11-21T17:14:32.680935: step 22001, loss 0.0477167, latentLoss: 467.812, reconstructionLoss: 0.0430386, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:14:32.696981: step 22001, loss 0.0298909, klDiv: 467.612, CE-loss: 0.0252148, acc 0.990625\n",
      "\n",
      "2018-11-21T17:14:48.313396: step 22101, loss 0.0313432, latentLoss: 476.02, reconstructionLoss: 0.026583, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:14:48.333682: step 22101, loss 0.0266436, klDiv: 476.002, CE-loss: 0.0218836, acc 0.992813\n",
      "\n",
      "2018-11-21T17:15:04.043357: step 22201, loss 0.0345529, latentLoss: 448.917, reconstructionLoss: 0.0300637, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:15:04.054888: step 22201, loss 0.0329469, klDiv: 446.072, CE-loss: 0.0284862, acc 0.991562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:15:19.739553: step 22301, loss 0.034112, latentLoss: 465.431, reconstructionLoss: 0.0294577, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:15:19.748400: step 22301, loss 0.0286591, klDiv: 464.599, CE-loss: 0.0240131, acc 0.992813\n",
      "\n",
      "2018-11-21T17:15:35.533059: step 22401, loss 0.0287823, latentLoss: 465.444, reconstructionLoss: 0.0241279, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:15:35.551442: step 22401, loss 0.0264571, klDiv: 461.287, CE-loss: 0.0218443, acc 0.992813\n",
      "\n",
      "2018-11-21T17:15:51.288399: step 22501, loss 0.0286257, latentLoss: 471.538, reconstructionLoss: 0.0239104, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:15:51.298990: step 22501, loss 0.0252683, klDiv: 472.038, CE-loss: 0.020548, acc 0.994375\n",
      "\n",
      "2018-11-21T17:16:07.014909: step 22601, loss 0.0312321, latentLoss: 463.21, reconstructionLoss: 0.0266, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:16:07.026538: step 22601, loss 0.0241521, klDiv: 460.141, CE-loss: 0.0195507, acc 0.993125\n",
      "\n",
      "2018-11-21T17:16:22.701570: step 22701, loss 0.0334344, latentLoss: 458.729, reconstructionLoss: 0.0288471, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:16:22.717200: step 22701, loss 0.0260191, klDiv: 459.366, CE-loss: 0.0214254, acc 0.993438\n",
      "\n",
      "2018-11-21T17:16:38.425214: step 22801, loss 0.0433569, latentLoss: 468.038, reconstructionLoss: 0.0386765, acc 0.988125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:16:38.435309: step 22801, loss 0.0327794, klDiv: 470.124, CE-loss: 0.0280782, acc 0.990937\n",
      "\n",
      "2018-11-21T17:16:54.162366: step 22901, loss 0.032597, latentLoss: 453.947, reconstructionLoss: 0.0280575, acc 0.988437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:16:54.173701: step 22901, loss 0.0280786, klDiv: 450.146, CE-loss: 0.0235771, acc 0.9925\n",
      "\n",
      "2018-11-21T17:17:09.920866: step 23001, loss 0.0312033, latentLoss: 469.644, reconstructionLoss: 0.0265069, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:17:09.932564: step 23001, loss 0.0241509, klDiv: 468.716, CE-loss: 0.0194637, acc 0.995938\n",
      "\n",
      "2018-11-21T17:17:25.620389: step 23101, loss 0.0440573, latentLoss: 485.953, reconstructionLoss: 0.0391978, acc 0.986875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:17:25.630452: step 23101, loss 0.0313862, klDiv: 488.781, CE-loss: 0.0264984, acc 0.9925\n",
      "\n",
      "2018-11-21T17:17:41.349709: step 23201, loss 0.0546992, latentLoss: 501.909, reconstructionLoss: 0.0496801, acc 0.98375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:17:41.366112: step 23201, loss 0.045184, klDiv: 504, CE-loss: 0.0401439, acc 0.986875\n",
      "\n",
      "2018-11-21T17:17:57.039215: step 23301, loss 0.0363538, latentLoss: 455.082, reconstructionLoss: 0.031803, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:17:57.049325: step 23301, loss 0.0327251, klDiv: 454.125, CE-loss: 0.0281838, acc 0.990625\n",
      "\n",
      "2018-11-21T17:18:12.827830: step 23401, loss 0.0420785, latentLoss: 479.616, reconstructionLoss: 0.0372823, acc 0.988125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:18:12.837877: step 23401, loss 0.0318029, klDiv: 473.352, CE-loss: 0.0270694, acc 0.99125\n",
      "\n",
      "2018-11-21T17:18:28.549382: step 23501, loss 0.0401715, latentLoss: 451.886, reconstructionLoss: 0.0356526, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:18:28.561425: step 23501, loss 0.0327885, klDiv: 451.367, CE-loss: 0.0282748, acc 0.990937\n",
      "\n",
      "2018-11-21T17:18:44.221785: step 23601, loss 0.0433206, latentLoss: 472.698, reconstructionLoss: 0.0385936, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:18:44.231932: step 23601, loss 0.0365792, klDiv: 466.384, CE-loss: 0.0319154, acc 0.988437\n",
      "\n",
      "2018-11-21T17:18:59.959928: step 23701, loss 0.0303315, latentLoss: 468.393, reconstructionLoss: 0.0256476, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:18:59.971127: step 23701, loss 0.0250358, klDiv: 467.164, CE-loss: 0.0203642, acc 0.995938\n",
      "\n",
      "2018-11-21T17:19:15.639656: step 23801, loss 0.0300753, latentLoss: 451.309, reconstructionLoss: 0.0255622, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:19:15.649058: step 23801, loss 0.0230284, klDiv: 448.887, CE-loss: 0.0185395, acc 0.993438\n",
      "\n",
      "2018-11-21T17:19:31.355147: step 23901, loss 0.0309274, latentLoss: 475.617, reconstructionLoss: 0.0261712, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:19:31.369495: step 23901, loss 0.0264566, klDiv: 474.557, CE-loss: 0.021711, acc 0.994062\n",
      "\n",
      "2018-11-21T17:19:47.104841: step 24001, loss 0.0371377, latentLoss: 459.838, reconstructionLoss: 0.0325393, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:19:47.120301: step 24001, loss 0.0258528, klDiv: 458.004, CE-loss: 0.0212728, acc 0.993125\n",
      "\n",
      "2018-11-21T17:20:02.869644: step 24101, loss 0.0328959, latentLoss: 451.537, reconstructionLoss: 0.0283805, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:20:02.883734: step 24101, loss 0.0258348, klDiv: 447.2, CE-loss: 0.0213628, acc 0.99375\n",
      "\n",
      "2018-11-21T17:20:18.532898: step 24201, loss 0.0318371, latentLoss: 474.092, reconstructionLoss: 0.0270961, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:20:18.546999: step 24201, loss 0.0288509, klDiv: 477.4, CE-loss: 0.0240769, acc 0.993125\n",
      "\n",
      "2018-11-21T17:20:34.254006: step 24301, loss 0.0318137, latentLoss: 467.111, reconstructionLoss: 0.0271426, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:20:34.271685: step 24301, loss 0.0271358, klDiv: 467.013, CE-loss: 0.0224657, acc 0.99125\n",
      "\n",
      "2018-11-21T17:20:49.972731: step 24401, loss 0.0413278, latentLoss: 461.787, reconstructionLoss: 0.0367099, acc 0.988437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:20:49.982293: step 24401, loss 0.0362536, klDiv: 464.274, CE-loss: 0.0316109, acc 0.990313\n",
      "\n",
      "2018-11-21T17:21:05.626614: step 24501, loss 0.0426167, latentLoss: 452.862, reconstructionLoss: 0.0380881, acc 0.988437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:21:05.643537: step 24501, loss 0.0327988, klDiv: 452.155, CE-loss: 0.0282772, acc 0.9925\n",
      "\n",
      "2018-11-21T17:21:21.328126: step 24601, loss 0.0362211, latentLoss: 442.133, reconstructionLoss: 0.0317998, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:21:21.339408: step 24601, loss 0.0292565, klDiv: 444.772, CE-loss: 0.0248088, acc 0.991875\n",
      "\n",
      "2018-11-21T17:21:37.000968: step 24701, loss 0.0328635, latentLoss: 461.568, reconstructionLoss: 0.0282478, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:21:37.011289: step 24701, loss 0.0265671, klDiv: 462.897, CE-loss: 0.0219381, acc 0.994062\n",
      "\n",
      "2018-11-21T17:21:52.725316: step 24801, loss 0.036011, latentLoss: 462.196, reconstructionLoss: 0.031389, acc 0.99125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:21:52.734000: step 24801, loss 0.0288555, klDiv: 463.145, CE-loss: 0.0242241, acc 0.99375\n",
      "\n",
      "2018-11-21T17:22:08.517515: step 24901, loss 0.0305786, latentLoss: 442.419, reconstructionLoss: 0.0261544, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:22:08.526770: step 24901, loss 0.0269762, klDiv: 440.602, CE-loss: 0.0225701, acc 0.99375\n",
      "\n",
      "2018-11-21T17:22:24.218296: step 25001, loss 0.0351289, latentLoss: 448.872, reconstructionLoss: 0.0306401, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:22:24.237241: step 25001, loss 0.0254711, klDiv: 445.871, CE-loss: 0.0210124, acc 0.993125\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-25000\n",
      "\n",
      "2018-11-21T17:22:40.446005: step 25101, loss 0.04812, latentLoss: 442.571, reconstructionLoss: 0.0436942, acc 0.984688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:22:40.455265: step 25101, loss 0.0312145, klDiv: 440.463, CE-loss: 0.0268099, acc 0.990937\n",
      "\n",
      "2018-11-21T17:22:56.078572: step 25201, loss 0.031793, latentLoss: 442.61, reconstructionLoss: 0.0273669, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:22:56.097255: step 25201, loss 0.024395, klDiv: 442.274, CE-loss: 0.0199722, acc 0.995938\n",
      "\n",
      "2018-11-21T17:23:11.740616: step 25301, loss 0.0286332, latentLoss: 441.649, reconstructionLoss: 0.0242168, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:23:11.760786: step 25301, loss 0.0225278, klDiv: 443.021, CE-loss: 0.0180976, acc 0.995938\n",
      "\n",
      "2018-11-21T17:23:27.511534: step 25401, loss 0.0245481, latentLoss: 435.117, reconstructionLoss: 0.0201969, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:23:27.521699: step 25401, loss 0.0231413, klDiv: 433.516, CE-loss: 0.0188061, acc 0.99375\n",
      "\n",
      "2018-11-21T17:23:43.260178: step 25501, loss 0.0252526, latentLoss: 438.436, reconstructionLoss: 0.0208682, acc 0.991875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:23:43.271077: step 25501, loss 0.0207864, klDiv: 436.352, CE-loss: 0.0164229, acc 0.994062\n",
      "\n",
      "2018-11-21T17:23:58.929314: step 25601, loss 0.0339131, latentLoss: 448.293, reconstructionLoss: 0.0294302, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:23:58.943442: step 25601, loss 0.0290537, klDiv: 446.511, CE-loss: 0.0245886, acc 0.993438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:24:14.660699: step 25701, loss 0.028633, latentLoss: 445.772, reconstructionLoss: 0.0241753, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:24:14.677422: step 25701, loss 0.0248441, klDiv: 445.947, CE-loss: 0.0203847, acc 0.99375\n",
      "\n",
      "2018-11-21T17:24:30.368296: step 25801, loss 0.0316959, latentLoss: 426.01, reconstructionLoss: 0.0274358, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:24:30.377063: step 25801, loss 0.0237895, klDiv: 426.29, CE-loss: 0.0195266, acc 0.99375\n",
      "\n",
      "2018-11-21T17:24:45.999377: step 25901, loss 0.0489328, latentLoss: 436.264, reconstructionLoss: 0.0445702, acc 0.985313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:24:46.016277: step 25901, loss 0.0348452, klDiv: 436.27, CE-loss: 0.0304825, acc 0.989375\n",
      "\n",
      "2018-11-21T17:25:01.718917: step 26001, loss 0.0305727, latentLoss: 471.914, reconstructionLoss: 0.0258535, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:25:01.734894: step 26001, loss 0.0218783, klDiv: 470.633, CE-loss: 0.017172, acc 0.995\n",
      "\n",
      "2018-11-21T17:25:17.453941: step 26101, loss 0.0289745, latentLoss: 447.452, reconstructionLoss: 0.0245, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:25:17.465061: step 26101, loss 0.0220514, klDiv: 448.117, CE-loss: 0.0175703, acc 0.995625\n",
      "\n",
      "2018-11-21T17:25:33.180250: step 26201, loss 0.0229533, latentLoss: 413.326, reconstructionLoss: 0.0188201, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:25:33.190798: step 26201, loss 0.0217513, klDiv: 413.546, CE-loss: 0.0176158, acc 0.994375\n",
      "\n",
      "2018-11-21T17:25:48.955379: step 26301, loss 0.0319145, latentLoss: 462.394, reconstructionLoss: 0.0272906, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:25:48.966109: step 26301, loss 0.0275597, klDiv: 462.078, CE-loss: 0.0229389, acc 0.992813\n",
      "\n",
      "2018-11-21T17:26:04.681596: step 26401, loss 0.0273212, latentLoss: 434.524, reconstructionLoss: 0.0229759, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:26:04.690894: step 26401, loss 0.0221927, klDiv: 435.177, CE-loss: 0.0178409, acc 0.993438\n",
      "\n",
      "2018-11-21T17:26:20.360525: step 26501, loss 0.0432469, latentLoss: 427.331, reconstructionLoss: 0.0389736, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:26:20.378926: step 26501, loss 0.0273317, klDiv: 429.459, CE-loss: 0.0230371, acc 0.992188\n",
      "\n",
      "2018-11-21T17:26:36.112104: step 26601, loss 0.0365143, latentLoss: 436.35, reconstructionLoss: 0.0321508, acc 0.988437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:26:36.123939: step 26601, loss 0.0286926, klDiv: 436.152, CE-loss: 0.0243311, acc 0.992188\n",
      "\n",
      "2018-11-21T17:26:51.796896: step 26701, loss 0.0318429, latentLoss: 425.76, reconstructionLoss: 0.0275853, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:26:51.808933: step 26701, loss 0.0227946, klDiv: 426.905, CE-loss: 0.0185256, acc 0.994687\n",
      "\n",
      "2018-11-21T17:27:07.514260: step 26801, loss 0.0349582, latentLoss: 437.817, reconstructionLoss: 0.03058, acc 0.98875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:27:07.526681: step 26801, loss 0.02322, klDiv: 437.168, CE-loss: 0.0188483, acc 0.994062\n",
      "\n",
      "2018-11-21T17:27:23.239626: step 26901, loss 0.037582, latentLoss: 430.464, reconstructionLoss: 0.0332773, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:27:23.254260: step 26901, loss 0.0306215, klDiv: 429.789, CE-loss: 0.0263236, acc 0.99\n",
      "\n",
      "2018-11-21T17:27:38.909799: step 27001, loss 0.0241347, latentLoss: 462.332, reconstructionLoss: 0.0195114, acc 0.99125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:27:38.928401: step 27001, loss 0.0206993, klDiv: 466.444, CE-loss: 0.0160348, acc 0.994062\n",
      "\n",
      "2018-11-21T17:27:54.637551: step 27101, loss 0.0275021, latentLoss: 457.647, reconstructionLoss: 0.0229257, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:27:54.648503: step 27101, loss 0.0198432, klDiv: 453.573, CE-loss: 0.0153074, acc 0.994375\n",
      "\n",
      "2018-11-21T17:28:10.320195: step 27201, loss 0.0269057, latentLoss: 431.388, reconstructionLoss: 0.0225919, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:28:10.340530: step 27201, loss 0.0190004, klDiv: 434.135, CE-loss: 0.0146591, acc 0.995938\n",
      "\n",
      "2018-11-21T17:28:26.036665: step 27301, loss 0.0381126, latentLoss: 459.219, reconstructionLoss: 0.0335204, acc 0.9875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:28:26.051820: step 27301, loss 0.0260217, klDiv: 461.453, CE-loss: 0.0214072, acc 0.994062\n",
      "\n",
      "2018-11-21T17:28:41.736069: step 27401, loss 0.0314606, latentLoss: 439.701, reconstructionLoss: 0.0270636, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:28:41.747438: step 27401, loss 0.019724, klDiv: 437.919, CE-loss: 0.0153448, acc 0.995625\n",
      "\n",
      "2018-11-21T17:28:57.430032: step 27501, loss 0.0284093, latentLoss: 437.63, reconstructionLoss: 0.024033, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:28:57.446858: step 27501, loss 0.0229919, klDiv: 437.711, CE-loss: 0.0186148, acc 0.994375\n",
      "\n",
      "2018-11-21T17:29:13.136468: step 27601, loss 0.0350138, latentLoss: 438.201, reconstructionLoss: 0.0306318, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:29:13.147857: step 27601, loss 0.0278052, klDiv: 440.883, CE-loss: 0.0233964, acc 0.993125\n",
      "\n",
      "2018-11-21T17:29:28.830688: step 27701, loss 0.0266075, latentLoss: 447.085, reconstructionLoss: 0.0221366, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:29:28.839900: step 27701, loss 0.0229204, klDiv: 445.871, CE-loss: 0.0184616, acc 0.99375\n",
      "\n",
      "2018-11-21T17:29:44.514054: step 27801, loss 0.0318172, latentLoss: 454.246, reconstructionLoss: 0.0272747, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:29:44.524621: step 27801, loss 0.0279339, klDiv: 453.613, CE-loss: 0.0233977, acc 0.992813\n",
      "\n",
      "2018-11-21T17:30:00.255822: step 27901, loss 0.037822, latentLoss: 437.845, reconstructionLoss: 0.0334436, acc 0.98875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:30:00.275769: step 27901, loss 0.0290735, klDiv: 439.072, CE-loss: 0.0246828, acc 0.991562\n",
      "\n",
      "2018-11-21T17:30:16.006754: step 28001, loss 0.0336429, latentLoss: 426.2, reconstructionLoss: 0.0293809, acc 0.989375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:30:16.023763: step 28001, loss 0.02777, klDiv: 425.701, CE-loss: 0.023513, acc 0.991875\n",
      "\n",
      "2018-11-21T17:30:31.675619: step 28101, loss 0.0283158, latentLoss: 460.531, reconstructionLoss: 0.0237105, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:30:31.693033: step 28101, loss 0.0208114, klDiv: 459.39, CE-loss: 0.0162175, acc 0.994687\n",
      "\n",
      "2018-11-21T17:30:47.429159: step 28201, loss 0.0207104, latentLoss: 448.065, reconstructionLoss: 0.0162298, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:30:47.442435: step 28201, loss 0.0208113, klDiv: 450.721, CE-loss: 0.016304, acc 0.994687\n",
      "\n",
      "2018-11-21T17:31:03.209029: step 28301, loss 0.0277377, latentLoss: 461.559, reconstructionLoss: 0.0231221, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:31:03.225224: step 28301, loss 0.0272124, klDiv: 465.953, CE-loss: 0.0225528, acc 0.993438\n",
      "\n",
      "2018-11-21T17:31:18.917401: step 28401, loss 0.0204251, latentLoss: 442.68, reconstructionLoss: 0.0159983, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:31:18.929358: step 28401, loss 0.0184893, klDiv: 442.954, CE-loss: 0.0140598, acc 0.996562\n",
      "\n",
      "2018-11-21T17:31:34.595594: step 28501, loss 0.0402054, latentLoss: 431.212, reconstructionLoss: 0.0358933, acc 0.986562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:31:34.605821: step 28501, loss 0.0361637, klDiv: 432.364, CE-loss: 0.03184, acc 0.990937\n",
      "\n",
      "2018-11-21T17:31:50.277022: step 28601, loss 0.0339443, latentLoss: 446.706, reconstructionLoss: 0.0294772, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:31:50.286673: step 28601, loss 0.0222074, klDiv: 447.405, CE-loss: 0.0177333, acc 0.995\n",
      "\n",
      "2018-11-21T17:32:05.943683: step 28701, loss 0.0274471, latentLoss: 446.132, reconstructionLoss: 0.0229858, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:32:05.959209: step 28701, loss 0.0222395, klDiv: 446.898, CE-loss: 0.0177705, acc 0.993438\n",
      "\n",
      "2018-11-21T17:32:21.680635: step 28801, loss 0.0322327, latentLoss: 444.322, reconstructionLoss: 0.0277895, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:32:21.692175: step 28801, loss 0.0256607, klDiv: 443.51, CE-loss: 0.0212256, acc 0.994375\n",
      "\n",
      "2018-11-21T17:32:37.395123: step 28901, loss 0.0343105, latentLoss: 421.497, reconstructionLoss: 0.0300956, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:32:37.404027: step 28901, loss 0.0255786, klDiv: 418.492, CE-loss: 0.0213937, acc 0.994687\n",
      "\n",
      "2018-11-21T17:32:53.180971: step 29001, loss 0.0275152, latentLoss: 448.844, reconstructionLoss: 0.0230268, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:32:53.199491: step 29001, loss 0.0233234, klDiv: 450.776, CE-loss: 0.0188156, acc 0.994375\n",
      "\n",
      "2018-11-21T17:33:08.876858: step 29101, loss 0.0206154, latentLoss: 428.094, reconstructionLoss: 0.0163345, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:33:08.887151: step 29101, loss 0.0183225, klDiv: 429.258, CE-loss: 0.0140299, acc 0.994687\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:33:24.567874: step 29201, loss 0.0341563, latentLoss: 424.65, reconstructionLoss: 0.0299098, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:33:24.578737: step 29201, loss 0.024092, klDiv: 424.942, CE-loss: 0.0198425, acc 0.994062\n",
      "\n",
      "2018-11-21T17:33:40.259033: step 29301, loss 0.021334, latentLoss: 422.244, reconstructionLoss: 0.0171116, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:33:40.276205: step 29301, loss 0.01701, klDiv: 422.387, CE-loss: 0.0127861, acc 0.995938\n",
      "\n",
      "2018-11-21T17:33:56.044699: step 29401, loss 0.0319469, latentLoss: 428.334, reconstructionLoss: 0.0276636, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:33:56.063629: step 29401, loss 0.0260811, klDiv: 430.522, CE-loss: 0.0217759, acc 0.993438\n",
      "\n",
      "2018-11-21T17:34:11.807421: step 29501, loss 0.0249607, latentLoss: 414.445, reconstructionLoss: 0.0208163, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:34:11.818563: step 29501, loss 0.0203257, klDiv: 414.041, CE-loss: 0.0161853, acc 0.994375\n",
      "\n",
      "2018-11-21T17:34:27.515761: step 29601, loss 0.0265423, latentLoss: 425.654, reconstructionLoss: 0.0222858, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:34:27.525083: step 29601, loss 0.021589, klDiv: 424.031, CE-loss: 0.0173487, acc 0.994375\n",
      "\n",
      "2018-11-21T17:34:43.217890: step 29701, loss 0.0237735, latentLoss: 424.49, reconstructionLoss: 0.0195286, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:34:43.229564: step 29701, loss 0.0225168, klDiv: 424.872, CE-loss: 0.0182681, acc 0.994062\n",
      "\n",
      "2018-11-21T17:34:58.930537: step 29801, loss 0.0228089, latentLoss: 445.285, reconstructionLoss: 0.018356, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:34:58.939630: step 29801, loss 0.0214474, klDiv: 443.648, CE-loss: 0.0170109, acc 0.994687\n",
      "\n",
      "2018-11-21T17:35:14.665299: step 29901, loss 0.0362745, latentLoss: 406.91, reconstructionLoss: 0.0322054, acc 0.98875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:35:14.683500: step 29901, loss 0.0239548, klDiv: 409.993, CE-loss: 0.0198548, acc 0.993125\n",
      "\n",
      "2018-11-21T17:35:30.330218: step 30001, loss 0.0256933, latentLoss: 423.543, reconstructionLoss: 0.0214578, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:35:30.344834: step 30001, loss 0.02189, klDiv: 424.226, CE-loss: 0.0176478, acc 0.994687\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-30000\n",
      "\n",
      "2018-11-21T17:35:46.595644: step 30101, loss 0.0290858, latentLoss: 433.072, reconstructionLoss: 0.024755, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:35:46.610979: step 30101, loss 0.0217226, klDiv: 433.871, CE-loss: 0.0173839, acc 0.995\n",
      "\n",
      "2018-11-21T17:36:02.271237: step 30201, loss 0.0301402, latentLoss: 405.995, reconstructionLoss: 0.0260802, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:36:02.288827: step 30201, loss 0.0239262, klDiv: 403.652, CE-loss: 0.0198897, acc 0.99375\n",
      "\n",
      "2018-11-21T17:36:17.940974: step 30301, loss 0.0240392, latentLoss: 407.532, reconstructionLoss: 0.0199639, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:36:17.952653: step 30301, loss 0.0196516, klDiv: 411.876, CE-loss: 0.0155328, acc 0.996562\n",
      "\n",
      "2018-11-21T17:36:33.687316: step 30401, loss 0.0270771, latentLoss: 407.125, reconstructionLoss: 0.0230058, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:36:33.696417: step 30401, loss 0.0182938, klDiv: 406.727, CE-loss: 0.0142265, acc 0.995625\n",
      "\n",
      "2018-11-21T17:36:49.418460: step 30501, loss 0.0164714, latentLoss: 423.794, reconstructionLoss: 0.0122335, acc 0.996875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:36:49.437452: step 30501, loss 0.0140353, klDiv: 421.997, CE-loss: 0.0098153, acc 0.9975\n",
      "\n",
      "2018-11-21T17:37:05.157375: step 30601, loss 0.0412704, latentLoss: 432.059, reconstructionLoss: 0.0369498, acc 0.988437\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:37:05.168752: step 30601, loss 0.0304748, klDiv: 428.623, CE-loss: 0.0261886, acc 0.99125\n",
      "\n",
      "2018-11-21T17:37:20.810957: step 30701, loss 0.0184333, latentLoss: 424.765, reconstructionLoss: 0.0141857, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:37:20.828044: step 30701, loss 0.0135786, klDiv: 423.795, CE-loss: 0.00934065, acc 0.997813\n",
      "\n",
      "2018-11-21T17:37:36.520207: step 30801, loss 0.026623, latentLoss: 416.718, reconstructionLoss: 0.0224558, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:37:36.532295: step 30801, loss 0.018747, klDiv: 416.002, CE-loss: 0.014587, acc 0.995\n",
      "\n",
      "2018-11-21T17:37:52.175937: step 30901, loss 0.0229195, latentLoss: 430.4, reconstructionLoss: 0.0186155, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:37:52.184492: step 30901, loss 0.0199651, klDiv: 432.29, CE-loss: 0.0156422, acc 0.995313\n",
      "\n",
      "2018-11-21T17:38:07.757443: step 31001, loss 0.0262381, latentLoss: 474.511, reconstructionLoss: 0.021493, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:38:07.767965: step 31001, loss 0.0200945, klDiv: 476.043, CE-loss: 0.0153341, acc 0.996562\n",
      "\n",
      "2018-11-21T17:38:23.444219: step 31101, loss 0.0311216, latentLoss: 438.64, reconstructionLoss: 0.0267352, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:38:23.458782: step 31101, loss 0.0228959, klDiv: 441.408, CE-loss: 0.0184818, acc 0.995313\n",
      "\n",
      "2018-11-21T17:38:39.092079: step 31201, loss 0.0312379, latentLoss: 424.709, reconstructionLoss: 0.0269908, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:38:39.107894: step 31201, loss 0.0202417, klDiv: 428.818, CE-loss: 0.0159536, acc 0.995625\n",
      "\n",
      "2018-11-21T17:38:54.756072: step 31301, loss 0.0231264, latentLoss: 411.578, reconstructionLoss: 0.0190106, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:38:54.765285: step 31301, loss 0.0179231, klDiv: 411.566, CE-loss: 0.0138074, acc 0.99625\n",
      "\n",
      "2018-11-21T17:39:10.412998: step 31401, loss 0.0242137, latentLoss: 430.361, reconstructionLoss: 0.01991, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:39:10.428111: step 31401, loss 0.0191003, klDiv: 432.067, CE-loss: 0.0147796, acc 0.996875\n",
      "\n",
      "2018-11-21T17:39:26.137360: step 31501, loss 0.024703, latentLoss: 438.513, reconstructionLoss: 0.0203179, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:39:26.156179: step 31501, loss 0.0208649, klDiv: 441.471, CE-loss: 0.0164502, acc 0.994687\n",
      "\n",
      "2018-11-21T17:39:41.853799: step 31601, loss 0.0288227, latentLoss: 415.552, reconstructionLoss: 0.0246672, acc 0.990625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:39:41.864696: step 31601, loss 0.0238284, klDiv: 416.416, CE-loss: 0.0196643, acc 0.993125\n",
      "\n",
      "2018-11-21T17:39:57.524918: step 31701, loss 0.0284264, latentLoss: 423.531, reconstructionLoss: 0.0241911, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:39:57.543699: step 31701, loss 0.0242665, klDiv: 423.184, CE-loss: 0.0200347, acc 0.992813\n",
      "\n",
      "2018-11-21T17:40:13.234240: step 31801, loss 0.018982, latentLoss: 434.048, reconstructionLoss: 0.0146415, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:40:13.251543: step 31801, loss 0.0157532, klDiv: 430.951, CE-loss: 0.0114437, acc 0.998438\n",
      "\n",
      "2018-11-21T17:40:28.896931: step 31901, loss 0.0167748, latentLoss: 408.928, reconstructionLoss: 0.0126855, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:40:28.911723: step 31901, loss 0.0168193, klDiv: 411.87, CE-loss: 0.0127006, acc 0.99625\n",
      "\n",
      "2018-11-21T17:40:44.620424: step 32001, loss 0.0337327, latentLoss: 420.667, reconstructionLoss: 0.029526, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:40:44.635637: step 32001, loss 0.0231298, klDiv: 418.629, CE-loss: 0.0189435, acc 0.995\n",
      "\n",
      "2018-11-21T17:41:00.363975: step 32101, loss 0.0309433, latentLoss: 406.778, reconstructionLoss: 0.0268756, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:41:00.374109: step 32101, loss 0.023403, klDiv: 410.549, CE-loss: 0.0192975, acc 0.993438\n",
      "\n",
      "2018-11-21T17:41:16.049385: step 32201, loss 0.0240128, latentLoss: 413.346, reconstructionLoss: 0.0198793, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:41:16.065439: step 32201, loss 0.0166888, klDiv: 412.32, CE-loss: 0.0125656, acc 0.995625\n",
      "\n",
      "2018-11-21T17:41:31.727204: step 32301, loss 0.0248888, latentLoss: 416.988, reconstructionLoss: 0.0207189, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:41:31.743920: step 32301, loss 0.0224761, klDiv: 416.015, CE-loss: 0.018316, acc 0.994687\n",
      "\n",
      "2018-11-21T17:41:47.412365: step 32401, loss 0.0228334, latentLoss: 407.912, reconstructionLoss: 0.0187543, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:41:47.424326: step 32401, loss 0.0201045, klDiv: 407.607, CE-loss: 0.0160284, acc 0.994687\n",
      "\n",
      "2018-11-21T17:42:03.129055: step 32501, loss 0.0258012, latentLoss: 422.287, reconstructionLoss: 0.0215783, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:42:03.144677: step 32501, loss 0.0196722, klDiv: 423.175, CE-loss: 0.0154405, acc 0.994375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:42:18.884160: step 32601, loss 0.0209195, latentLoss: 441.276, reconstructionLoss: 0.0165067, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:42:18.895000: step 32601, loss 0.0163738, klDiv: 438.298, CE-loss: 0.0119908, acc 0.99625\n",
      "\n",
      "2018-11-21T17:42:34.571745: step 32701, loss 0.0251835, latentLoss: 415.812, reconstructionLoss: 0.0210254, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:42:34.588891: step 32701, loss 0.0208538, klDiv: 416.45, CE-loss: 0.0166893, acc 0.99375\n",
      "\n",
      "2018-11-21T17:42:50.279712: step 32801, loss 0.0266177, latentLoss: 425.088, reconstructionLoss: 0.0223669, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:42:50.296475: step 32801, loss 0.0226206, klDiv: 426.073, CE-loss: 0.0183599, acc 0.992813\n",
      "\n",
      "2018-11-21T17:43:05.914732: step 32901, loss 0.018227, latentLoss: 414.598, reconstructionLoss: 0.0140811, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:43:05.929911: step 32901, loss 0.0136264, klDiv: 418.243, CE-loss: 0.00944393, acc 0.997813\n",
      "\n",
      "2018-11-21T17:43:21.563966: step 33001, loss 0.0216391, latentLoss: 428.263, reconstructionLoss: 0.0173564, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:43:21.575160: step 33001, loss 0.0126122, klDiv: 426.493, CE-loss: 0.0083473, acc 0.998438\n",
      "\n",
      "2018-11-21T17:43:37.263994: step 33101, loss 0.0316968, latentLoss: 420.395, reconstructionLoss: 0.0274929, acc 0.989062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:43:37.280480: step 33101, loss 0.0261605, klDiv: 420.507, CE-loss: 0.0219555, acc 0.991875\n",
      "\n",
      "2018-11-21T17:43:52.937494: step 33201, loss 0.0236124, latentLoss: 410.549, reconstructionLoss: 0.0195069, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:43:52.946504: step 33201, loss 0.0164818, klDiv: 409.766, CE-loss: 0.0123841, acc 0.996875\n",
      "\n",
      "2018-11-21T17:44:08.600467: step 33301, loss 0.0297481, latentLoss: 426.692, reconstructionLoss: 0.0254812, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:44:08.619469: step 33301, loss 0.0194485, klDiv: 427.054, CE-loss: 0.015178, acc 0.995\n",
      "\n",
      "2018-11-21T17:44:24.308184: step 33401, loss 0.0218663, latentLoss: 397.832, reconstructionLoss: 0.017888, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:44:24.325332: step 33401, loss 0.0174549, klDiv: 399.853, CE-loss: 0.0134563, acc 0.996562\n",
      "\n",
      "2018-11-21T17:44:39.955256: step 33501, loss 0.0176533, latentLoss: 414.88, reconstructionLoss: 0.0135045, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:44:39.970225: step 33501, loss 0.0149188, klDiv: 412.556, CE-loss: 0.0107933, acc 0.9975\n",
      "\n",
      "2018-11-21T17:44:55.670265: step 33601, loss 0.0357699, latentLoss: 415.096, reconstructionLoss: 0.0316189, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:44:55.688041: step 33601, loss 0.028476, klDiv: 412.773, CE-loss: 0.0243482, acc 0.994375\n",
      "\n",
      "2018-11-21T17:45:11.379010: step 33701, loss 0.0282451, latentLoss: 402.318, reconstructionLoss: 0.024222, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:45:11.391093: step 33701, loss 0.0242104, klDiv: 403.697, CE-loss: 0.0201734, acc 0.993438\n",
      "\n",
      "2018-11-21T17:45:27.035291: step 33801, loss 0.0239626, latentLoss: 420.235, reconstructionLoss: 0.0197602, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:45:27.046507: step 33801, loss 0.017207, klDiv: 419.001, CE-loss: 0.013017, acc 0.995938\n",
      "\n",
      "2018-11-21T17:45:42.743773: step 33901, loss 0.0237325, latentLoss: 418.329, reconstructionLoss: 0.0195492, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:45:42.753346: step 33901, loss 0.0224499, klDiv: 418.828, CE-loss: 0.0182616, acc 0.995625\n",
      "\n",
      "2018-11-21T17:45:58.441981: step 34001, loss 0.0235978, latentLoss: 419.231, reconstructionLoss: 0.0194055, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:45:58.453577: step 34001, loss 0.0184189, klDiv: 418.142, CE-loss: 0.0142375, acc 0.995\n",
      "\n",
      "2018-11-21T17:46:14.127831: step 34101, loss 0.0197643, latentLoss: 418.426, reconstructionLoss: 0.0155801, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:46:14.137951: step 34101, loss 0.016953, klDiv: 418.353, CE-loss: 0.0127695, acc 0.996562\n",
      "\n",
      "2018-11-21T17:46:29.864582: step 34201, loss 0.020834, latentLoss: 423.566, reconstructionLoss: 0.0165983, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:46:29.875923: step 34201, loss 0.0215849, klDiv: 423.554, CE-loss: 0.0173493, acc 0.994687\n",
      "\n",
      "2018-11-21T17:46:45.746204: step 34301, loss 0.0257324, latentLoss: 414.659, reconstructionLoss: 0.0215858, acc 0.991562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:46:45.755901: step 34301, loss 0.0200192, klDiv: 416.376, CE-loss: 0.0158554, acc 0.994062\n",
      "\n",
      "2018-11-21T17:47:02.270102: step 34401, loss 0.0202678, latentLoss: 409.18, reconstructionLoss: 0.016176, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:47:02.283878: step 34401, loss 0.0205887, klDiv: 411.583, CE-loss: 0.0164729, acc 0.995\n",
      "\n",
      "2018-11-21T17:47:18.148109: step 34501, loss 0.0266951, latentLoss: 425.646, reconstructionLoss: 0.0224386, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:47:18.164493: step 34501, loss 0.0206535, klDiv: 428.391, CE-loss: 0.0163696, acc 0.99625\n",
      "\n",
      "2018-11-21T17:47:33.754034: step 34601, loss 0.030357, latentLoss: 393.589, reconstructionLoss: 0.0264211, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:47:33.772732: step 34601, loss 0.0259891, klDiv: 394.719, CE-loss: 0.022042, acc 0.99375\n",
      "\n",
      "2018-11-21T17:47:49.484205: step 34701, loss 0.0228585, latentLoss: 410.495, reconstructionLoss: 0.0187536, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:47:49.499835: step 34701, loss 0.0172742, klDiv: 409.256, CE-loss: 0.0131817, acc 0.99625\n",
      "\n",
      "2018-11-21T17:48:05.167174: step 34801, loss 0.0257492, latentLoss: 411.067, reconstructionLoss: 0.0216385, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:48:05.177575: step 34801, loss 0.0214019, klDiv: 411.904, CE-loss: 0.0172829, acc 0.994062\n",
      "\n",
      "2018-11-21T17:48:20.834120: step 34901, loss 0.0207912, latentLoss: 445.9, reconstructionLoss: 0.0163322, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:48:20.843595: step 34901, loss 0.0187188, klDiv: 447.102, CE-loss: 0.0142478, acc 0.996562\n",
      "\n",
      "2018-11-21T17:48:36.469432: step 35001, loss 0.0231251, latentLoss: 418.387, reconstructionLoss: 0.0189413, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:48:36.485758: step 35001, loss 0.0185157, klDiv: 420.98, CE-loss: 0.0143059, acc 0.996562\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-35000\n",
      "\n",
      "2018-11-21T17:48:52.656662: step 35101, loss 0.0139494, latentLoss: 398.247, reconstructionLoss: 0.0099669, acc 0.998125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:48:52.670526: step 35101, loss 0.0124968, klDiv: 398.351, CE-loss: 0.00851332, acc 0.997813\n",
      "\n",
      "2018-11-21T17:49:08.246566: step 35201, loss 0.0192414, latentLoss: 403.597, reconstructionLoss: 0.0152055, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:49:08.261698: step 35201, loss 0.014832, klDiv: 400.248, CE-loss: 0.0108295, acc 0.997187\n",
      "\n",
      "2018-11-21T17:49:23.978339: step 35301, loss 0.0309815, latentLoss: 417.835, reconstructionLoss: 0.0268032, acc 0.989688\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:49:23.992026: step 35301, loss 0.0217668, klDiv: 417.125, CE-loss: 0.0175956, acc 0.995313\n",
      "\n",
      "2018-11-21T17:49:39.645499: step 35401, loss 0.0188926, latentLoss: 426.494, reconstructionLoss: 0.0146276, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:49:39.654698: step 35401, loss 0.0139759, klDiv: 424.785, CE-loss: 0.00972805, acc 0.997813\n",
      "\n",
      "2018-11-21T17:49:55.301510: step 35501, loss 0.025114, latentLoss: 431.802, reconstructionLoss: 0.0207959, acc 0.991875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:49:55.314433: step 35501, loss 0.0207679, klDiv: 432.899, CE-loss: 0.016439, acc 0.995938\n",
      "\n",
      "2018-11-21T17:50:11.024761: step 35601, loss 0.0166874, latentLoss: 406.274, reconstructionLoss: 0.0126247, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:50:11.039730: step 35601, loss 0.0159647, klDiv: 408.504, CE-loss: 0.0118797, acc 0.997187\n",
      "\n",
      "2018-11-21T17:50:26.767818: step 35701, loss 0.023706, latentLoss: 403.24, reconstructionLoss: 0.0196736, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:50:26.782356: step 35701, loss 0.0180065, klDiv: 402.699, CE-loss: 0.0139795, acc 0.995625\n",
      "\n",
      "2018-11-21T17:50:42.399526: step 35801, loss 0.0236651, latentLoss: 408.572, reconstructionLoss: 0.0195794, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:50:42.415268: step 35801, loss 0.0183798, klDiv: 408.144, CE-loss: 0.0142983, acc 0.99625\n",
      "\n",
      "2018-11-21T17:50:58.129247: step 35901, loss 0.0223796, latentLoss: 393.339, reconstructionLoss: 0.0184463, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:50:58.138443: step 35901, loss 0.0157519, klDiv: 393.334, CE-loss: 0.0118186, acc 0.9975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T17:51:13.811525: step 36001, loss 0.0274413, latentLoss: 426.273, reconstructionLoss: 0.0231786, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:51:13.822638: step 36001, loss 0.0219739, klDiv: 425.32, CE-loss: 0.0177207, acc 0.995625\n",
      "\n",
      "2018-11-21T17:51:29.416946: step 36101, loss 0.0315048, latentLoss: 406.829, reconstructionLoss: 0.0274365, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:51:29.430501: step 36101, loss 0.0210758, klDiv: 406.051, CE-loss: 0.0170153, acc 0.994375\n",
      "\n",
      "2018-11-21T17:51:45.155405: step 36201, loss 0.0221936, latentLoss: 412.018, reconstructionLoss: 0.0180735, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:51:45.170274: step 36201, loss 0.0180586, klDiv: 408.824, CE-loss: 0.0139703, acc 0.99375\n",
      "\n",
      "2018-11-21T17:52:00.835017: step 36301, loss 0.0242798, latentLoss: 413.825, reconstructionLoss: 0.0201416, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:52:00.847218: step 36301, loss 0.0184068, klDiv: 411.556, CE-loss: 0.0142912, acc 0.995938\n",
      "\n",
      "2018-11-21T17:52:16.528316: step 36401, loss 0.0258346, latentLoss: 415.88, reconstructionLoss: 0.0216758, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:52:16.537489: step 36401, loss 0.0221922, klDiv: 415.419, CE-loss: 0.018038, acc 0.994375\n",
      "\n",
      "2018-11-21T17:52:32.248935: step 36501, loss 0.0208408, latentLoss: 423.683, reconstructionLoss: 0.016604, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:52:32.260188: step 36501, loss 0.0159576, klDiv: 422.359, CE-loss: 0.011734, acc 0.9975\n",
      "\n",
      "2018-11-21T17:52:47.844153: step 36601, loss 0.0195827, latentLoss: 400.134, reconstructionLoss: 0.0155814, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:52:47.860044: step 36601, loss 0.0159803, klDiv: 400.063, CE-loss: 0.0119797, acc 0.996875\n",
      "\n",
      "2018-11-21T17:53:03.500291: step 36701, loss 0.0279133, latentLoss: 406.66, reconstructionLoss: 0.0238467, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:53:03.516787: step 36701, loss 0.0235786, klDiv: 407.837, CE-loss: 0.0195002, acc 0.994375\n",
      "\n",
      "2018-11-21T17:53:19.249547: step 36801, loss 0.0144186, latentLoss: 403.261, reconstructionLoss: 0.010386, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:53:19.261188: step 36801, loss 0.0126608, klDiv: 399.878, CE-loss: 0.00866206, acc 0.997187\n",
      "\n",
      "2018-11-21T17:53:34.933846: step 36901, loss 0.0209299, latentLoss: 400.352, reconstructionLoss: 0.0169263, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:53:34.945480: step 36901, loss 0.014331, klDiv: 400.907, CE-loss: 0.0103219, acc 0.9975\n",
      "\n",
      "2018-11-21T17:53:50.728355: step 37001, loss 0.0173488, latentLoss: 388.209, reconstructionLoss: 0.0134667, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:53:50.739591: step 37001, loss 0.0142407, klDiv: 389.198, CE-loss: 0.0103488, acc 0.99625\n",
      "\n",
      "2018-11-21T17:54:06.423226: step 37101, loss 0.0216862, latentLoss: 394.837, reconstructionLoss: 0.0177378, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:54:06.432693: step 37101, loss 0.0143487, klDiv: 394.384, CE-loss: 0.0104049, acc 0.996875\n",
      "\n",
      "2018-11-21T17:54:22.115745: step 37201, loss 0.016735, latentLoss: 421.674, reconstructionLoss: 0.0125183, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:54:22.131379: step 37201, loss 0.016211, klDiv: 419.38, CE-loss: 0.0120172, acc 0.996562\n",
      "\n",
      "2018-11-21T17:54:37.754045: step 37301, loss 0.0207855, latentLoss: 419.595, reconstructionLoss: 0.0165895, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:54:37.771666: step 37301, loss 0.0169255, klDiv: 419.089, CE-loss: 0.0127346, acc 0.995313\n",
      "\n",
      "2018-11-21T17:54:53.408355: step 37401, loss 0.0167605, latentLoss: 401.465, reconstructionLoss: 0.0127458, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:54:53.424093: step 37401, loss 0.0141787, klDiv: 399.23, CE-loss: 0.0101864, acc 0.996562\n",
      "\n",
      "2018-11-21T17:55:09.103442: step 37501, loss 0.017176, latentLoss: 413.851, reconstructionLoss: 0.0130375, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:55:09.114649: step 37501, loss 0.0145888, klDiv: 414.397, CE-loss: 0.0104448, acc 0.99625\n",
      "\n",
      "2018-11-21T17:55:24.737519: step 37601, loss 0.0442693, latentLoss: 431.219, reconstructionLoss: 0.0399572, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:55:24.752136: step 37601, loss 0.0277313, klDiv: 431.419, CE-loss: 0.0234171, acc 0.99125\n",
      "\n",
      "2018-11-21T17:55:40.432547: step 37701, loss 0.0326254, latentLoss: 421.643, reconstructionLoss: 0.028409, acc 0.990937\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:55:40.446843: step 37701, loss 0.0291833, klDiv: 421.938, CE-loss: 0.0249639, acc 0.993125\n",
      "\n",
      "2018-11-21T17:55:56.127258: step 37801, loss 0.0159045, latentLoss: 389.996, reconstructionLoss: 0.0120046, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:55:56.143997: step 37801, loss 0.0139659, klDiv: 391.231, CE-loss: 0.0100536, acc 0.997187\n",
      "\n",
      "2018-11-21T17:56:11.888144: step 37901, loss 0.0204302, latentLoss: 387.374, reconstructionLoss: 0.0165565, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:56:11.901173: step 37901, loss 0.0145601, klDiv: 389.814, CE-loss: 0.010662, acc 0.996875\n",
      "\n",
      "2018-11-21T17:56:27.565523: step 38001, loss 0.0353554, latentLoss: 415.045, reconstructionLoss: 0.0312049, acc 0.987188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:56:27.575497: step 38001, loss 0.0245414, klDiv: 417.805, CE-loss: 0.0203633, acc 0.991875\n",
      "\n",
      "2018-11-21T17:56:43.213920: step 38101, loss 0.0203782, latentLoss: 399.713, reconstructionLoss: 0.016381, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:56:43.225062: step 38101, loss 0.0182132, klDiv: 398.815, CE-loss: 0.014225, acc 0.994687\n",
      "\n",
      "2018-11-21T17:56:58.865363: step 38201, loss 0.0309734, latentLoss: 427.872, reconstructionLoss: 0.0266946, acc 0.990313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:56:58.876446: step 38201, loss 0.0220946, klDiv: 425.201, CE-loss: 0.0178426, acc 0.994062\n",
      "\n",
      "2018-11-21T17:57:14.623130: step 38301, loss 0.0164996, latentLoss: 404.411, reconstructionLoss: 0.0124555, acc 0.996875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:57:14.634696: step 38301, loss 0.0148476, klDiv: 402.923, CE-loss: 0.0108184, acc 0.9975\n",
      "\n",
      "2018-11-21T17:57:30.375400: step 38401, loss 0.0222156, latentLoss: 402.43, reconstructionLoss: 0.0181913, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:57:30.391400: step 38401, loss 0.0149985, klDiv: 401.003, CE-loss: 0.0109885, acc 0.99625\n",
      "\n",
      "2018-11-21T17:57:46.140076: step 38501, loss 0.0214941, latentLoss: 399.316, reconstructionLoss: 0.017501, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:57:46.149621: step 38501, loss 0.0183272, klDiv: 399.06, CE-loss: 0.0143366, acc 0.995313\n",
      "\n",
      "2018-11-21T17:58:01.865079: step 38601, loss 0.0172093, latentLoss: 383.522, reconstructionLoss: 0.013374, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:58:01.874185: step 38601, loss 0.0182194, klDiv: 383.236, CE-loss: 0.0143871, acc 0.994375\n",
      "\n",
      "2018-11-21T17:58:17.534316: step 38701, loss 0.0228329, latentLoss: 407.696, reconstructionLoss: 0.0187559, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:58:17.543601: step 38701, loss 0.0179472, klDiv: 406.605, CE-loss: 0.0138811, acc 0.99625\n",
      "\n",
      "2018-11-21T17:58:33.348550: step 38801, loss 0.0198302, latentLoss: 439.533, reconstructionLoss: 0.0154349, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:58:33.358006: step 38801, loss 0.0152, klDiv: 440.823, CE-loss: 0.0107918, acc 0.997187\n",
      "\n",
      "2018-11-21T17:58:49.059029: step 38901, loss 0.0278115, latentLoss: 409.082, reconstructionLoss: 0.0237207, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:58:49.077201: step 38901, loss 0.0199847, klDiv: 408.626, CE-loss: 0.0158984, acc 0.995\n",
      "\n",
      "2018-11-21T17:59:04.710923: step 39001, loss 0.021843, latentLoss: 407.069, reconstructionLoss: 0.0177723, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:59:04.725898: step 39001, loss 0.0167185, klDiv: 404.917, CE-loss: 0.0126694, acc 0.997187\n",
      "\n",
      "2018-11-21T17:59:20.484534: step 39101, loss 0.0196434, latentLoss: 400.56, reconstructionLoss: 0.0156378, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:59:20.493523: step 39101, loss 0.016183, klDiv: 401.724, CE-loss: 0.0121658, acc 0.996875\n",
      "\n",
      "2018-11-21T17:59:36.436058: step 39201, loss 0.0221182, latentLoss: 392.03, reconstructionLoss: 0.0181979, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:59:36.447573: step 39201, loss 0.0186152, klDiv: 391.403, CE-loss: 0.0147011, acc 0.995938\n",
      "\n",
      "2018-11-21T17:59:52.698676: step 39301, loss 0.0266609, latentLoss: 404.854, reconstructionLoss: 0.0226123, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T17:59:52.707980: step 39301, loss 0.0188753, klDiv: 403.895, CE-loss: 0.0148363, acc 0.995313\n",
      "\n",
      "2018-11-21T18:00:09.302311: step 39401, loss 0.0174234, latentLoss: 407.578, reconstructionLoss: 0.0133477, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:00:09.320449: step 39401, loss 0.0128178, klDiv: 407.017, CE-loss: 0.00874767, acc 0.998125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T18:00:25.708904: step 39501, loss 0.0208382, latentLoss: 404.25, reconstructionLoss: 0.0167957, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:00:25.721959: step 39501, loss 0.0184161, klDiv: 403.206, CE-loss: 0.0143841, acc 0.994687\n",
      "\n",
      "2018-11-21T18:00:42.092444: step 39601, loss 0.0173479, latentLoss: 379.516, reconstructionLoss: 0.0135528, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:00:42.109053: step 39601, loss 0.0161399, klDiv: 378.812, CE-loss: 0.0123518, acc 0.997187\n",
      "\n",
      "2018-11-21T18:00:58.640328: step 39701, loss 0.0257881, latentLoss: 401.948, reconstructionLoss: 0.0217687, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:00:58.653271: step 39701, loss 0.0193794, klDiv: 399.469, CE-loss: 0.0153847, acc 0.995625\n",
      "\n",
      "2018-11-21T18:01:15.037816: step 39801, loss 0.017923, latentLoss: 371.187, reconstructionLoss: 0.0142111, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:01:15.051193: step 39801, loss 0.0160739, klDiv: 372.881, CE-loss: 0.0123451, acc 0.996562\n",
      "\n",
      "2018-11-21T18:01:31.459500: step 39901, loss 0.0256932, latentLoss: 404.188, reconstructionLoss: 0.0216513, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:01:31.469176: step 39901, loss 0.0191631, klDiv: 403.343, CE-loss: 0.0151296, acc 0.995625\n",
      "\n",
      "2018-11-21T18:01:47.714657: step 40001, loss 0.0259356, latentLoss: 406.324, reconstructionLoss: 0.0218724, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:01:47.725927: step 40001, loss 0.0172119, klDiv: 408.046, CE-loss: 0.0131314, acc 0.995938\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-40000\n",
      "\n",
      "2018-11-21T18:02:04.267843: step 40101, loss 0.0149718, latentLoss: 397.925, reconstructionLoss: 0.0109925, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:02:04.277495: step 40101, loss 0.0115195, klDiv: 397.158, CE-loss: 0.00754791, acc 0.998438\n",
      "\n",
      "2018-11-21T18:02:19.956134: step 40201, loss 0.0200477, latentLoss: 384.992, reconstructionLoss: 0.0161977, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:02:19.975643: step 40201, loss 0.0153468, klDiv: 384.943, CE-loss: 0.0114974, acc 0.995625\n",
      "\n",
      "2018-11-21T18:02:35.656218: step 40301, loss 0.0214309, latentLoss: 402.369, reconstructionLoss: 0.0174072, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:02:35.671062: step 40301, loss 0.0177389, klDiv: 404.355, CE-loss: 0.0136954, acc 0.995\n",
      "\n",
      "2018-11-21T18:02:51.377822: step 40401, loss 0.0232559, latentLoss: 402.656, reconstructionLoss: 0.0192293, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:02:51.394223: step 40401, loss 0.0140457, klDiv: 400.777, CE-loss: 0.0100379, acc 0.9975\n",
      "\n",
      "2018-11-21T18:03:07.066572: step 40501, loss 0.0151235, latentLoss: 403.886, reconstructionLoss: 0.0110847, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:03:07.078018: step 40501, loss 0.01394, klDiv: 402.901, CE-loss: 0.00991102, acc 0.996562\n",
      "\n",
      "2018-11-21T18:03:22.883546: step 40601, loss 0.0229163, latentLoss: 400.018, reconstructionLoss: 0.0189161, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:03:22.895137: step 40601, loss 0.0161162, klDiv: 398.717, CE-loss: 0.012129, acc 0.995313\n",
      "\n",
      "2018-11-21T18:03:38.547120: step 40701, loss 0.0208294, latentLoss: 397.277, reconstructionLoss: 0.0168566, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:03:38.564070: step 40701, loss 0.0163523, klDiv: 397.647, CE-loss: 0.0123758, acc 0.996875\n",
      "\n",
      "2018-11-21T18:03:54.215983: step 40801, loss 0.0187715, latentLoss: 386.905, reconstructionLoss: 0.0149024, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:03:54.225993: step 40801, loss 0.0136252, klDiv: 386.778, CE-loss: 0.00975738, acc 0.997813\n",
      "\n",
      "2018-11-21T18:04:09.892818: step 40901, loss 0.0186052, latentLoss: 383.852, reconstructionLoss: 0.0147667, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:04:09.908268: step 40901, loss 0.0142601, klDiv: 382.385, CE-loss: 0.0104362, acc 0.995313\n",
      "\n",
      "2018-11-21T18:04:25.619771: step 41001, loss 0.0183639, latentLoss: 407.611, reconstructionLoss: 0.0142878, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:04:25.628771: step 41001, loss 0.0156851, klDiv: 407.477, CE-loss: 0.0116103, acc 0.996562\n",
      "\n",
      "2018-11-21T18:04:41.363663: step 41101, loss 0.0217249, latentLoss: 382.2, reconstructionLoss: 0.0179029, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:04:41.381177: step 41101, loss 0.0166469, klDiv: 380.908, CE-loss: 0.0128378, acc 0.995\n",
      "\n",
      "2018-11-21T18:04:57.234227: step 41201, loss 0.0196271, latentLoss: 399.192, reconstructionLoss: 0.0156352, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:04:57.243482: step 41201, loss 0.0147195, klDiv: 399.479, CE-loss: 0.0107247, acc 0.995938\n",
      "\n",
      "2018-11-21T18:05:13.607545: step 41301, loss 0.0188949, latentLoss: 381.465, reconstructionLoss: 0.0150803, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:05:13.616947: step 41301, loss 0.0154693, klDiv: 383.685, CE-loss: 0.0116325, acc 0.996562\n",
      "\n",
      "2018-11-21T18:05:29.637891: step 41401, loss 0.0196842, latentLoss: 397.491, reconstructionLoss: 0.0157093, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:05:29.655501: step 41401, loss 0.0178452, klDiv: 395.484, CE-loss: 0.0138904, acc 0.996562\n",
      "\n",
      "2018-11-21T18:05:45.447262: step 41501, loss 0.0223494, latentLoss: 392.024, reconstructionLoss: 0.0184292, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:05:45.461090: step 41501, loss 0.0179499, klDiv: 392.37, CE-loss: 0.0140262, acc 0.995938\n",
      "\n",
      "2018-11-21T18:06:01.177729: step 41601, loss 0.0174989, latentLoss: 399.452, reconstructionLoss: 0.0135044, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:06:01.191465: step 41601, loss 0.0138054, klDiv: 400.188, CE-loss: 0.00980352, acc 0.9975\n",
      "\n",
      "2018-11-21T18:06:16.811896: step 41701, loss 0.0176016, latentLoss: 385.548, reconstructionLoss: 0.0137461, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:06:16.821931: step 41701, loss 0.0131235, klDiv: 386.988, CE-loss: 0.00925361, acc 0.996562\n",
      "\n",
      "2018-11-21T18:06:32.470792: step 41801, loss 0.0244664, latentLoss: 398.109, reconstructionLoss: 0.0204853, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:06:32.483469: step 41801, loss 0.0185577, klDiv: 400.44, CE-loss: 0.0145533, acc 0.994687\n",
      "\n",
      "2018-11-21T18:06:48.206020: step 41901, loss 0.0206751, latentLoss: 395.034, reconstructionLoss: 0.0167247, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:06:48.220439: step 41901, loss 0.0172891, klDiv: 392.226, CE-loss: 0.0133668, acc 0.995313\n",
      "\n",
      "2018-11-21T18:07:03.907885: step 42001, loss 0.0165572, latentLoss: 384.233, reconstructionLoss: 0.0127148, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:07:03.923561: step 42001, loss 0.0135749, klDiv: 381.215, CE-loss: 0.0097627, acc 0.995938\n",
      "\n",
      "2018-11-21T18:07:20.082125: step 42101, loss 0.0218781, latentLoss: 390.054, reconstructionLoss: 0.0179776, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:07:20.091060: step 42101, loss 0.0153706, klDiv: 388.729, CE-loss: 0.0114833, acc 0.996562\n",
      "\n",
      "2018-11-21T18:07:35.782906: step 42201, loss 0.0236758, latentLoss: 385.587, reconstructionLoss: 0.01982, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:07:35.799123: step 42201, loss 0.0156032, klDiv: 384.839, CE-loss: 0.0117548, acc 0.997187\n",
      "\n",
      "2018-11-21T18:07:51.540295: step 42301, loss 0.0221154, latentLoss: 393.11, reconstructionLoss: 0.0181843, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:07:51.552074: step 42301, loss 0.0147936, klDiv: 394.951, CE-loss: 0.0108441, acc 0.996562\n",
      "\n",
      "2018-11-21T18:08:07.254264: step 42401, loss 0.0165774, latentLoss: 383.349, reconstructionLoss: 0.0127439, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:08:07.269973: step 42401, loss 0.0138027, klDiv: 383.57, CE-loss: 0.00996698, acc 0.996875\n",
      "\n",
      "2018-11-21T18:08:22.909273: step 42501, loss 0.0176363, latentLoss: 392.661, reconstructionLoss: 0.0137096, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:08:22.928040: step 42501, loss 0.0142012, klDiv: 391.66, CE-loss: 0.0102846, acc 0.997187\n",
      "\n",
      "2018-11-21T18:08:39.033374: step 42601, loss 0.0262333, latentLoss: 388.345, reconstructionLoss: 0.0223499, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:08:39.051666: step 42601, loss 0.0212364, klDiv: 389.19, CE-loss: 0.0173445, acc 0.994062\n",
      "\n",
      "2018-11-21T18:08:54.953894: step 42701, loss 0.0206654, latentLoss: 387.492, reconstructionLoss: 0.0167905, acc 0.992813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:08:54.967121: step 42701, loss 0.0152397, klDiv: 389.53, CE-loss: 0.0113444, acc 0.99625\n",
      "\n",
      "2018-11-21T18:09:10.588772: step 42801, loss 0.0199952, latentLoss: 394.025, reconstructionLoss: 0.0160549, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:09:10.602590: step 42801, loss 0.0163593, klDiv: 392.75, CE-loss: 0.0124318, acc 0.995938\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T18:09:26.332006: step 42901, loss 0.0208502, latentLoss: 367.384, reconstructionLoss: 0.0171763, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:09:26.351111: step 42901, loss 0.0145654, klDiv: 368.223, CE-loss: 0.0108832, acc 0.99625\n",
      "\n",
      "2018-11-21T18:09:42.040175: step 43001, loss 0.0167949, latentLoss: 398.175, reconstructionLoss: 0.0128132, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:09:42.055011: step 43001, loss 0.0150072, klDiv: 399.66, CE-loss: 0.0110106, acc 0.996562\n",
      "\n",
      "2018-11-21T18:09:57.737500: step 43101, loss 0.0287204, latentLoss: 387.427, reconstructionLoss: 0.0248461, acc 0.991875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:09:57.746493: step 43101, loss 0.0236865, klDiv: 388.956, CE-loss: 0.019797, acc 0.992813\n",
      "\n",
      "2018-11-21T18:10:13.467679: step 43201, loss 0.0173765, latentLoss: 382.559, reconstructionLoss: 0.0135509, acc 0.996875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:10:13.481994: step 43201, loss 0.0143355, klDiv: 379.208, CE-loss: 0.0105435, acc 0.996562\n",
      "\n",
      "2018-11-21T18:10:29.153264: step 43301, loss 0.0190142, latentLoss: 375.868, reconstructionLoss: 0.0152555, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:10:29.169295: step 43301, loss 0.0139252, klDiv: 376.546, CE-loss: 0.0101597, acc 0.9975\n",
      "\n",
      "2018-11-21T18:10:44.857631: step 43401, loss 0.0155221, latentLoss: 369.288, reconstructionLoss: 0.0118292, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:10:44.868294: step 43401, loss 0.0137428, klDiv: 367.279, CE-loss: 0.01007, acc 0.996875\n",
      "\n",
      "2018-11-21T18:11:00.589019: step 43501, loss 0.0168321, latentLoss: 395.802, reconstructionLoss: 0.0128741, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:11:00.598981: step 43501, loss 0.0105615, klDiv: 395.807, CE-loss: 0.0066034, acc 0.998125\n",
      "\n",
      "2018-11-21T18:11:16.329129: step 43601, loss 0.0198576, latentLoss: 392.314, reconstructionLoss: 0.0159345, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:11:16.342870: step 43601, loss 0.0145679, klDiv: 390.089, CE-loss: 0.010667, acc 0.997187\n",
      "\n",
      "2018-11-21T18:11:32.120019: step 43701, loss 0.0234159, latentLoss: 394.868, reconstructionLoss: 0.0194672, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:11:32.129077: step 43701, loss 0.0182688, klDiv: 395.497, CE-loss: 0.0143139, acc 0.995625\n",
      "\n",
      "2018-11-21T18:11:47.824078: step 43801, loss 0.0193903, latentLoss: 382.815, reconstructionLoss: 0.0155621, acc 0.996875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:11:47.838766: step 43801, loss 0.0153203, klDiv: 382.069, CE-loss: 0.0114996, acc 0.996562\n",
      "\n",
      "2018-11-21T18:12:03.496073: step 43901, loss 0.0148409, latentLoss: 382.583, reconstructionLoss: 0.011015, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:12:03.507491: step 43901, loss 0.0132339, klDiv: 384.078, CE-loss: 0.00939316, acc 0.996875\n",
      "\n",
      "2018-11-21T18:12:19.158742: step 44001, loss 0.0161055, latentLoss: 372.68, reconstructionLoss: 0.0123787, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:12:19.167996: step 44001, loss 0.0127291, klDiv: 371.952, CE-loss: 0.00900957, acc 0.9975\n",
      "\n",
      "2018-11-21T18:12:34.851860: step 44101, loss 0.0259037, latentLoss: 371.612, reconstructionLoss: 0.0221876, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:12:34.867534: step 44101, loss 0.0198282, klDiv: 369.932, CE-loss: 0.0161289, acc 0.995313\n",
      "\n",
      "2018-11-21T18:12:50.571057: step 44201, loss 0.0157586, latentLoss: 405.748, reconstructionLoss: 0.0117011, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:12:50.583216: step 44201, loss 0.0118179, klDiv: 406.465, CE-loss: 0.00775321, acc 0.998438\n",
      "\n",
      "2018-11-21T18:13:06.299969: step 44301, loss 0.0223665, latentLoss: 373.187, reconstructionLoss: 0.0186346, acc 0.993438\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:13:06.316969: step 44301, loss 0.0188994, klDiv: 373.997, CE-loss: 0.0151595, acc 0.995938\n",
      "\n",
      "2018-11-21T18:13:21.949803: step 44401, loss 0.0197088, latentLoss: 391.242, reconstructionLoss: 0.0157964, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:13:21.964234: step 44401, loss 0.0156467, klDiv: 391.637, CE-loss: 0.0117303, acc 0.996875\n",
      "\n",
      "2018-11-21T18:13:37.614505: step 44501, loss 0.0211909, latentLoss: 391.098, reconstructionLoss: 0.0172799, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:13:37.628720: step 44501, loss 0.0155224, klDiv: 388.363, CE-loss: 0.0116387, acc 0.99625\n",
      "\n",
      "2018-11-21T18:13:53.268740: step 44601, loss 0.0142794, latentLoss: 364.071, reconstructionLoss: 0.0106387, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:13:53.285798: step 44601, loss 0.0141114, klDiv: 363.441, CE-loss: 0.010477, acc 0.996875\n",
      "\n",
      "2018-11-21T18:14:08.968834: step 44701, loss 0.0287386, latentLoss: 437.204, reconstructionLoss: 0.0243666, acc 0.9925\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:14:08.985712: step 44701, loss 0.025966, klDiv: 437.637, CE-loss: 0.0215896, acc 0.9925\n",
      "\n",
      "2018-11-21T18:14:24.729818: step 44801, loss 0.0182076, latentLoss: 405.807, reconstructionLoss: 0.0141496, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:14:24.745158: step 44801, loss 0.0154282, klDiv: 403.004, CE-loss: 0.0113982, acc 0.997187\n",
      "\n",
      "2018-11-21T18:14:40.368531: step 44901, loss 0.0180017, latentLoss: 436.081, reconstructionLoss: 0.0136409, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:14:40.382303: step 44901, loss 0.0168356, klDiv: 433.639, CE-loss: 0.0124992, acc 0.995313\n",
      "\n",
      "2018-11-21T18:14:56.164434: step 45001, loss 0.0196879, latentLoss: 379.837, reconstructionLoss: 0.0158895, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:14:56.175983: step 45001, loss 0.0168034, klDiv: 381.238, CE-loss: 0.0129911, acc 0.996875\n",
      "\n",
      "Saved model checkpoint to /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-45000\n",
      "\n",
      "2018-11-21T18:15:12.387430: step 45101, loss 0.0134353, latentLoss: 388.416, reconstructionLoss: 0.00955111, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:15:12.397468: step 45101, loss 0.0110996, klDiv: 389.09, CE-loss: 0.00720867, acc 0.999062\n",
      "\n",
      "2018-11-21T18:15:28.144343: step 45201, loss 0.0132926, latentLoss: 386.164, reconstructionLoss: 0.00943093, acc 0.997187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:15:28.159948: step 45201, loss 0.0106617, klDiv: 384.429, CE-loss: 0.00681738, acc 0.99875\n",
      "\n",
      "2018-11-21T18:15:43.893824: step 45301, loss 0.0113317, latentLoss: 369.79, reconstructionLoss: 0.00763376, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:15:43.910918: step 45301, loss 0.00988287, klDiv: 368.489, CE-loss: 0.00619798, acc 0.998125\n",
      "\n",
      "2018-11-21T18:15:59.599712: step 45401, loss 0.012536, latentLoss: 370.912, reconstructionLoss: 0.00882685, acc 0.997813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:15:59.616921: step 45401, loss 0.0110891, klDiv: 371.172, CE-loss: 0.00737736, acc 0.998438\n",
      "\n",
      "2018-11-21T18:16:15.201199: step 45501, loss 0.0144578, latentLoss: 389.027, reconstructionLoss: 0.0105675, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:16:15.212392: step 45501, loss 0.0110919, klDiv: 389.329, CE-loss: 0.00719862, acc 0.998438\n",
      "\n",
      "2018-11-21T18:16:30.975024: step 45601, loss 0.0180451, latentLoss: 383.388, reconstructionLoss: 0.0142112, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:16:30.991007: step 45601, loss 0.0150144, klDiv: 382.169, CE-loss: 0.0111927, acc 0.996875\n",
      "\n",
      "2018-11-21T18:16:46.786380: step 45701, loss 0.0132499, latentLoss: 372.798, reconstructionLoss: 0.0095219, acc 0.997187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:16:46.802575: step 45701, loss 0.0117358, klDiv: 374.027, CE-loss: 0.0079955, acc 0.998125\n",
      "\n",
      "2018-11-21T18:17:02.474160: step 45801, loss 0.0170909, latentLoss: 366.07, reconstructionLoss: 0.0134302, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:17:02.484521: step 45801, loss 0.0134954, klDiv: 366.656, CE-loss: 0.00982882, acc 0.996875\n",
      "\n",
      "2018-11-21T18:17:18.200133: step 45901, loss 0.0185606, latentLoss: 362.621, reconstructionLoss: 0.0149344, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:17:18.212277: step 45901, loss 0.0156357, klDiv: 364.217, CE-loss: 0.0119935, acc 0.995938\n",
      "\n",
      "2018-11-21T18:17:33.921492: step 46001, loss 0.0203897, latentLoss: 361.049, reconstructionLoss: 0.0167793, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:17:33.936362: step 46001, loss 0.01708, klDiv: 361.117, CE-loss: 0.0134689, acc 0.996562\n",
      "\n",
      "2018-11-21T18:17:49.624324: step 46101, loss 0.0191796, latentLoss: 386.899, reconstructionLoss: 0.0153106, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:17:49.640496: step 46101, loss 0.0150018, klDiv: 385.314, CE-loss: 0.0111486, acc 0.995938\n",
      "\n",
      "2018-11-21T18:18:05.376461: step 46201, loss 0.0156734, latentLoss: 374.172, reconstructionLoss: 0.0119317, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:18:05.385772: step 46201, loss 0.0124846, klDiv: 372.186, CE-loss: 0.00876275, acc 0.99875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T18:18:21.041673: step 46301, loss 0.0193754, latentLoss: 415.464, reconstructionLoss: 0.0152207, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:18:21.059740: step 46301, loss 0.0164862, klDiv: 417.151, CE-loss: 0.0123147, acc 0.995625\n",
      "\n",
      "2018-11-21T18:18:37.195575: step 46401, loss 0.0190506, latentLoss: 367.508, reconstructionLoss: 0.0153755, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:18:37.207070: step 46401, loss 0.0148305, klDiv: 367.674, CE-loss: 0.0111538, acc 0.996875\n",
      "\n",
      "2018-11-21T18:18:53.229189: step 46501, loss 0.0136359, latentLoss: 359.878, reconstructionLoss: 0.0100371, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:18:53.241293: step 46501, loss 0.010506, klDiv: 358.847, CE-loss: 0.00691757, acc 0.998438\n",
      "\n",
      "2018-11-21T18:19:09.178288: step 46601, loss 0.0156603, latentLoss: 378.71, reconstructionLoss: 0.0118732, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:19:09.195236: step 46601, loss 0.0113575, klDiv: 377.358, CE-loss: 0.0075839, acc 0.99875\n",
      "\n",
      "2018-11-21T18:19:25.521900: step 46701, loss 0.0177423, latentLoss: 378.418, reconstructionLoss: 0.0139581, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:19:25.540647: step 46701, loss 0.0142869, klDiv: 379.2, CE-loss: 0.0104949, acc 0.99625\n",
      "\n",
      "2018-11-21T18:19:42.160153: step 46801, loss 0.0119867, latentLoss: 373.793, reconstructionLoss: 0.00824879, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:19:42.175109: step 46801, loss 0.0113084, klDiv: 374.89, CE-loss: 0.00755949, acc 0.998125\n",
      "\n",
      "2018-11-21T18:19:58.608074: step 46901, loss 0.0208775, latentLoss: 372.743, reconstructionLoss: 0.0171501, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:19:58.617447: step 46901, loss 0.0154696, klDiv: 371.269, CE-loss: 0.0117569, acc 0.995938\n",
      "\n",
      "2018-11-21T18:20:14.953769: step 47001, loss 0.0187301, latentLoss: 384.51, reconstructionLoss: 0.014885, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:20:14.965471: step 47001, loss 0.0133919, klDiv: 383.78, CE-loss: 0.00955406, acc 0.996875\n",
      "\n",
      "2018-11-21T18:20:31.426672: step 47101, loss 0.0162595, latentLoss: 383.634, reconstructionLoss: 0.0124231, acc 0.995938\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:20:31.442755: step 47101, loss 0.0135733, klDiv: 382.23, CE-loss: 0.00975097, acc 0.996875\n",
      "\n",
      "2018-11-21T18:20:47.678871: step 47201, loss 0.0704859, latentLoss: 382.022, reconstructionLoss: 0.0666656, acc 0.981875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:20:47.690621: step 47201, loss 0.0621313, klDiv: 375.824, CE-loss: 0.0583731, acc 0.981562\n",
      "\n",
      "2018-11-21T18:21:03.985531: step 47301, loss 0.0179649, latentLoss: 414.252, reconstructionLoss: 0.0138224, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:21:04.000066: step 47301, loss 0.0160603, klDiv: 412.35, CE-loss: 0.0119368, acc 0.996875\n",
      "\n",
      "2018-11-21T18:21:20.291499: step 47401, loss 0.0123262, latentLoss: 389.846, reconstructionLoss: 0.00842775, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:21:20.300725: step 47401, loss 0.00928628, klDiv: 388.992, CE-loss: 0.00539636, acc 0.998438\n",
      "\n",
      "2018-11-21T18:21:36.660378: step 47501, loss 0.0142211, latentLoss: 356.379, reconstructionLoss: 0.0106574, acc 0.997813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:21:36.676119: step 47501, loss 0.0101372, klDiv: 356.067, CE-loss: 0.00657657, acc 0.998125\n",
      "\n",
      "2018-11-21T18:21:53.047631: step 47601, loss 0.0162823, latentLoss: 371.408, reconstructionLoss: 0.0125682, acc 0.997187\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:21:53.058582: step 47601, loss 0.0149728, klDiv: 372.282, CE-loss: 0.01125, acc 0.996562\n",
      "\n",
      "2018-11-21T18:22:09.584648: step 47701, loss 0.0214626, latentLoss: 371.272, reconstructionLoss: 0.0177499, acc 0.99375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:22:09.597083: step 47701, loss 0.0137471, klDiv: 372.208, CE-loss: 0.010025, acc 0.996875\n",
      "\n",
      "2018-11-21T18:22:25.857776: step 47801, loss 0.0107429, latentLoss: 369.429, reconstructionLoss: 0.00704862, acc 0.998125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:22:25.873352: step 47801, loss 0.0102478, klDiv: 368.099, CE-loss: 0.00656684, acc 0.998125\n",
      "\n",
      "2018-11-21T18:22:41.649351: step 47901, loss 0.0155715, latentLoss: 360.138, reconstructionLoss: 0.0119701, acc 0.995313\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:22:41.664869: step 47901, loss 0.0114939, klDiv: 361.076, CE-loss: 0.0078831, acc 0.997813\n",
      "\n",
      "2018-11-21T18:22:57.359376: step 48001, loss 0.0136736, latentLoss: 380.005, reconstructionLoss: 0.00987351, acc 0.997813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:22:57.369172: step 48001, loss 0.0113047, klDiv: 381.226, CE-loss: 0.00749245, acc 0.998438\n",
      "\n",
      "2018-11-21T18:23:13.097374: step 48101, loss 0.0171176, latentLoss: 377.121, reconstructionLoss: 0.0133464, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:23:13.106669: step 48101, loss 0.0135216, klDiv: 378.772, CE-loss: 0.00973392, acc 0.9975\n",
      "\n",
      "2018-11-21T18:23:28.767380: step 48201, loss 0.00971316, latentLoss: 375.221, reconstructionLoss: 0.00596095, acc 0.997813\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:23:28.780732: step 48201, loss 0.00982275, klDiv: 377.769, CE-loss: 0.00604506, acc 0.998125\n",
      "\n",
      "2018-11-21T18:23:44.500045: step 48301, loss 0.0188164, latentLoss: 376.831, reconstructionLoss: 0.0150481, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:23:44.513697: step 48301, loss 0.0156868, klDiv: 375.772, CE-loss: 0.011929, acc 0.995938\n",
      "\n",
      "2018-11-21T18:24:00.233254: step 48401, loss 0.0150849, latentLoss: 388.002, reconstructionLoss: 0.0112049, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:24:00.248206: step 48401, loss 0.0127055, klDiv: 387.401, CE-loss: 0.00883145, acc 0.996875\n",
      "\n",
      "2018-11-21T18:24:15.999373: step 48501, loss 0.0255293, latentLoss: 369.176, reconstructionLoss: 0.0218375, acc 0.991875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:24:16.008119: step 48501, loss 0.0163464, klDiv: 367.003, CE-loss: 0.0126764, acc 0.995625\n",
      "\n",
      "2018-11-21T18:24:31.755912: step 48601, loss 0.0136245, latentLoss: 370.269, reconstructionLoss: 0.00992177, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:24:31.772438: step 48601, loss 0.0110714, klDiv: 370.921, CE-loss: 0.00736222, acc 0.99875\n",
      "\n",
      "2018-11-21T18:24:47.490252: step 48701, loss 0.0237241, latentLoss: 375.036, reconstructionLoss: 0.0199737, acc 0.993125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:24:47.505696: step 48701, loss 0.0183423, klDiv: 374.218, CE-loss: 0.0146002, acc 0.995313\n",
      "\n",
      "2018-11-21T18:25:03.200214: step 48801, loss 0.0185285, latentLoss: 364.425, reconstructionLoss: 0.0148843, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:25:03.214697: step 48801, loss 0.0121571, klDiv: 363.869, CE-loss: 0.00851837, acc 0.997813\n",
      "\n",
      "2018-11-21T18:25:18.844817: step 48901, loss 0.0143409, latentLoss: 376.551, reconstructionLoss: 0.0105754, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:25:18.854602: step 48901, loss 0.0129273, klDiv: 378.861, CE-loss: 0.00913867, acc 0.997187\n",
      "\n",
      "2018-11-21T18:25:34.482254: step 49001, loss 0.0156427, latentLoss: 374.984, reconstructionLoss: 0.0118929, acc 0.996562\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:25:34.496706: step 49001, loss 0.0110235, klDiv: 373.426, CE-loss: 0.00728928, acc 0.998125\n",
      "\n",
      "2018-11-21T18:25:50.134286: step 49101, loss 0.0187917, latentLoss: 364.681, reconstructionLoss: 0.0151449, acc 0.994062\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:25:50.151380: step 49101, loss 0.0123713, klDiv: 365.507, CE-loss: 0.00871626, acc 0.9975\n",
      "\n",
      "2018-11-21T18:26:05.811196: step 49201, loss 0.0147876, latentLoss: 382.519, reconstructionLoss: 0.0109624, acc 0.99625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:26:05.827703: step 49201, loss 0.0117327, klDiv: 383.576, CE-loss: 0.00789692, acc 0.998438\n",
      "\n",
      "2018-11-21T18:26:21.561796: step 49301, loss 0.0160288, latentLoss: 360.013, reconstructionLoss: 0.0124287, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:26:21.570370: step 49301, loss 0.0113895, klDiv: 359.919, CE-loss: 0.00779032, acc 0.997813\n",
      "\n",
      "2018-11-21T18:26:37.256578: step 49401, loss 0.0183625, latentLoss: 389.933, reconstructionLoss: 0.0144632, acc 0.995\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:26:37.270304: step 49401, loss 0.0119871, klDiv: 387.495, CE-loss: 0.0081121, acc 0.998125\n",
      "\n",
      "2018-11-21T18:26:52.997201: step 49501, loss 0.0174249, latentLoss: 386.417, reconstructionLoss: 0.0135607, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:26:53.006447: step 49501, loss 0.0139125, klDiv: 382.931, CE-loss: 0.0100832, acc 0.99625\n",
      "\n",
      "2018-11-21T18:27:08.771716: step 49601, loss 0.013876, latentLoss: 381.358, reconstructionLoss: 0.0100624, acc 0.9975\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:27:08.786012: step 49601, loss 0.0108207, klDiv: 382.373, CE-loss: 0.00699696, acc 0.99875\n",
      "\n",
      "2018-11-21T18:27:24.444081: step 49701, loss 0.0212678, latentLoss: 386.998, reconstructionLoss: 0.0173978, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:27:24.460572: step 49701, loss 0.0135061, klDiv: 387.038, CE-loss: 0.00963567, acc 0.996562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21T18:27:40.192933: step 49801, loss 0.0208243, latentLoss: 393.74, reconstructionLoss: 0.0168869, acc 0.994687\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:27:40.209361: step 49801, loss 0.0164973, klDiv: 393.358, CE-loss: 0.0125637, acc 0.995938\n",
      "\n",
      "2018-11-21T18:27:55.959700: step 49901, loss 0.013541, latentLoss: 352.611, reconstructionLoss: 0.0100149, acc 0.995625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:27:55.977212: step 49901, loss 0.0129158, klDiv: 350.171, CE-loss: 0.00941409, acc 0.99625\n",
      "\n",
      "2018-11-21T18:28:11.747051: step 50001, loss 0.0168309, latentLoss: 369.208, reconstructionLoss: 0.0131388, acc 0.994375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-21T18:28:11.764156: step 50001, loss 0.0146152, klDiv: 370.846, CE-loss: 0.0109068, acc 0.996875\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "/home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-50000.data-00000-of-00001.tempstate10334406296181928179; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-36-900d8d3add4a>:116)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power/_337, beta2_power/_339, conv-maxpool-3/W/_341, conv-maxpool-3/W/Adam/_343, conv-maxpool-3/W/Adam_1/_345, conv-maxpool-3/b/_347, conv-maxpool-3/b/Adam/_349, conv-maxpool-3/b/Adam_1/_351, conv-maxpool-4/W/_353, conv-maxpool-4/W/Adam/_355, conv-maxpool-4/W/Adam_1/_357, conv-maxpool-4/b/_359, conv-maxpool-4/b/Adam/_361, conv-maxpool-4/b/Adam_1/_363, conv-maxpool-5/W/_365, conv-maxpool-5/W/Adam/_367, conv-maxpool-5/W/Adam_1/_369, conv-maxpool-5/b/_371, conv-maxpool-5/b/Adam/_373, conv-maxpool-5/b/Adam_1/_375, conv-maxpool-6/W/_377, conv-maxpool-6/W/Adam/_379, conv-maxpool-6/W/Adam_1/_381, conv-maxpool-6/b/_383, conv-maxpool-6/b/Adam/_385, conv-maxpool-6/b/Adam_1/_387, decoder_output_to_logits/fully_connected/biases/_389, decoder_output_to_logits/fully_connected/biases/Adam/_391, decoder_output_to_logits/fully_connected/biases/Adam_1/_393, decoder_output_to_logits/fully_connected/weights/_395, decoder_output_to_logits/fully_connected/weights/Adam/_397, decoder_output_to_logits/fully_connected/weights/Adam_1/_399, embedding/W/_401, embedding/W/Adam/_403, embedding/W/Adam_1/_405, fully_connected/biases/_407, fully_connected/biases/Adam/_409, fully_connected/biases/Adam_1/_411, fully_connected/weights/_413, fully_connected/weights/Adam/_415, fully_connected/weights/Adam_1/_417, fully_connected_1/biases/_419, fully_connected_1/biases/Adam/_421, fully_connected_1/biases/Adam_1/_423, fully_connected_1/weights/_425, fully_connected_1/weights/Adam/_427, fully_connected_1/weights/Adam_1/_429, fully_connected_2/biases/_431, fully_connected_2/biases/Adam/_433, fully_connected_2/biases/Adam_1/_435, fully_connected_2/weights/_437, fully_connected_2/weights/Adam/_439, fully_connected_2/weights/Adam_1/_441, fully_connected_3/biases/_443, fully_connected_3/weights/_445, global_step, lstm_cell/bias/_447, lstm_cell/bias/Adam/_449, lstm_cell/bias/Adam_1/_451, lstm_cell/kernel/_453, lstm_cell/kernel/Adam/_455, lstm_cell/kernel/Adam_1/_457)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-36-900d8d3add4a>\", line 116, in <module>\n    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-50000.data-00000-of-00001.tempstate10334406296181928179; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-36-900d8d3add4a>:116)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power/_337, beta2_power/_339, conv-maxpool-3/W/_341, conv-maxpool-3/W/Adam/_343, conv-maxpool-3/W/Adam_1/_345, conv-maxpool-3/b/_347, conv-maxpool-3/b/Adam/_349, conv-maxpool-3/b/Adam_1/_351, conv-maxpool-4/W/_353, conv-maxpool-4/W/Adam/_355, conv-maxpool-4/W/Adam_1/_357, conv-maxpool-4/b/_359, conv-maxpool-4/b/Adam/_361, conv-maxpool-4/b/Adam_1/_363, conv-maxpool-5/W/_365, conv-maxpool-5/W/Adam/_367, conv-maxpool-5/W/Adam_1/_369, conv-maxpool-5/b/_371, conv-maxpool-5/b/Adam/_373, conv-maxpool-5/b/Adam_1/_375, conv-maxpool-6/W/_377, conv-maxpool-6/W/Adam/_379, conv-maxpool-6/W/Adam_1/_381, conv-maxpool-6/b/_383, conv-maxpool-6/b/Adam/_385, conv-maxpool-6/b/Adam_1/_387, decoder_output_to_logits/fully_connected/biases/_389, decoder_output_to_logits/fully_connected/biases/Adam/_391, decoder_output_to_logits/fully_connected/biases/Adam_1/_393, decoder_output_to_logits/fully_connected/weights/_395, decoder_output_to_logits/fully_connected/weights/Adam/_397, decoder_output_to_logits/fully_connected/weights/Adam_1/_399, embedding/W/_401, embedding/W/Adam/_403, embedding/W/Adam_1/_405, fully_connected/biases/_407, fully_connected/biases/Adam/_409, fully_connected/biases/Adam_1/_411, fully_connected/weights/_413, fully_connected/weights/Adam/_415, fully_connected/weights/Adam_1/_417, fully_connected_1/biases/_419, fully_connected_1/biases/Adam/_421, fully_connected_1/biases/Adam_1/_423, fully_connected_1/weights/_425, fully_connected_1/weights/Adam/_427, fully_connected_1/weights/Adam_1/_429, fully_connected_2/biases/_431, fully_connected_2/biases/Adam/_433, fully_connected_2/biases/Adam_1/_435, fully_connected_2/weights/_437, fully_connected_2/weights/Adam/_439, fully_connected_2/weights/Adam_1/_441, fully_connected_3/biases/_443, fully_connected_3/weights/_445, global_step, lstm_cell/bias/_447, lstm_cell/bias/Adam/_449, lstm_cell/bias/Adam_1/_451, lstm_cell/kernel/_453, lstm_cell/kernel/Adam/_455, lstm_cell/kernel/Adam_1/_457)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-50000.data-00000-of-00001.tempstate10334406296181928179; No space left on device\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power/_337, beta2_power/_339, conv-maxpool-3/W/_341, conv-maxpool-3/W/Adam/_343, conv-maxpool-3/W/Adam_1/_345, conv-maxpool-3/b/_347, conv-maxpool-3/b/Adam/_349, conv-maxpool-3/b/Adam_1/_351, conv-maxpool-4/W/_353, conv-maxpool-4/W/Adam/_355, conv-maxpool-4/W/Adam_1/_357, conv-maxpool-4/b/_359, conv-maxpool-4/b/Adam/_361, conv-maxpool-4/b/Adam_1/_363, conv-maxpool-5/W/_365, conv-maxpool-5/W/Adam/_367, conv-maxpool-5/W/Adam_1/_369, conv-maxpool-5/b/_371, conv-maxpool-5/b/Adam/_373, conv-maxpool-5/b/Adam_1/_375, conv-maxpool-6/W/_377, conv-maxpool-6/W/Adam/_379, conv-maxpool-6/W/Adam_1/_381, conv-maxpool-6/b/_383, conv-maxpool-6/b/Adam/_385, conv-maxpool-6/b/Adam_1/_387, decoder_output_to_logits/fully_connected/biases/_389, decoder_output_to_logits/fully_connected/biases/Adam/_391, decoder_output_to_logits/fully_connected/biases/Adam_1/_393, decoder_output_to_logits/fully_connected/weights/_395, decoder_output_to_logits/fully_connected/weights/Adam/_397, decoder_output_to_logits/fully_connected/weights/Adam_1/_399, embedding/W/_401, embedding/W/Adam/_403, embedding/W/Adam_1/_405, fully_connected/biases/_407, fully_connected/biases/Adam/_409, fully_connected/biases/Adam_1/_411, fully_connected/weights/_413, fully_connected/weights/Adam/_415, fully_connected/weights/Adam_1/_417, fully_connected_1/biases/_419, fully_connected_1/biases/Adam/_421, fully_connected_1/biases/Adam_1/_423, fully_connected_1/weights/_425, fully_connected_1/weights/Adam/_427, fully_connected_1/weights/Adam_1/_429, fully_connected_2/biases/_431, fully_connected_2/biases/Adam/_433, fully_connected_2/biases/Adam_1/_435, fully_connected_2/weights/_437, fully_connected_2/weights/Adam/_439, fully_connected_2/weights/Adam_1/_441, fully_connected_3/biases/_443, fully_connected_3/weights/_445, global_step, lstm_cell/bias/_447, lstm_cell/bias/Adam/_449, lstm_cell/bias/Adam_1/_451, lstm_cell/kernel/_453, lstm_cell/kernel/Adam/_455, lstm_cell/kernel/Adam_1/_457)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-900d8d3add4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model checkpoint to {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-50000.data-00000-of-00001.tempstate10334406296181928179; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-36-900d8d3add4a>:116)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power/_337, beta2_power/_339, conv-maxpool-3/W/_341, conv-maxpool-3/W/Adam/_343, conv-maxpool-3/W/Adam_1/_345, conv-maxpool-3/b/_347, conv-maxpool-3/b/Adam/_349, conv-maxpool-3/b/Adam_1/_351, conv-maxpool-4/W/_353, conv-maxpool-4/W/Adam/_355, conv-maxpool-4/W/Adam_1/_357, conv-maxpool-4/b/_359, conv-maxpool-4/b/Adam/_361, conv-maxpool-4/b/Adam_1/_363, conv-maxpool-5/W/_365, conv-maxpool-5/W/Adam/_367, conv-maxpool-5/W/Adam_1/_369, conv-maxpool-5/b/_371, conv-maxpool-5/b/Adam/_373, conv-maxpool-5/b/Adam_1/_375, conv-maxpool-6/W/_377, conv-maxpool-6/W/Adam/_379, conv-maxpool-6/W/Adam_1/_381, conv-maxpool-6/b/_383, conv-maxpool-6/b/Adam/_385, conv-maxpool-6/b/Adam_1/_387, decoder_output_to_logits/fully_connected/biases/_389, decoder_output_to_logits/fully_connected/biases/Adam/_391, decoder_output_to_logits/fully_connected/biases/Adam_1/_393, decoder_output_to_logits/fully_connected/weights/_395, decoder_output_to_logits/fully_connected/weights/Adam/_397, decoder_output_to_logits/fully_connected/weights/Adam_1/_399, embedding/W/_401, embedding/W/Adam/_403, embedding/W/Adam_1/_405, fully_connected/biases/_407, fully_connected/biases/Adam/_409, fully_connected/biases/Adam_1/_411, fully_connected/weights/_413, fully_connected/weights/Adam/_415, fully_connected/weights/Adam_1/_417, fully_connected_1/biases/_419, fully_connected_1/biases/Adam/_421, fully_connected_1/biases/Adam_1/_423, fully_connected_1/weights/_425, fully_connected_1/weights/Adam/_427, fully_connected_1/weights/Adam_1/_429, fully_connected_2/biases/_431, fully_connected_2/biases/Adam/_433, fully_connected_2/biases/Adam_1/_435, fully_connected_2/weights/_437, fully_connected_2/weights/Adam/_439, fully_connected_2/weights/Adam_1/_441, fully_connected_3/biases/_443, fully_connected_3/weights/_445, global_step, lstm_cell/bias/_447, lstm_cell/bias/Adam/_449, lstm_cell/bias/Adam_1/_451, lstm_cell/kernel/_453, lstm_cell/kernel/Adam/_455, lstm_cell/kernel/Adam_1/_457)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-36-900d8d3add4a>\", line 116, in <module>\n    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): /home/fabsta/projects/deeplearning/vae_playground/runs/1542813331/checkpoints/model-50000.data-00000-of-00001.tempstate10334406296181928179; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-36-900d8d3add4a>:116)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power/_337, beta2_power/_339, conv-maxpool-3/W/_341, conv-maxpool-3/W/Adam/_343, conv-maxpool-3/W/Adam_1/_345, conv-maxpool-3/b/_347, conv-maxpool-3/b/Adam/_349, conv-maxpool-3/b/Adam_1/_351, conv-maxpool-4/W/_353, conv-maxpool-4/W/Adam/_355, conv-maxpool-4/W/Adam_1/_357, conv-maxpool-4/b/_359, conv-maxpool-4/b/Adam/_361, conv-maxpool-4/b/Adam_1/_363, conv-maxpool-5/W/_365, conv-maxpool-5/W/Adam/_367, conv-maxpool-5/W/Adam_1/_369, conv-maxpool-5/b/_371, conv-maxpool-5/b/Adam/_373, conv-maxpool-5/b/Adam_1/_375, conv-maxpool-6/W/_377, conv-maxpool-6/W/Adam/_379, conv-maxpool-6/W/Adam_1/_381, conv-maxpool-6/b/_383, conv-maxpool-6/b/Adam/_385, conv-maxpool-6/b/Adam_1/_387, decoder_output_to_logits/fully_connected/biases/_389, decoder_output_to_logits/fully_connected/biases/Adam/_391, decoder_output_to_logits/fully_connected/biases/Adam_1/_393, decoder_output_to_logits/fully_connected/weights/_395, decoder_output_to_logits/fully_connected/weights/Adam/_397, decoder_output_to_logits/fully_connected/weights/Adam_1/_399, embedding/W/_401, embedding/W/Adam/_403, embedding/W/Adam_1/_405, fully_connected/biases/_407, fully_connected/biases/Adam/_409, fully_connected/biases/Adam_1/_411, fully_connected/weights/_413, fully_connected/weights/Adam/_415, fully_connected/weights/Adam_1/_417, fully_connected_1/biases/_419, fully_connected_1/biases/Adam/_421, fully_connected_1/biases/Adam_1/_423, fully_connected_1/weights/_425, fully_connected_1/weights/Adam/_427, fully_connected_1/weights/Adam_1/_429, fully_connected_2/biases/_431, fully_connected_2/biases/Adam/_433, fully_connected_2/biases/Adam_1/_435, fully_connected_2/weights/_437, fully_connected_2/weights/Adam/_439, fully_connected_2/weights/Adam_1/_441, fully_connected_3/biases/_443, fully_connected_3/weights/_445, global_step, lstm_cell/bias/_447, lstm_cell/bias/Adam/_449, lstm_cell/bias/Adam_1/_451, lstm_cell/kernel/_453, lstm_cell/kernel/Adam/_455, lstm_cell/kernel/Adam_1/_457)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# in the scope of a default TF graph... why? TL;DR: It's unnecessary, but it's a good practice to follow.\n",
    "# See https://stackoverflow.com/a/39616491\n",
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True)\n",
    "    )\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    # here we begin the tensorflow session\n",
    "    with sess.as_default():\n",
    "        cnn = model.CDN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            vocab_size=FLAGS.vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            max_molecule_length=FLAGS.max_molecule_length,\n",
    "            gaussian_samples=FLAGS.unit_gaussian_dim,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            variational=True,\n",
    "            test_mode=False,\n",
    "            generation_mode=False\n",
    "        )\n",
    "        # cnn is a simple CDN object, nothing inherited from TF classes\n",
    "        assert isinstance(cnn, model.CDN)\n",
    " \n",
    "        # TODO why do we need a global_step non-trainable variable?\n",
    "        # According to this StackOverflow, it's a way to train AdamOptimizer with custom learning rate, see\n",
    "        # https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay\n",
    "        global_step = tf.Variable(1, name=\"global_step\", trainable=False)\n",
    " \n",
    "        # A scalar `Tensor` of the same type as `learning_rate`.  The decayed learning rate.\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=FLAGS.initial_learning_rate, global_step=global_step,\n",
    "                                                   decay_steps=5000, decay_rate=0.96, staircase=True)\n",
    " \n",
    "        # somehow confusing how the learning rate is passed to the AdamOptimizer...\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        # but we know the type: it's AdamOptimizer instance, inherited from Optimizer\n",
    "        assert isinstance(optimizer, tf.train.AdamOptimizer)\n",
    " \n",
    "        # CDN.encode() defines the computational graph - returns two tensors\n",
    "        # TODO explore encode() in detail\n",
    "       \n",
    "        #####\n",
    "        # Encoding part\n",
    "        #####\n",
    "        # returns pooling and loss function (mean_latent_loss)\n",
    "        encoded, latent_loss = cnn.encode()\n",
    "        assert isinstance(encoded, tf.Tensor)\n",
    "        assert isinstance(latent_loss, tf.Tensor)\n",
    "       \n",
    "        #####\n",
    "        # Decoder part\n",
    "        #####\n",
    "        # another Tensor output from the decoder\n",
    "        logits = cnn.decode_rnn(encoded)\n",
    "        assert isinstance(logits, tf.Tensor)\n",
    " \n",
    "        #####\n",
    "        # loss function\n",
    "        #####\n",
    "        # and CDN.loss() returns two Tensors: loss and accuracy\n",
    "        loss, accuracy = cnn.loss(logits, latent_loss)\n",
    "        assert isinstance(loss, tf.Tensor)\n",
    "        assert isinstance(accuracy, tf.Tensor)\n",
    " \n",
    "        # grads_and_vars is a list\n",
    "        # It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\".\n",
    "        # It is the first part of minimize()\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    " \n",
    "        # The second part of minimize() which applies gradients to variables\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        assert isinstance(train_op, tf.Operation)\n",
    " \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    " \n",
    "        # Output directory\n",
    "        timestamp = str(int(time.time()))\n",
    "        if FLAGS.parameters_file is not None:\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, FLAGS.parameters_file))\n",
    "        else:\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print((\"Writing to {}\\n\".format(out_dir)))\n",
    " \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        kl_div_summary = tf.summary.scalar(\"KL-div\", latent_loss)\n",
    "        CE_loss_summary = tf.summary.scalar(\"CE-loss\", cnn.CE_loss)\n",
    "        learning_rate_summary = tf.summary.scalar(\"learningRate\", learning_rate)\n",
    " \n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge(\n",
    "            [loss_summary, acc_summary, grad_summaries_merged, kl_div_summary, CE_loss_summary,\n",
    "             learning_rate_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\") # file directory\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph) # save in file\n",
    " \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary, kl_div_summary, CE_loss_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    " \n",
    "        # Checkpoint directory.\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "       \n",
    "        # Resuming training\n",
    "        if (FLAGS.load_param) and (FLAGS.parameters_file is not None):\n",
    "            # model file to restart training\n",
    "            checkpoint_file = FLAGS.parameters_file\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            print(\"Loaded: \" + str(checkpoint_file))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # Generate batches - a batch iterator for a data set\n",
    "        num_epochs  = 1\n",
    "        # Simulate 1 batch and 1 epoch\n",
    "        #batches = preProcess.batch_iter(x_train, 168001, 1)\n",
    "        batches = preProcess.batch_iter(x_train, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # what is actually 'batches'? - it is a generator\n",
    "        # a batch is defined as an x and a y.\n",
    "       \n",
    "        for idx, batch in enumerate(batches):\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            # split each batch into x and y\n",
    "            #print(f\"splitting batch {batch.size} \")\n",
    "\n",
    "            x_bat, y_bat = split_input(batch)\n",
    "            # feed data to placeholders, so data that changes with each batch\n",
    "            #print(f\"starting batch {idx} \")\n",
    "            feed_dict = {\n",
    "                # these are CDN placeholders\n",
    "                cnn.encoder_input: y_bat,\n",
    "                cnn.encoder_input_GO: x_bat,\n",
    "                cnn.gaussian_samples: np.random.normal(size=[x_bat.shape[0], FLAGS.unit_gaussian_dim]),\n",
    "            }\n",
    " \n",
    "            # Runs operations and evaluates tensors in `fetches`.\n",
    "            # TODO Why the heck do we need to call train_op AND total_loss, summary, etc.? Just for debugging?\n",
    "            _, step, summaries, loss, accuracy, kldiv, CEloss = sess.run(\n",
    "                fetches=[train_op, global_step, train_summary_op, cnn.total_loss, cnn.accuracy, cnn.mean_latent_loss,\n",
    "                         cnn.CE_loss], feed_dict=feed_dict)\n",
    " \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if current_step % 100 == 0:\n",
    "                print((\"{}: step {}, loss {:g}, latentLoss: {:g}, reconstructionLoss: {:g}, acc {:g}\".format(\n",
    "                    time_str, step, loss, kldiv, CEloss, accuracy)))\n",
    " \n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    " \n",
    "            # every N steps we run also evaluation\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    " \n",
    "                x_bat, y_bat = split_input(batch)\n",
    "                feed_dict = {\n",
    "                    cnn.encoder_input: y_bat,\n",
    "                    cnn.encoder_input_GO: x_bat,\n",
    "                    cnn.gaussian_samples: np.random.normal(size=[x_bat.shape[0], FLAGS.unit_gaussian_dim]),\n",
    "                }\n",
    " \n",
    "                step, summaries, loss, accuracy, kldiv, CEloss = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.total_loss, cnn.accuracy, cnn.mean_latent_loss, cnn.CE_loss],\n",
    "                    feed_dict)\n",
    " \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\n",
    "                    (\"{}: step {}, loss {:g}, klDiv: {:g}, CE-loss: {:g}, acc {:g}\".format(time_str, step, loss, kldiv,\n",
    "                                                                                           CEloss, accuracy)))\n",
    "                if dev_summary_writer:\n",
    "                    dev_summary_writer.add_summary(summaries, step)\n",
    " \n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print((\"Saved model checkpoint to {}\\n\".format(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T14:41:43.884747Z",
     "start_time": "2018-11-21T14:41:43.850608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:01:11.760650Z",
     "start_time": "2018-11-21T15:01:11.729991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(576, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "fda_file = \"/home/fabsta/projects/deeplearning/vae_playground/CDN_Molecule/data/FDA\"\n",
    "fda_x = np.array(pickle.load(open(fda_file, \"rb\")))\n",
    "fda_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:01:13.567279Z",
     "start_time": "2018-11-21T15:01:13.532522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of input molecules:  576\n",
      "Train/Dev split: 559/17\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(10)\n",
    "print(\"Num of input molecules: \", str(len(fda_x)))\n",
    "shuffle_indices = np.random.permutation(np.arange(len(fda_x)))\n",
    "fda_x_shuffled = fda_x[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(fda_x)))\n",
    "fda_x_train, fda_x_dev = fda_x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "print((\"Train/Dev split: {:d}/{:d}\".format(len(fda_x_train), len(fda_x_dev))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:01:45.765064Z",
     "start_time": "2018-11-21T15:01:45.733795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_input(batch):\n",
    "    x_batch = batch\n",
    "    y_batch = np.concatenate([x_batch[:, 1:], np.zeros(shape=[x_batch.shape[0], 1], dtype=np.int32) + 36], axis=1)\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "def get_smile(ar, num2char):\n",
    "    smile = \"\"\n",
    "    for i in ar:\n",
    "        smile += num2char[str(i)]\n",
    "    return smile\n",
    "\n",
    "def analyze_output(predictions):\n",
    "    # analyze output\n",
    "    num2char = preProcess.load_json_file('data/num2char.json')\n",
    "    num2char['34'] = 'GO' ; num2char['35'] = 'EN' ; num2char['36'] = 'PA'\n",
    "    count = 0\n",
    "    new_mol = []#set()\n",
    "    real_mol = []\n",
    "    for index, pred in enumerate(predictions):\n",
    "        real_smile = get_smile(x_bat[index], num2char)\n",
    "        real_smile = real_smile.split('EN')[0].split('GO')[1]\n",
    "        fake_smile = get_smile(pred, num2char)\n",
    "        fake_smile = fake_smile.split('EN')[0]\n",
    "        res = preProcess.mol_analysis(fake_smile, real_smile)\n",
    "        count += res\n",
    "        if res == 1:\n",
    "            if fake_smile != real_smile:\n",
    "                # new_mol.add(fake_smile)\n",
    "                new_mol.append(fake_smile)\n",
    "                real_mol.append(real_smile)\n",
    "\n",
    "    return new_mol, real_mol, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T15:07:31.343769Z",
     "start_time": "2018-11-21T15:07:29.943258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac89319358>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459\n"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": "Unable to open table file /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459: Failed precondition: /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-32-945db3675af7>:29)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-945db3675af7>\", line 29, in <module>\n    saver = tf.train.Saver().restore(sess, checkpoint_file)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 795, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 862, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459: Failed precondition: /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-32-945db3675af7>:29)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459: Failed precondition: /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-945db3675af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#checkpoint_file = FLAGS.parameters_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"restored \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1544\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1546\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1547\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459: Failed precondition: /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-32-945db3675af7>:29)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-945db3675af7>\", line 29, in <module>\n    saver = tf.train.Saver().restore(sess, checkpoint_file)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 795, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 862, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/fabsta/.conda/envs/cdn_molecule/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459: Failed precondition: /home/fabsta/projects/deeplearning/vae_playground/runs/1542811459; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-32-945db3675af7>:29)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = '/home/fabsta/projects/deeplearning/vae_playground/runs/1542811459'\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "graph = tf.Graph()\n",
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True)\n",
    "    )\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    with sess.as_default():\n",
    "        CDN = model.CDN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            vocab_size=FLAGS.vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            max_molecule_length=FLAGS.max_molecule_length,\n",
    "            gaussian_samples=FLAGS.unit_gaussian_dim,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            variational=True,\n",
    "            test_mode=True,\n",
    "        )\n",
    "\n",
    "        encoded, latent_loss = CDN.encode()\n",
    "        logits = CDN.decode_rnn(encoded)\n",
    "        loss, accuracy = CDN.loss(logits, latent_loss)\n",
    "\n",
    "        #checkpoint_file = FLAGS.parameters_file\n",
    "        saver = tf.train.Saver().restore(sess, checkpoint_file)\n",
    "        print(\"restored \" + str(checkpoint_file))\n",
    "\n",
    "        x_bat, y_bat = split_input(x_dev)\n",
    "        feed_dict = {\n",
    "            CDN.encoder_input: y_bat,\n",
    "            CDN.encoder_input_GO: x_bat,\n",
    "            CDN.gaussian_samples: np.random.normal(size=[x_bat.shape[0], FLAGS.unit_gaussian_dim], scale=1.0),\n",
    "        }\n",
    "\n",
    "        outputs = sess.run(\n",
    "            [encoded, latent_loss, logits, loss, accuracy, CDN.all_symbols], feed_dict=feed_dict)\n",
    "\n",
    "        predictions = np.argmax(outputs[2], axis=2)\n",
    "        # predictions = outputs[-1]\n",
    "\n",
    "        new_mol, real_mol, valid = analyze_output(predictions)\n",
    "        with open(FLAGS.output_file, 'w') as f:\n",
    "            for ori, gen in zip(real_mol, new_mol):\n",
    "                f.write(ori + ',' + gen + '\\n')\n",
    "\n",
    "        print(\"Loss \" + str(outputs[3]))\n",
    "        print(\"Acc with respect to real: \" + str(outputs[4]))\n",
    "        print(\"Total evaluated molecules: \" + str(len(predictions)))\n",
    "        print(\"Total valid molecules: \" + str(valid))\n",
    "        print(\"Total valid molecules ratio: \" + str(valid/float(len(predictions))))\n",
    "        print(\"New mols: \" + str(len(set(new_mol))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "cdn_molecule",
   "language": "python",
   "name": "cdn_molecule"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
